<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PA &amp; CODING</title>
  
  <subtitle>求仁得仁</subtitle>
  <link href="https://jxch.github.io/atom.xml" rel="self"/>
  
  <link href="https://jxch.github.io/"/>
  <updated>2024-09-10T08:06:04.008Z</updated>
  <id>https://jxch.github.io/</id>
  
  <author>
    <name>钱不寒</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Redis-7.0新特性</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-7.0-xin-te-xing/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-7.0-xin-te-xing/</id>
    <published>2024-09-10T08:04:00.000Z</published>
    <updated>2024-09-10T08:06:04.008Z</updated>
    
    <content type="html"><![CDATA[<ul><li>共享复制缓存区<ul><li>老版本多从库时主库内存占用过多（每个从库都有一个从库复制缓冲区）<ul><li><img src="/static/IT/Redis/Redis-7.0%E6%96%B0%E7%89%B9%E6%80%A7-1.png" alt=""></li></ul></li><li>Redis 为了提升多从库全量复制的效率和减少 fork 产生RDB 的次数，会尽可能的让多个从库共用一个 RDB<ul><li>将 ReplicationBuffer 数据切割为多个 16KB 的数据块 (replBufBlock)，然后使用链表来维护起来</li><li><img src="/static/IT/Redis/Redis-7.0%E6%96%B0%E7%89%B9%E6%80%A7-2.png" alt=""></li><li>ReplicationBuffer 由多个 replBufBlock 组成链表，当复制积压区或从库对某个block 使用时，便对正在使用的 replBufBlock 增加引用计数</li><li>当从库使用完当前的 replBufBlock（已经将数据发送给从库）时，就会对其 refcount 减 1 而且移动到下一个 replBufBlock，并对其refcount 加1</li></ul></li></ul></li><li>ReplicationBuffer 的裁剪和释放<ul><li>ReplicationBuffer 不可能无限增长，Redis 有相应的逻辑对其进行裁剪，简单来说，Redis 会从头访问 replBufBlock 链表，如果发现 replBufBlock refcount为0，则会释放它，直到迭代到第一个 replBufBlock refcount 不为0 才停止<ul><li>当从库使用完当前的 replBufBlock 会对其refcount 减1</li><li>当从库断开链接时会对正在引用的 replBufBlock refcount 减1，无论是因为超过client-output-buffer-limit 导致的断开还是网络原因导致的断开</li><li>当 ReplicationBacklog 引用的replBufBlock 数据量超过设置的该值大小时，会对正在引用的 replBufBlock refcount 减1，以尝试释放内存</li></ul></li><li>当一个从库引用的 replBufBlock 过多，它断开时释放的 replBufBlock 可能很多，也可能造成堵塞问题，所以Redis7 里会限制一次释放的个数，未及时释放的内存在系统的定时任务中渐进式释放</li></ul></li><li>使用 Rax 树实现对 replBufBlock 固定区间间隔的索引，每 64 个记录一个索引点<ul><li>Rax 索引占用的内存较少；查询效率也是非常高</li><li>streams 里面的 consumer group (消费者组) 的名称还有和 Redis 集群名称的存储也是使用的 Rax 树</li></ul></li><li>Trie 字典树（前缀树）<ul><li><img src="/static/IT/Redis/Redis-7.0%E6%96%B0%E7%89%B9%E6%80%A7-3.png" alt=""></li></ul></li><li>Rax 基数树（前缀压缩树）：压缩后的 Trie 树<ul><li><img src="/static/IT/Redis/Redis-7.0%E6%96%B0%E7%89%B9%E6%80%A7-4.png" alt=""></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;共享复制缓存区
&lt;ul&gt;
&lt;li&gt;老版本多从库时主库内存占用过多（每个从库都有一个从库复制缓冲区）
&lt;ul&gt;
&lt;li&gt;&lt;img src=&quot;/static/IT/Redis/Redis-7.0%E6%96%B0%E7%89%B9%E6%80%A7-1.png&quot; a</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-6.0新特性</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-6.0-xin-te-xing/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-6.0-xin-te-xing/</id>
    <published>2024-09-10T08:02:00.000Z</published>
    <updated>2024-09-10T08:04:01.427Z</updated>
    
    <content type="html"><![CDATA[<ul><li>多线程</li><li>Client&nbsp;Side&nbsp;Cache</li><li>Acls</li></ul><hr><h2 id="多线程">多线程</h2><ul><li>redis&nbsp;6.0&nbsp;提供了多线程的读写IO,&nbsp;但是最终执行用户命令的线程依然是单线程的<ul><li><code>io‐threads&nbsp;4</code> 有三个IO线程，还有一个线程是main线程，，main线程负责IO读写和命令执行操作<ul><li>这三个IO线程只会执行&nbsp;IO中的write操作，也就是说， read和命令执行都由main线程执行。最后多线程将数据写回到客户端</li><li><img src="/static/IT/Redis/Redis-6.0%E6%96%B0%E7%89%B9%E6%80%A7-1.png" alt=""></li></ul></li><li><code>io‐threads‐do‐reads&nbsp;yes</code> 将支持IO线程执行读写任务<ul><li><img src="/static/IT/Redis/Redis-6.0%E6%96%B0%E7%89%B9%E6%80%A7-2.png" alt=""></li></ul></li></ul></li></ul><hr><h2 id="Client-Side-Cache">Client&nbsp;Side&nbsp;Cache</h2><ul><li>客户端缓存：redis&nbsp;6&nbsp;提供了服务端追踪key的变化，客户端缓存数据的特性<ul><li><img src="/static/IT/Redis/Redis-6.0%E6%96%B0%E7%89%B9%E6%80%A7-3.png" alt=""></li></ul></li><li>执行流程<ul><li>当客户端访问某个key时，服务端将记录key&nbsp;和&nbsp;client</li><li>客户端拿到数据后，进行客户端缓存</li><li>这时，当key再次被访问时，key将被直接返回，避免了与redis服务器的再次交互</li><li>当数据被其他请求修改时，服务端将主动通知客户端失效的key</li><li>客户端进行本地失效</li><li>下次请求时，重新获取最新数据</li></ul></li><li>目前只有lettuce对其进行了支持</li></ul><hr><h2 id="ACL">ACL</h2><ul><li>ACL&nbsp;是对于命令的访问和执行权限的控制</li><li>ACL 设置有两种方式<ul><li>命令方式：ACL&nbsp;SETUSER&nbsp;+&nbsp;具体的权限规则，&nbsp;通过&nbsp;ACL&nbsp;SAVE&nbsp;进行持久化</li><li>对&nbsp;ACL&nbsp;配置文件进行编写，并且执行&nbsp;ACL&nbsp;LOAD&nbsp;进行加载</li></ul></li><li>ACL存储有两种方式，但是两种方式不能同时配置，否则直接报错退出进程<ul><li>redis&nbsp;配置文件：&nbsp;redis.conf</li><li>ACL 配置文件，在redis.conf&nbsp;中通过&nbsp;aclfile&nbsp;&nbsp;/path&nbsp;&nbsp;配置acl文件的路径</li></ul></li><li><code>ACL&nbsp;SETUSER&nbsp;alice</code> 创建一个用户名为&nbsp;alice的用户<ul><li>用户alice&nbsp;没有任何意义</li><li>处于&nbsp;off&nbsp;状态，&nbsp;它是被禁用的，不能用auth进行认证</li><li>不能访问任何命令</li><li>不能访问任意的key</li><li>没有密码</li></ul></li><li><code>acl&nbsp;setuser&nbsp;alice&nbsp;on&nbsp;&gt;pass123&nbsp;~cached:*&nbsp;+get</code> 创建一个对&nbsp;cached:&nbsp;前缀具有get命令执行权限的用户，并且设置密码</li><li>切换其他用户进行登录<ul><li><code>ACL&nbsp;GETUSER&nbsp;alice</code></li><li><code>ACL&nbsp;SETUSER&nbsp;alice&nbsp;~objects:*&nbsp;~items:*&nbsp;~public:*</code> 添加多个访问模式，空格分隔</li></ul></li><li><code>ACL&nbsp;SETUSER&nbsp;alice&nbsp;on&nbsp;+@all&nbsp;‐@dangerous&nbsp;&gt;密码&nbsp;~*</code> 针对类型命令的约束<ul><li>+@all:&nbsp;&nbsp;包含所有得命令</li><li>然后用 -@&nbsp;去除在 redis&nbsp;command&nbsp;table&nbsp;中定义的&nbsp;dangerous&nbsp;命令</li></ul></li><li>查看具体有哪些命令属于某个类别<ul><li><code>acl&nbsp;cat</code></li><li><code>acl&nbsp;cat&nbsp;dangerous</code></li></ul></li><li>开放子命令<ul><li><code>ACL&nbsp;SETUSER&nbsp;myuser&nbsp;‐client&nbsp;+client|setname&nbsp;+client|getname</code><ul><li>禁用client&nbsp;命令，但是开放&nbsp;client&nbsp;命令中的子命令&nbsp;&nbsp;setname&nbsp;和&nbsp;getname<ul><li>只能是先禁用，后追加子命令，因为后续可能会有新的命令增加</li></ul></li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;多线程&lt;/li&gt;
&lt;li&gt;Client&amp;nbsp;Side&amp;nbsp;Cache&lt;/li&gt;
&lt;li&gt;Acls&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;多线程&quot;&gt;多线程&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;redis&amp;nbsp;6.0&amp;nbsp;提供了多线程的读写</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-分布式锁</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-fen-bu-shi-suo/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-fen-bu-shi-suo/</id>
    <published>2024-09-10T08:01:00.000Z</published>
    <updated>2024-09-10T08:01:56.494Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Redisson -&gt; LUA<ul><li>自旋信号量拿锁<ul><li>解锁后信号量唤醒尝试再加锁（Redis订阅发布通知唤醒）</li></ul></li><li>锁续命<ul><li>避免线程执行时锁失效，导致该线程释放其它线程的锁</li></ul></li><li>锁重入</li><li>读写锁</li><li>锁失效：无解，Redis的可用性高于可靠性</li><li>原子加锁&amp;原子解锁：避免加解锁中途服务故障</li></ul></li><li>RedLock（超半数加锁）锁失效:<ul><li>主从锁失效<ul><li>复制延迟</li><li>主从切换</li></ul></li><li>1s持久化数据丢失</li><li>网络分区</li><li>时钟漂移</li><li>宕机（无法在足够多的实例上获取锁）</li></ul></li><li>锁粒度：分段锁（将一个key拆分成多个，供不同的服务器读写）</li><li>Redisson 分布式锁<ul><li><img src="/static/IT/Redis/Redis-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81-1.png" alt="Redisson 分布式锁"></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Redisson -&amp;gt; LUA
&lt;ul&gt;
&lt;li&gt;自旋信号量拿锁
&lt;ul&gt;
&lt;li&gt;解锁后信号量唤醒尝试再加锁（Redis订阅发布通知唤醒）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;锁续命
&lt;ul&gt;
&lt;li&gt;避免线程执行时锁失效，导致该线程释放其它线程的</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-Stream</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-stream/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-stream/</id>
    <published>2024-09-10T07:56:00.000Z</published>
    <updated>2024-09-10T07:56:59.001Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Stream：支持多播的可持久化的消息队列<ul><li>常用操作命令</li></ul></li><li>Stream 消息队列的问题</li></ul><hr><h2 id="Stream">Stream</h2><p><img src="/static/IT/Redis/Redis-Stream-1.png" alt="Stream"></p><ul><li>每一个Stream都有一个消息链表，将所有加入的消息都串起来，每个消息都有一个唯一的 ID 和对应的内容</li><li>消息是持久化的，Redis 重启后，内容还在</li><li>每个 Stream 都有唯一的名称，它就是 Redis 的 key，在我们首次使用xadd指令追加消息时自动创建</li><li>每个 Stream 都可以挂多个消费组，每个消费组会有个游标last_delivered_id在Stream 数组之上往前移动，表示当前消费组已经消费到哪条消息了<ul><li>每个消费组都有一个 Stream 内唯一的名称，消费组不会自动创建，它需要单独的指令xgroup create进行创建，需要指定从 Stream 的某个消息 ID 开始消费，这个ID 用来初始化last_delivered_id变量</li><li>每个消费组 (Consumer Group) 的状态都是独立的，相互不受影响<ul><li>同一份 Stream 内部的消息会被每个消费组都消费到</li></ul></li><li>同一个消费组 (Consumer Group) 可以挂接多个消费者 (Consumer)<ul><li>这些消费者之间是竞争关系，任意一个消费者读取了消息都会使游标last_delivered_id往前移动</li><li>每个消费者有一个组内唯一名称</li></ul></li></ul></li><li>消费者 (Consumer) 内部会有个状态变量 pending_ids，它记录了当前已经被客户端读取,但是还没有 ack 的消息<ul><li>如果客户端没有 ack，这个变量里面的消息 ID 会越来越多，一旦某个消息被 ack，它就开始减少</li><li>这个 pending_ids 变量在 Redis 官方被称之为 PEL，也就是 Pending Entries List，这是一个很核心的数据结构<ul><li>它用来确保客户端至少消费了消息一次，而不会在网络传输的中途丢失了没处理</li></ul></li></ul></li><li>消息 ID 的形式是 timestampInMillis-sequence，例如1527846880572-5，它表示当前的消息在毫米时间戳 1527846880572 时产生，并且是该毫秒内产生的第5 条消息<ul><li>消息 ID 可以由服务器自动生成，也可以由客户端自己指定，但是形式必须是整数-整数，而且必须是后面加入的消息的 ID 要大于前面的消息 ID</li></ul></li><li>消息内容就是键值对，形如 hash 结构的键值对</li></ul><hr><h2 id="常用操作命令">常用操作命令</h2><ul><li>生产端<ul><li>命令<ul><li>xadd 追加消息</li><li>xdel 删除消息，这里的删除仅仅是设置了标志位，不会实际删除消息</li><li>xrange 获取消息列表，会自动过滤已经删除的消息</li><li>xlen 消息长度</li><li>del 删除 Stream</li></ul></li><li><code>xadd streamtest * name mark age 18</code><ul><li>streamtest 表示当前这个队列的名字，也就是我们一般意义上Redis 中的key</li><li><code>*</code> 号表示服务器自动生成 ID</li><li><code>name mark age 18</code> 是 key/value 的存储形式</li><li>返回值 1626705954593-0 则是生成的消息 ID，由两部分组成：时间戳-序号<ul><li>时间戳时毫秒级单位，是生成消息的 Redis 服务器时间，它是个 64 位整型</li><li>序号是在这个毫秒时间点内的消息序号。它也是个 64 位整型</li></ul></li><li>为了保证消息是有序的，因此 Redis 生成的ID 是单调递增有序的<ul><li>由于 ID中包含时间戳部分，为了避免服务器时间错误而带来的问题（例如服务器时间延后了），Redis 的每个 Stream 类型数据都维护一个 latest_generated_id 属性，用于记录最后一个消息的 ID</li><li>若发现当前时间戳退后（小于 latest_generated_id 所记录的），则采用时间戳不变而序号递增的方案来作为新消息 ID（这也是序号为什么使用int64 的原因，保证有足够多的的序号），从而保证 ID 的单调递增性质</li></ul></li><li>如果不是非常特别的需求，强烈建议使用 Redis 的方案生成消息ID，因为这种时间戳+序号的单调递增的 ID 方案，几乎可以满足全部的需求，但 ID 是支持自定义的</li></ul></li><li><code>xrange streamtest - +</code><ul><li><code>-</code> 表示最小值 , <code>+</code> 表示最大值</li></ul></li><li><code>xrange streamtest - 1626705954593-0</code> 指定消息 ID</li><li><code>xdel streamtest 1626706380924-0</code></li><li><code>xlen streamtest</code></li><li><code>del streamtest</code> 删除整个 Stream</li></ul></li><li>单消费者<ul><li>虽然 Stream 中有消费者组的概念，但是可以在不定义消费组的情况下进行Stream 消息的独立消费，当 Stream 没有新消息时，甚至可以阻塞等待</li><li>Redis 设计了一个单独的消费指令 xread，可以将 Stream 当成普通的消息队列 (list) 来使用<ul><li>使用 xread 时，我们可以完全忽略消费组 (Consumer Group) 的存在，就好比 Stream 就是一个普通的列表 (list)</li></ul></li><li><code>xread count 1 streams stream2 0-0</code><ul><li>“count 1” 表示从 Stream 读取1 条消息，缺省当然是头部</li><li>“streams”  可以理解为Redis 关键字</li><li>“stream2” 指明了要读取的队列名称</li><li>“0-0” 指从头开始</li></ul></li><li><code>xread count 2 streams stream2 1626710882927-0</code><ul><li>指定从 streams 的消息Id 开始(不包括命令中的消息 id)</li></ul></li><li><code>xread count 1 streams stream2 $</code><ul><li><code>$</code> 代表从尾部读取，上面的意思就是从尾部读取最新的一条消息</li><li>此时默认不返回任何消息，所以最好以阻塞的方式读取尾部最新的一条消息，直到新的消息的到来<ul><li><code>xread block 0 count 1 streams stream2 $</code><ul><li>block 后面的数字代表阻塞时间，单位毫秒</li></ul></li></ul></li></ul></li><li>一般来说客户端如果想要使用 xread 进行顺序消费，一定要记住当前消费到哪里了，也就是返回的消息 ID<ul><li>下次继续调用 xread 时，将上次返回的最后一个消息 ID 作为参数传递进去，就可以继续消费后续的消息</li></ul></li></ul></li><li>消费组<ul><li>xgroup create 指令创建消费组 (Consumer Group)，需要传递起始消息 ID 参数用来初始化 last_delivered_id 变量<ul><li><code>xgroup create stream2 cg1 0-0</code><ul><li>“stream2” 指明了要读取的队列名称</li><li>“cg1” 表示消费组的名称</li><li>“0-0” 表示从头开始消费</li></ul></li><li><code>xgroup create stream2 cg2 $</code><ul><li><code>$</code> 表示从尾部开始消费，只接受新消息，当前 Stream 消息会全部忽略</li></ul></li><li><code>xinfo stream stream2</code></li><li><code>xinfo groups stream2</code></li></ul></li><li>xreadgroup 指令可以进行消费组的组内消费，需要提供消费组名称、消费者名称和起始消息 ID；同 xread 一样，也可以阻塞等待新消息。读到新消息后，对应的消息 ID 就会进入消费者的 PEL(正在处理的消息) 结构里，客户端处理完毕后使用 xack 指令通知服务器，本条消息已经处理完毕，该消息 ID 就会从 PEL 中移除<ul><li><code>xreadgroup GROUP cg1 c1 count 1 streams stream2 &gt;</code><ul><li>“GROUP”属于关键字</li><li>“cg1”是消费组名称</li><li>“c1”是消费者名称</li><li>“count 1”指明了消费数量</li><li><code>&gt;</code> 号表示从当前消费组的 last_delivered_id 后面开始读<ul><li>每当消费者读取一条消息，last_delivered_id 变量就会前进</li></ul></li></ul></li><li><code>xreadgroup GROUP cg1 c1 block 0 count 1 streams stream2 &gt;</code> 阻塞等待</li></ul></li><li>如果同一个消费组有多个消费者，我们还可以通过 xinfo consumers 指令观察每个消费者的状态<ul><li><code>xinfo consumers stream2 cg1</code></li><li><code>xack stream2 cg1 1626751586744-0</code> 确认一条消息<ul><li>xack 允许带多个消息 id</li></ul></li></ul></li><li>XPENDIING 用来获消费组或消费内消费者的未处理完毕的消息<ul><li>每个 Pending 的消息有 4 个属性：消息 ID；所属消费者；IDLE 已读取时长；delivery counter 消息被读取次数</li></ul></li><li>XCLAIM 用以进行消息转移的操作，将某个消息转移到自己的 Pending列表中。需要设置组、转移的目标消费者和消息 ID，同时需要提供 IDLE（已被读取时长），只有超过这个时长，才能被转移</li></ul></li></ul><hr><h2 id="Stream-消息队列的问题">Stream 消息队列的问题</h2><ul><li>消息太多<ul><li>定长 Stream ：在 xadd 的指令提供一个定长长度 maxlen，就可以将老的消息干掉，确保最多不超过指定长度</li></ul></li><li>消费者忘记 ACK<ul><li>导致 PEL 列表不断增长，如果有很多消费组的话，那么这个 PEL 占用的内存就会放大</li></ul></li><li>PEL 如何避免消息丢失<ul><li>在客户端消费者读取 Stream 消息时，Redis 服务器将消息回复给客户端的过程中，客户端突然断开了连接，消息就丢失了</li><li>但是 PEL 里已经保存了发出去的消息 ID</li><li>客户端重新连上之后，可以再次收到 PEL 中的消息 ID 列表<ul><li>不过此时 xreadgroup 的起始消息 ID 不能为参数 <code>&gt;</code>，而必须是任意有效的消息ID，一般将参数设为 0-0，表示读取所有的 PEL 消息以及自last_delivered_id之后的新消息</li></ul></li></ul></li><li>死信问题<ul><li>如果某个消息，不能被消费者处理，也就是不能被 XACK，这是要长时间处于 Pending 列表中，即使被反复的转移给各个消费者也是如此</li><li>此时该消息的delivery counter（通过XPENDING 可以查询到）就会累加，当累加到某个我们预设的临界值时，我们就认为是坏消息（也叫死信，DeadLetter，无法投递的消息）<ul><li>将坏消息处理掉即可，删除即可，使用XDEL 语法<ul><li>注意，这个命令并没有删除 Pending 中的消息，因此查看 Pending，消息还会在，可以在执行执行 XDEL 之后，XACK 这个消息标识其处理完毕</li></ul></li></ul></li></ul></li><li>Stream 的高可用<ul><li>Stream 的高可用是建立主从复制基础上的，它和其它数据结构的复制机制没有区别</li><li>也就是说在 Sentinel 和 Cluster 集群环境下 Stream 是可以支持高可用的</li><li>不过鉴于 Redis 的指令复制是异步的，在 failover 发生时，Redis 可能会丢失极小部分数据<ul><li>这点 Redis 的其它数据结构也是一样的</li></ul></li></ul></li><li>分区 Partition<ul><li>Redis 的服务器没有原生支持分区能力，如果想要使用分区，那就需要分配多个 Stream<ul><li>提供不同的 Stream 名称，对消息进行 hash 取模来选择往哪个 Stream 里塞</li></ul></li><li>然后在客户端使用一定的策略来生产消息到不同的 Stream</li></ul></li><li>使用场景<ul><li>如果是中小项目和企业，在工作中已经使用了 Redis，在业务量不是很大，而又需要消息中间件功能的情况下，可以考虑使用 Redis 的Stream 功能</li><li>如果并发量很高，资源足够支持下，还是以专业的消息中间件，比如RocketMQ、Kafka 等来支持业务更好</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Stream：支持多播的可持久化的消息队列
&lt;ul&gt;
&lt;li&gt;常用操作命令&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stream 消息队列的问题&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Stream&quot;&gt;Stream&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-IO模型&amp;多线程</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-io-mo-xing-duo-xian-cheng/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-io-mo-xing-duo-xian-cheng/</id>
    <published>2024-09-10T07:50:00.000Z</published>
    <updated>2024-09-10T07:53:47.063Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Redis I/O 多路复用</li><li>Redis 多线程</li><li>Reactor 模式</li></ul><hr><h2 id="I-O-多路复用">I/O 多路复用</h2><ul><li>Redis 基于 Reactor 模式开发了自己的网络事件处理器</li><li>I/O 多路复用同时监听多个 socket，根据socket 当前执行的事件来为 socket 选择对应的事件处理器<ul><li>当被监听的 socket 准备好执行accept、read、write、close 等操作时，和操作对应的文件事件就会产生，这时 FEH 就会调用 socket 之前关联好的事件处理器来处理对应事件</li></ul></li><li>文件事件处理器（file event handler，后文简称为 FEH）是单线程的，所以 redis 设计为单线程模型</li><li>虽然 FEH 是单线程运行，但通过 I/O 多路复用监听多个 socket，不仅实现高性能的网络通信模型，又能和 Redis 服务器中其它同样单线程运行的模块交互，保证了Redis 内部单线程模型的简洁设计</li><li><img src="/static/IT/Redis/Redis-IO%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E7%BA%BF%E7%A8%8B-1.png" alt=""></li><li>socket<ul><li>文件事件就是对 socket 操作的抽象， 每当一个 socket 准备好执行连接accept、read、write、close 等操作时， 就会产生一个文件事件。一个服务器通常会连接多个socket， 多个 socket 可能并发产生不同操作，每个操作对应不同文件事件</li></ul></li><li>I/O 多路复用程序：负责监听多个 socket<ul><li>尽管文件事件可能并发出现， 但 I/O 多路复用程序会将所有产生事件的socket 放入队列， 通过该队列以有序、同步且每次一个 socket 的方式向文件事件分派器传送 socket</li><li>当上一个 socket 产生的事件被对应事件处理器执行完后， I/O 多路复用程序才会向文件事件分派器传送下个 socket</li><li><img src="/static/IT/Redis/Redis-IO%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E7%BA%BF%E7%A8%8B-2.png" alt=""></li></ul></li><li>Redis 的 I/O 多路复用程序的所有功能都是通过包装常见的 select、epoll、evport 和 kqueue 这些 I/O 多路复用函数库实现的<ul><li>编译时自动选择系统中性能最高的I/O 多路复用函数库作为 Redis 的 I/O 多路复用程序的底层实现：性能降序排列<ul><li>Evport，Epoll 和 KQueue 具有 O(1)描述符选择算法复杂度，可以提供很多(数十万个)文件描述符</li><li>select 复杂性是 O(n)，最多只能提供 1024 个描述符</li></ul></li></ul></li><li>文件事件分派器：接收 I/O 多路复用程序传来的 socket， 并根据 socket 产生的事件类型， 调用相应的事件处理器</li><li>文件事件处理器：服务器会为执行不同任务的套接字关联不同的事件处理器</li><li>文件事件的类型<ul><li>I/O 多路复用程序可以监听多个 socket 的 ae.h/AE_READABLE 事件和ae.h/AE_WRITABLE 事件</li><li>当 socket 可读（比如客户端对 Redis 执行write/close 操作），或有新的可应答的socket 出现时（即客户端对 Redis 执行 connect 操作），socket 就会产生一个AE_READABLE 事件</li><li>当 socket 可写时（比如客户端对 Redis 执行read 操作），socket 会产生一个AE_WRITABLE 事件</li><li>I/O 多路复用程序可以同时监听 AE_REABLE 和AE_WRITABLE 两种事件，要是一个socket 同时产生这两种事件，那么文件事件分派器优先处理 AE_REABLE 事件</li><li>即一个socket 又可读又可写时， Redis 服务器先读后写 socket</li></ul></li><li>客户端和 Redis 服务器通信的整个过程<ul><li>Redis 启动初始化时，将连接应答处理器跟 AE_READABLE 事件关联</li><li>若一个客户端发起连接，会产生一个 AE_READABLE 事件，然后由连接应答处理器负责和客户端建立连接，创建客户端对应的 socket，同时将这个 socket的AE_READABLE 事件和命令请求处理器关联，使得客户端可以向主服务器发送命令请求</li><li>当客户端向 Redis 发请求时（不管读还是写请求），客户端 socket 都会产生一个AE_READABLE 事件，触发命令请求处理器。处理器读取客户端的命令内容，然后传给相关程序执行</li><li>当 Redis 服务器准备好给客户端的响应数据后，会将 socket 的AE_WRITABLE事件和命令回复处理器关联，当客户端准备好读取响应数据时，会在 socket 产生一个AE_WRITABLE 事件，由对应命令回复处理器处理，即将准备好的响应数据写入socket，供客户端读取</li><li>命令回复处理器全部写完到 socket 后，就会删除该socket 的AE_WRITABLE事件和命令回复处理器的映射</li></ul></li></ul><hr><h2 id="Redis-多线程">Redis 多线程</h2><ul><li>严格来讲从 Redis4.0 之后并不是单线程，除了主线程外，它也有后台线程在处理一些较为缓慢的操作，例如清理脏数据、无用连接的释放、大 key 的删除等等</li><li>通过AE 事件模型以及 IO 多路复用等技术，处理性能非常高，因此没有必要使用多线程<ul><li>Hash 的惰性Rehash、Lpush 等等 “线程不安全” 的命令都可以无锁进行</li></ul></li><li>读写网络的 read/write 系统调用占用了Redis执行期间大部分CPU 时间，瓶颈主要在于网络的 IO 消耗</li><li><code>io-threads-do-reads yes</code> 开启多线程<ul><li><code>io-threads 4</code> 开启多线程后，还需要设置线程数，否则是不生效的</li><li>线程数一定要小于机器核数<ul><li>4 核的机器建议设置为2 或3 个线程</li><li>8 核的建议设置为 6 个线程</li><li>超过了 8 个基本就没什么意义了</li></ul></li></ul></li><li>至少要 4 核的机器，且 Redis 实例已经占用相当大的 CPU耗时的时候才建议采用，否则使用多线程没有意义</li><li>多线程的实现机制<ul><li>主线程负责接收建立连接请求，获取 socket 放入全局等待读处理队列</li><li>主线程处理完读事件之后，通过 RR(Round Robin) 将这些连接分配给这些 IO 线程</li><li>主线程阻塞等待 IO 线程读取 socket 完毕</li><li>主线程通过单线程的方式执行请求命令，请求数据读取并解析完成，但并不执行回写 socket</li><li>主线程阻塞等待 IO 线程将数据回写 socket 完毕</li><li>解除绑定，清空等待队列</li></ul></li><li>特点<ul><li>IO 线程要么同时在读 socket，要么同时在写，不会同时读或写</li><li>IO 线程只负责读写 socket 解析命令，不负责命令处理</li></ul></li><li>不需要去考虑控制key、lua、事务，LPUSH/LPOP 等等的并发及线程安全问题<ul><li>Redis 的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程顺序执行</li></ul></li></ul><hr><h2 id="Reactor-模式">Reactor 模式</h2><p>单线程 Reactor 模式：<br><img src="/static/IT/Redis/Redis-IO%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E7%BA%BF%E7%A8%8B-3.png" alt="单线程 Reactor 模式"></p><p>单线程 Reactor + 工作者线程池：<br><img src="/static/IT/Redis/Redis-IO%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E7%BA%BF%E7%A8%8B-4.png" alt="单线程 Reactor + 工作者线程池"></p><p>多 Reactor 线程模式：<br><img src="/static/IT/Redis/Redis-IO%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E7%BA%BF%E7%A8%8B-5.png" alt="多 Reactor 线程模式"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Redis I/O 多路复用&lt;/li&gt;
&lt;li&gt;Redis 多线程&lt;/li&gt;
&lt;li&gt;Reactor 模式&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;I-O-多路复用&quot;&gt;I/O 多路复用&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Redis 基于 Reactor 模式开</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-HyperLogLog&amp;事务&amp;Pipeline</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-hyperloglog-shi-wu-pipeline/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-hyperloglog-shi-wu-pipeline/</id>
    <published>2024-09-10T07:48:00.000Z</published>
    <updated>2024-09-10T07:48:53.998Z</updated>
    
    <content type="html"><![CDATA[<ul><li>HyperLogLog 操作命令<ul><li><code>pfadd key element [element …]</code><ul><li>pfadd 用于向HyperLogLog 添加元素,如果添加成功返回1</li></ul></li><li><code>pfcount key [key …]</code><ul><li>pfcount 用于计算一个或多个 HyperLogLog 的独立总数</li><li>统计结果会出现误差（0.81%的失误率）</li></ul></li><li><code>pfmerge destkey sourcekey [sourcekey ... ]</code><ul><li>pfmerge 可以求出多个HyperLogLog 的并集并赋值给destkey</li></ul></li></ul></li><li>HyperLogLog 数学原理：基于概率论中伯努利试验并结合了极大似然估算方法，并做了分桶优化</li><li>HyperLogLog 实现<ul><li>HyperLogLog 占据12KB (<code>占用内存为=16834 * 6 / 8 / 1024 = 12K</code>) 的大小<ul><li>共设有 16384 个桶，每个桶有 6 位，每个桶可以表达的最大数字是：<code>25+24+...+1 = 63</code> ，二进制为：<code>111 111</code></li></ul></li><li>对于命令：<code>pfadd key value</code><ul><li>在存入时，value 会被 hash 成64 位，即64 bit 的比特字符串，前14 位用来分桶（分了 16384 个桶），剩下50 位用来记录第一个1 出现的位置</li><li>分桶：假设一个字符串的前 14 位是：00 0000 0000 0010  ，其十进制值为 2。那么 value 对应转化后的值放到编号为 2 的桶</li><li>value：剩下的50位找第一个1出现的位置，转成二进制存入找到的桶中<ul><li>不同的 value，会被设置到不同桶中去</li><li>如果出现了在同一个桶的，即前 14 位值是一样的，但是后面出现 1 的位置不一样<ul><li>那么比较原来的index 是否比新index 大。是，则替换。否，则不变</li></ul></li></ul></li></ul></li><li>最终，一个 key 所对应的 16384 个桶都设置了很多的 value ，每个桶有一个 k_max<ul><li>调用 pfcount 时，按照调和平均数进行估算，同时加以偏差修正，便可以计算出 key 的设置了多少次 value，也就是统计值</li><li>同时，在具体的算法实现上，HLL 还有一个分阶段偏差修正算法</li></ul></li></ul></li><li>Redis 只提供了弱事务，不提供回滚（能不用尽量不用）<ul><li>命令本身有错，会回滚</li><li>但是命令没错，执行报错，不会回滚</li><li>watch 实现乐观锁机制</li></ul></li><li>Redis 事务和pipline的区别<ul><li>pipline是客户端行为，非原子，降低网络开销，某条命令失败后不影响后续命令的执行</li><li>事务是服务端行为<ul><li>用户执行MULTI 命令时，服务器会将对应这个用户的客户端对象设置为一个特殊的状态，在这个状态下后续用户执行的查询命令不会被真的执行，而是被服务器缓存起来，直到用户执行EXEC 命令为止，服务器会将这个用户对应的客户端对象中缓存的命令按照提交的顺序依次执行</li></ul></li></ul></li><li>尽量使用pipline，尽量不用事务，可以用LUA保证事务原子性<ul><li><code>EVAL script numkeys key [key ...] arg [arg ...]</code></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;HyperLogLog 操作命令
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;pfadd key element [element …]&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;pfadd 用于向HyperLogLog 添加元素,如果添加成功返回1&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-Cluster集群</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-cluster-ji-qun/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-cluster-ji-qun/</id>
    <published>2024-09-10T07:46:00.000Z</published>
    <updated>2024-09-10T07:47:17.163Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Cluster高可用集群</li><li>槽位&amp;选举&amp;脑裂</li><li>集群完整性&amp;奇数节点&amp;批量操作</li><li>通信</li></ul><hr><h2 id="Cluster-高可用集群">Cluster 高可用集群</h2><ul><li>配置文件<ul><li>cluster‐enabled yes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 开启集群模式</li><li>cluster‐config‐file nodes.conf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 集群节点信息文件，会自动生成</li><li>cluster‐node‐timeout 10000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  &nbsp; 超过1s则认为故障（防止网络抖动）</li><li>requirepass jxch&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 设置Redis密码</li><li>masterauth jxch&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; 设置Redis集群密码，与上面保持一致</li></ul></li><li>启动所有节点<ul><li>防火墙设置：避免gossip端口号通信问题（16379，默认端口+10000）</li></ul></li><li>创建集群<ul><li>redis‐cli ‐a jxch ‐‐cluster create ‐‐cluster‐replicas 1 IP:PORT …</li><li>‐‐cluster‐replicas 1 代表为每个主节点设置一个从节点</li></ul></li><li>连接任意节点<ul><li>redis‐cli ‐a jxch ‐c ‐h 192.168.0.61 ‐p 800*<ul><li>-a 服务器密码；-c 集群模式</li></ul></li><li>cluster info</li><li>cluster nodes</li></ul></li><li>关闭集群需要逐个关闭<ul><li>redis‐cli ‐a jxch ‐c ‐h 192.168.0.60 ‐p 800* shutdown</li><li>集群关闭后只需逐个重启Redis服务即可重启集群，无需重复创建集群<ul><li>各节点存有自动生成的集群元数据文件</li></ul></li></ul></li><li>扩容：add-node<ul><li>先改好配置文件，启动主从节点</li><li>添加主节点：第一个是新增节点，第二个是集群中任意已存在的节点<ul><li>redis‐cli ‐a jxch ‐‐cluster add‐node 192.168.0.61:8007 192.168.0.61:8001</li></ul></li><li>为新增的主节点分配hash槽：使用集群中任意其他主节点分配<ul><li>redis‐cli ‐a jxch ‐‐cluster reshard 192.168.0.61:8001</li></ul></li><li>添加从节点：第一个是新增节点，第二个是集群中任意已存在的节点<ul><li>redis‐cli ‐a jxch ‐‐cluster add‐node 192.168.0.61:8008 192.168.0.61:8001</li><li>各主节点平均分配，数据也同时迁移了</li></ul></li><li>为从节点分配主节点<ul><li>cluster nodes</li><li>cluster replicate 主节点ID</li></ul></li></ul></li><li>缩容：del‐node<ul><li>删除从节点<ul><li>redis‐cli ‐a jxch ‐‐cluster del‐node 192.168.0.61:8008 从节点ID</li></ul></li><li>转移主节点hash槽（同时迁移数据）<ul><li>redis‐cli ‐a jxch ‐‐cluster reshard 192.168.0.61:8007</li></ul></li><li>只能迁移到一个节点上，无法平均分配</li><li>删除主节点<ul><li>redis‐cli ‐a jxch ‐‐cluster del‐node 192.168.0.61:8007 主节点ID</li></ul></li></ul></li><li><img src="/static/IT/Redis/Redis-Cluster%E9%9B%86%E7%BE%A4-1.png" alt=""></li></ul><hr><h2 id="槽位-选举-脑裂">槽位&amp;选举&amp;脑裂</h2><ul><li>槽位<ul><li>Redis Cluster 将所有数据划分为 16384 个 slots(槽位)，每个节点负责其中一部分槽位。槽位的信息存储于每个节点中。客户端连接集群时也会得到一份集群的槽位配置信息并将其缓存在客户端本地。16384个槽位（2的14次方），具有更好的质数性质，减少哈希冲突的概率，不选65535是为了防止维持健康状态大量发送ping/pong导致网络拥堵</li><li>槽位定位算法<ul><li>HASH_SLOT = CRC16(key) mod 16384</li></ul></li><li>跳转重定位<ul><li>当客户端向一个错误的节点发出了指令，该节点会发现指令的 key 所在的槽位并不归自己管理，这时它会向客户端发送一个特殊的跳转指令携带目标操作的节点地址，告诉客户端去连这个节点去获取数据</li><li>客户端收到指令后除了跳转到正确的节点上去操作，还会同步更新纠正本地的槽位映射表缓存，后续所有 key 将使用新的槽位映射表</li></ul></li></ul></li><li>选举<ul><li>当slave发现自己的master变为FAIL状态时，便尝试进行Failover，以期成为新master。由于挂掉的master可能会有多个slave，从而存在多个slave竞争成为master节点的过程<ul><li>slave发现自己的master变为FAIL</li><li>将自己记录的集群currentEpoch加1，并广播FAILOVER_AUTH_REQUEST 信息</li><li>其他节点收到该信息，只有master响应，判断请求者的合法性，并发送FAILOVER_AUTH_ACK，对每个epoch只发送一次ack（只返回最先接受到的那个）</li><li>尝试failover的slave收集master返回的FAILOVER_AUTH_ACK</li><li>slave收到超过半数master的ack后变成新Master（所以至少需要三个主节点，如果只有两个，当其中一个挂了，只剩一个主节点是不能选举成功的）</li><li>slave广播Pong消息通知其他集群节点</li></ul></li><li>延时机制<ul><li>DELAY = 500ms + random(0 ~ 500ms) + SLAVE_RANK * 1000ms<ul><li>SLAVE_RANK表示此slave已经从master复制数据的总量的rank。Rank越小代表已复制的数据越新。这种方式下，持有最新数据的slave将会首先发起选举（理论上）</li></ul></li><li>一定的延迟确保我们等待FAIL状态在集群中传播，slave如果立即尝试选举，其它masters或许尚未意识到FAIL状态，可能会拒绝投票</li></ul></li></ul></li><li>脑裂<ul><li>网络问题导致集群其他节点无法连接某一主节点，但客户端可以连接此主节点</li><li>redis集群没有过半机制会有脑裂问题，导致多个主节点对外提供写服务，一旦网络恢复，会将其中一个主节点变为从节点，这时会有大量数据丢失</li><li>min‐replicas‐to‐write 1<ul><li>最少同步一台salve，再返回写成功</li><li>影响集群的可用性，如果集群内没有salve了，就无法对外提供服务了</li></ul></li></ul></li></ul><hr><h2 id="集群完整性-奇数节点-批量操作">集群完整性&amp;奇数节点&amp;批量操作</h2><ul><li>集群完整性<ul><li>cluster-require-full-coverage no</li><li>表示当负责一个插槽的主库下线且没有相应的从库进行故障恢复时，集群仍然可用</li></ul></li><li>奇数节点<ul><li>因为新master的选举需要大于半数的集群master节点同意才能选举成功，如果只有两个master节点，当其中一个挂了，是达不到选举新master的条件的</li><li>奇数个master节点可以在满足选举该条件的基础上节省一个节点，比如三个master节点和四个master节点的集群相比，大家如果都挂了一个master节点都能选举新master节点，如果都挂了两个master节点都没法选举新master节点了，所以奇数的master节点更多的是从节省机器资源角度出发说的</li></ul></li><li>批量操作：集群模式只支持所有key落在同一slot<ul><li>mset {user1}:1:name zhuge {user1}:1:age 18</li><li>在集群模式下只会将{}内的值作为key计算slot</li></ul></li></ul><hr><h2 id="通信">通信</h2><ul><li>网络抖动：cluster‐node‐timeout 10000<ul><li>避免网络抖动会导致主从频繁切换 （数据的重新复制）</li></ul></li><li>集群节点间的通信机制：gossip协议（ping，pong，meet，fail）<ul><li>meet：某个节点发送meet给新加入的节点，让新节点加入集群中，然后新节点就会开始与其他节点进行通信；</li><li>ping：每个节点都会频繁给其他节点发送ping，其中包含自己的状态还有自己维护的集群元数据，互相通过ping交换元数据 (类似自己感知到的集群节点增加和移除，hash slot信息等)</li><li>pong: 对ping和meet消息的返回，包含自己的状态和其他信息，也可以用于信息广播和更新</li><li>fail: 某个节点判断另一个节点fail之后，就发送fail给其他节点，通知其他节点，指定的节点宕机了</li></ul></li><li>每个节点每隔一段时间都会往另外几个节点发送ping消息，同时其他几点接收到ping消息之后返回pong消息</li><li>优点：元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续，打到所有节点上去更新，有一定的延时，降低了压力</li><li>缺点：元数据更新有延时可能导致集群的一些操作会有一些滞后</li><li>端口：自己提供服务的端口号+10000</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Cluster高可用集群&lt;/li&gt;
&lt;li&gt;槽位&amp;amp;选举&amp;amp;脑裂&lt;/li&gt;
&lt;li&gt;集群完整性&amp;amp;奇数节点&amp;amp;批量操作&lt;/li&gt;
&lt;li&gt;通信&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Cluster-高可用集群&quot;&gt;Cluster </summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-主从&amp;哨兵</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-zhu-cong-shao-bing/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-zhu-cong-shao-bing/</id>
    <published>2024-09-10T07:42:00.000Z</published>
    <updated>2024-09-10T07:44:56.061Z</updated>
    
    <content type="html"><![CDATA[<ul><li>主从架构</li><li>哨兵</li></ul><hr><h2 id="主从架构">主从架构</h2><ul><li>从节点配置文件<ul><li>replicaof 192.168.0.60 6379&nbsp;&nbsp;&nbsp; 从主节点IP-Port复制数据</li><li>replica‐read‐only yes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 从节点只读</li></ul></li><li>全量复制：bgsave，如果master收到了多个slave并发连接请求，它只会进行一次持久化<ul><li><img src="/static/IT/Redis/Redis-%E4%B8%BB%E4%BB%8E-%E5%93%A8%E5%85%B5-1.png" alt=""></li></ul></li><li>部分复制（断点续传，从节点断线重连）：offset<ul><li><img src="/static/IT/Redis/Redis-%E4%B8%BB%E4%BB%8E-%E5%93%A8%E5%85%B5-2.png" alt=""></li></ul></li><li>info命令查看主从节点信息</li><li>主从复制风暴：多个从节点同时复制主节点导致主节点压力过大<ul><li><img src="/static/IT/Redis/Redis-%E4%B8%BB%E4%BB%8E-%E5%93%A8%E5%85%B5-3.png" alt=""></li></ul></li></ul><hr><h2 id="哨兵">哨兵</h2><ul><li>哨兵（sentinel）：不提供读写服务，主要用来监控redis实例节点</li><li>配置文件<ul><li>sentinel monitor mymaster 192.168.0.60 6379 2<ul><li>sentinel总数/2 + 1 ，指明当有多少个sentinel认为一个master失效时，master才算真正失效，这里是：3/2 + 1 = 2</li><li>mymaster这个名字随便取，客户端访问时会用到</li></ul></li><li>sentinel集群都启动完毕后，会将哨兵集群的元数据信息写入所有sentinel的配置文件里去（追加在文件的最下面）</li><li>当redis主节点如果挂了，哨兵集群会重新选举出新的redis主节点，同时会修改所有sentinel节点配置文件的集群元数据信息，同时还会修改sentinel配置文件中的 mymaster</li><li>当挂了的redis实例再次启动时，哨兵集群就会根据集群元数据信息将它作为从节点加入集群</li></ul></li><li>Leader选举<ul><li>当一个master服务器被某sentinel视为下线状态后，该sentinel会与其他sentinel协商选出sentinel的leader进行故障转移工作</li><li>每个发现master服务器进入下线的sentinel都可以要求其他sentinel选自己为sentinel的leader，选举是先到先得</li><li>同时每个sentinel每次选举都会自增配置纪元（选举周期），每个纪元中只会选择一个sentinel的leader</li><li>如果所有超过一半的sentinel选举某sentinel作为leader。之后该sentinel进行故障转移操作，从存活的slave中选举出新的master，这个选举过程跟集群的master选举很类似（超过半数选举）</li><li>哨兵集群即使只有一个哨兵节点，redis的主从也能正常运行以及选举master，如果master挂了，那唯一的那个哨兵节点就是哨兵leader了，可以正常选举新master。不过为了高可用一般都推荐至少部署三个哨兵节点（奇数）</li><li>为什么推荐奇数个哨兵节点原理跟集群奇数个master节点类似</li></ul></li><li>缺点</li><li>主从切换时会发生访问瞬断</li><li>无法水平扩容<ul><li>只有一个提供服务的主节点</li><li>主节点内存不易过大，否则持久化会消耗大量性能</li><li><img src="/static/IT/Redis/Redis-%E4%B8%BB%E4%BB%8E-%E5%93%A8%E5%85%B5-4.png" alt=""></li></ul></li><li>client端第一次从哨兵找出redis的主节点，后续就直接访问redis的主节点，不会每次都通过sentinel代理访问redis的主节点，当redis的主节点发生变化，哨兵会第一时间感知到，并且将新的redis主节点通知给client端（这里面redis的client端一般都实现了订阅功能，订阅sentinel发布的节点变动消息）</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;主从架构&lt;/li&gt;
&lt;li&gt;哨兵&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;主从架构&quot;&gt;主从架构&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;从节点配置文件
&lt;ul&gt;
&lt;li&gt;replicaof 192.168.0.60 6379&amp;nbsp;&amp;nbsp;&amp;nbsp; 从主</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-开发规范与性能优化</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-kai-fa-gui-fan-yu-xing-neng-you-hua/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-kai-fa-gui-fan-yu-xing-neng-you-hua/</id>
    <published>2024-09-10T07:41:00.000Z</published>
    <updated>2024-09-10T07:41:31.233Z</updated>
    
    <content type="html"><![CDATA[<ul><li>键值设计</li><li>命令使用</li><li>客户端使用</li><li>系统内核参数优化</li></ul><hr><h2 id="键值设计">键值设计</h2><ul><li>key名设计<ul><li>可读性和可管理性</li><li>简洁性</li><li>不要包含特殊字符</li></ul></li><li>value设计：拒绝 bigkey (防止网卡流量、慢查询)<ul><li>字符串类型：它的big体现在单个value值很大，一般认为超过10KB就是bigkey</li><li>非字符串类型：哈希、列表、集合、有序集合，它们的big体现在元素个数太多</li><li>string类型控制在10KB以内，hash、list、set、zset元素个数不要超过5000</li></ul></li><li>非字符串的bigkey，不要使用del删除，使用hscan、sscan、zscan方式渐进式删除<ul><li>防止bigkey过期时间自动删除问题</li></ul></li><li>bigkey的危害<ul><li>导致redis阻塞</li><li>网络拥塞</li><li>过期删除：阻塞Redis<ul><li>使用过期异步删除 <code>lazyfree-lazy-expire&nbsp;yes</code></li></ul></li></ul></li><li>bigkey的产生：程序设计不当&amp;数据规模预料不清楚<ul><li>社交类：粉丝列表，如果某些明星或者大v不精心设计下，必是bigkey</li><li>统计类：例如按天存储某项功能或者网站的用户集合，除非没几个人用，否则必是bigkey</li><li>缓存类：将数据从数据库load出来序列化放到Redis里，这个方式非常常用，但有两个地方需要注意<ul><li>是不是有必要把所有字段都缓存</li><li>有没有相关关联的数据</li></ul></li></ul></li><li>如何优化bigkey<ul><li>拆<ul><li>big&nbsp;list：&nbsp;list1、list2、…listN</li><li>big&nbsp;hash：数据分段存储</li><li>如果bigkey不可避免，也要思考一下要不要每次把所有元素都取出来，删除也是一样</li></ul></li><li>选择适合的数据类型</li><li>控制key的生命周期，redis不是垃圾桶<ul><li>建议使用expire设置过期时间（打散过期时间）</li></ul></li></ul></li></ul><hr><h2 id="命令使用">命令使用</h2><ul><li>O(N)命令关注N的数量：可以使用hscan、sscan、zscan代替</li><li>禁用命令（rename）：keys、flushall、flushdb，使用scan的方式渐进式处理</li><li>合理使用select：redis的多数据库较弱，使用数字进行区分，很多客户端支持较差，同时多业务用多数据库实际还是单线程处理，会有干扰</li><li>使用批量操作提高效率：控制一次批量操作的元素个数 (例如500以内，实际也和元素字节数有关)<ul><li>原生命令：例如mget、mset</li><li>非原生命令：可以使用pipeline提高效率</li></ul></li><li>Redis事务功能较弱，不建议过多使用，可以用lua替代：只提供了弱事务，不提供回滚（能不用尽量不用）<ul><li>命令本身有错，会回滚</li><li>命令没错，执行报错，不会回滚</li><li>watch 实现乐观锁机制</li></ul></li><li>事务和pipline的区别<ul><li>pipline是客户端行为，非原子，降低网络开销，某条命令失败后不影响后续命令的执行</li><li>事务是服务端行为</li></ul></li><li>尽量使用pipline，尽量不用事务，可以用LUA保证事务原子性<ul><li><code>EVAL script numkeys key [key ...] arg [arg ...]</code></li></ul></li></ul><hr><h2 id="客户端使用">客户端使用</h2><ul><li>避免多个应用使用一个Redis实例</li><li>使用带有连接池的客户端（JedisPool#getResource），可以有效控制连接，同时提高效率<ul><li>maxTotal：最大连接数<ul><li>业务希望Redis并发量</li><li>客户端执行命令时间</li><li>Redis资源：例如&nbsp;nodes(例如应用个数)&nbsp;*&nbsp;maxTotal&nbsp;是不能超过redis的最大连接数maxclients</li><li>资源开销：例如虽然希望控制空闲连接 (连接池此刻可马上使用的连接)，但是不希望因为连接池的频繁释放创建连接造成不必靠开销</li></ul></li><li>maxIdle：maxIdle实际上才是业务需要的最大连接数，maxTotal是为了给出余量，所以maxIdle不要设置过小，否则会有new&nbsp;Jedis(新连接)开销<ul><li>连接池的最佳性能是maxTotal&nbsp;=&nbsp;maxIdle，这样就避免连接池伸缩带来的性能干扰</li><li>并发量不大或者maxTotal设置过高，会导致不必要的连接资源浪费</li><li>maxIdle可以设置为业务期望QPS计算出来的理论连接数，maxTotal可以再放大一倍</li></ul></li><li>minIdle（最小空闲连接数），与其说是最小空闲连接数，不如说是"至少需要保持的空闲连接数"，在使用连接的过程中，如果连接数超过了minIdle，那么继续建立连接，如果超过了maxIdle，当超过的连接执行完业务后会慢慢被移出连接池释放掉<ul><li>如果系统启动完马上就会有很多的请求过来，那么可以给redis连接池做预热，比如快速的创建一些redis连接，执行简单命令，类似ping()，快速的将连接池里的空闲连接提升到minIdle的数量</li></ul></li></ul></li><li>高并发下建议客户端添加熔断功能 (例如sentinel、hystrix)</li><li>设置合理的密码，如有必要可以使用SSL加密访问</li><li>Redis对于过期键有三种清除策略<ul><li>被动删除：当读/写一个已经过期的key时，会触发惰性删除策略，直接删除掉这个过期 key</li><li>主动删除：由于惰性删除策略无法保证冷数据被及时删掉，所以Redis会定期主动淘汰一批已过期的key<ul><li>针对设置了过期时间的key做处理<ul><li>volatile-ttl：在筛选时，会针对设置了过期时间的键值对，根据过期时间的先后进行删除，越早过期的越先被删除</li><li>volatile-random：就像它的名称一样，在设置了过期时间的键值对中，进行随机删除</li><li>volatile-lru（推荐）：会使用&nbsp;LRU&nbsp;算法筛选设置了过期时间的键值对删除</li><li>volatile-lfu：会使用&nbsp;LFU&nbsp;算法筛选设置了过期时间的键值对删除</li></ul></li><li>针对所有的key做处理<ul><li>allkeys-random：从所有键值对中随机选择并删除数据</li><li>allkeys-lru：使用&nbsp;LRU&nbsp;算法在所有数据中进行筛选删除</li><li>allkeys-lfu：使用&nbsp;LFU&nbsp;算法在所有数据中进行筛选删除</li></ul></li><li>不处理<ul><li>noeviction（默认）：不会剔除任何数据，拒绝所有写入操作并返回客户端错误信息"(error)&nbsp;OOM&nbsp;command&nbsp;not&nbsp;allowed&nbsp;when&nbsp;used&nbsp;memory"，此时Redis只响应读操作</li></ul></li><li>淘汰算法<ul><li>LRU&nbsp;算法（Least&nbsp;Recently&nbsp;Used，最近最少使用）：淘汰很久没被访问过的数据，以最近一次访问时间作为参考</li><li>LFU&nbsp;算法（Least&nbsp;Frequently&nbsp;Used，最不经常使用）：淘汰最近一段时间被访问次数最少的数据，以次数作为参考</li><li>当存在热点数据时，LRU的效率很好，但偶发性的、周期性的批量操作会导致LRU命中率急剧下降，缓存污染情况比较严重。这时使用LFU可能更好点</li><li>如果不设置最大内存，当&nbsp;Redis&nbsp;内存超出物理内存限制时，内存的数据会开始和磁盘产生频繁的交换&nbsp;(swap)，会让&nbsp;Redis&nbsp;的性能急剧下降</li><li>当Redis运行在主从模式时，只有主结点才会执行过期删除策略，然后把删除操作”del&nbsp;key”同步到从结点删除数据</li></ul></li></ul></li><li>当前已用内存超过maxmemory限定时，触发主动清理策略</li></ul></li></ul><hr><h2 id="系统内核参数优化">系统内核参数优化</h2><ul><li><code>vm.swapiness</code><ul><li>在Linux中，并不是要等到所有物理内存都使用完才会使用到swap，系统参数swppiness会决定操作系统使用swap的倾向程度</li><li>swappiness的取值范围是0~100，swappiness的值越大，说明操作系统可能使用swap的概率越高</li><li>一般需要保证redis不会被kill掉<ul><li>linux内核版本&lt;3.5，那么swapiness设置为0，这样系统宁愿swap也不会oom&nbsp;killer</li><li>linux内核版本&gt;=3.5，那么swapiness设置为1，这样系统宁愿swap也不会oom&nbsp;killer</li></ul></li><li>OOM&nbsp;killer&nbsp;机制是指Linux操作系统发现可用内存不足时，强制杀死一些用户进程（非内核进程），来保证系统有足够的可用内存进行分配</li><li>配置<ul><li><code>cat&nbsp;/proc/version&nbsp;#查看linux内核版本</code></li><li><code>echo&nbsp;1&nbsp;&gt;&nbsp;/proc/sys/vm/swappiness</code></li><li><code>echo&nbsp;vm.swapiness=1&nbsp;&gt;&gt;&nbsp;/etc/sysctl.conf</code></li></ul></li></ul></li><li><code>vm.overcommit_memory</code> (默认0)<ul><li>0：表示内核将检查是否有足够的可用物理内存 (实际不一定用满) 供应用进程使用<ul><li>如果有足够的可用物理内存，内存申请允许</li><li>否则，内存申请失败，并把错误返回给应用进程</li></ul></li><li>1：表示内核允许分配所有的物理内存，而不管当前的内存状态如何</li><li>如果是0的话，可能导致类似fork等操作执行失败，申请不到足够的内存空间<ul><li>Redis建议把这个值设置为1，就是为了让fork操作能够在低内存下也执行成功</li></ul></li></ul></li><li>合理设置文件句柄数<ul><li>操作系统进程试图打开一个文件(或者叫句柄)，但是现在进程打开的句柄数已经达到了上限，继续打开会报错：“Too&nbsp;many&nbsp;open&nbsp;files”</li><li><code>ulimit&nbsp;‐a</code> 查看系统文件句柄数，看open&nbsp;files那项</li><li><code>ulimit&nbsp;‐n&nbsp;65535</code> 设置系统文件句柄数</li></ul></li><li>慢查询日志：slowlog</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">config get slow*  <span class="token comment">#查询有关慢日志的配置信息</span>config <span class="token builtin class-name">set</span> slowlog‐log‐slower‐than <span class="token number">20000</span> <span class="token comment">#设置慢日志使时间阈值,单位微秒，此处为20毫秒，即超过20毫秒的操作都会记录下来，生产环境建议设置1000，也就是1ms，这样理论上redis并发至少达到1000，如果要求单机并发达到1万以上，这个值可以设置为100</span>config <span class="token builtin class-name">set</span> slowlog‐max‐len <span class="token number">1024</span> <span class="token comment">#设置慢日志记录保存数量，如果保存数量已满，会删除最早的记录，最新的记录追加进来。记录慢查询日志时Redis会对长命令做截断操作，并不会占用大量内存，建议设置稍大些，防止丢失日志</span>config rewrite  <span class="token comment">#将服务器当前所使用的配置保存到 redis.conf</span>slowlog len     <span class="token comment">#获取慢查询日志列表的当前长度</span>slowlog get <span class="token number">5</span>   <span class="token comment">#获取最新的5条慢查询日志。慢查询日志由四个属性组成：标识ID，发生时间戳，命令耗时，执行命令和参数</span>slowlog reset   <span class="token comment">#重置慢查询日志</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;键值设计&lt;/li&gt;
&lt;li&gt;命令使用&lt;/li&gt;
&lt;li&gt;客户端使用&lt;/li&gt;
&lt;li&gt;系统内核参数优化&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;键值设计&quot;&gt;键值设计&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;key名设计
&lt;ul&gt;
&lt;li&gt;可读性和可管理性&lt;/li&gt;
</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-核心数据结构&amp;IO多路复用</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-he-xin-shu-ju-jie-gou-io-duo-lu-fu-yong/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-he-xin-shu-ju-jie-gou-io-duo-lu-fu-yong/</id>
    <published>2024-09-10T07:37:00.000Z</published>
    <updated>2024-09-10T07:39:21.817Z</updated>
    
    <content type="html"><![CDATA[<ul><li>五种数据结构：string hash list set zset<ul><li>应用场景</li></ul></li><li>RedisTemplate 默认采用的是JDK的序列化，直接看的话key会有乱码，建议用 StringRedisTemplate</li><li>IO多路复用</li><li>SCAN</li></ul><pre class="line-numbers language-java" data-language="java"><code class="language-java">redisTemplate<span class="token punctuation">.</span><span class="token function">opsForValue</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment">//操作字符串</span>redisTemplate<span class="token punctuation">.</span><span class="token function">opsForHash</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">//操作hash</span>redisTemplate<span class="token punctuation">.</span><span class="token function">opsForList</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">//操作list</span>redisTemplate<span class="token punctuation">.</span><span class="token function">opsForSet</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>     <span class="token comment">//操作set</span>redisTemplate<span class="token punctuation">.</span><span class="token function">opsForZSet</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token comment">//操作有序set</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="数据结构">数据结构</h2><table><thead><tr><th>命令</th><th>参数</th><th>含义</th><th>代码</th></tr></thead><tbody><tr><td><em><strong>STRING</strong></em></td><td><strong>==STRING==</strong></td><td></td><td>RedisTemplate rt</td></tr><tr><td>SET</td><td>key&nbsp; value</td><td>存入字符串键值对</td><td>rt.opsForValue().set(“key”,“value”)</td></tr><tr><td>MSET</td><td>key&nbsp; value &nbsp;&nbsp;[key value …]</td><td>批量存入字符串键值对</td><td></td></tr><tr><td>SETNX</td><td>key&nbsp; value</td><td>存入一个不存在的字符串键值对</td><td></td></tr><tr><td>GET</td><td>key</td><td>获取字符串键值</td><td>rt.opsForValue().get(“key”)</td></tr><tr><td>MGET</td><td>key&nbsp; [key …]</td><td>批量获取字符串键值</td><td></td></tr><tr><td>DEL</td><td>key&nbsp; [key …]</td><td>删除一个键</td><td>rt.delete(“key”)</td></tr><tr><td>EXPIRE</td><td>key&nbsp; seconds</td><td>设置一个键的过期时间(秒)</td><td></td></tr><tr><td>INCR</td><td>key</td><td>原子 +1</td><td></td></tr><tr><td>DECR</td><td>key</td><td>原子 -1</td><td></td></tr><tr><td>INCRBY</td><td>key&nbsp; num</td><td>原子 +num</td><td></td></tr><tr><td>DECRBY</td><td>key&nbsp; num</td><td>原子 -num</td><td></td></tr><tr><td>STRLEN</td><td>key</td><td></td><td>rt.opsForValue().size(“key”)</td></tr><tr><td>GETSET</td><td>key value</td><td></td><td>rt.opsForValue().getAndSet(“key”,“value”)</td></tr><tr><td>GETRANGE</td><td>key start end</td><td></td><td>rt.opsForValue().get(“key”,start,end)</td></tr><tr><td>APPEND</td><td>key value</td><td></td><td>rt.opsForValue().append(“key”,“value”)</td></tr><tr><td><em><strong>HASH</strong></em></td><td><strong>==HASH==</strong></td><td></td><td></td></tr><tr><td>HSET</td><td>key&nbsp; field&nbsp; value</td><td>存入一个哈希表key的键值</td><td>rt.opsForHash().put(“key”,“field”,“value”)</td></tr><tr><td>HSETNX</td><td>key&nbsp; field&nbsp; value</td><td>存入一个不存在的哈希表key的键值</td><td>rt.opsForHash().putIfAbsent(“key”,“field”,“value”)</td></tr><tr><td>HMSET</td><td>key&nbsp; field&nbsp; value [field value …]</td><td>在一个哈希表key中存入多个键值对</td><td>rt.opsForHash().putAll(“key”,map)</td></tr><tr><td>HGET</td><td>key&nbsp; field</td><td>获取哈希表key对应的field键值</td><td>rt.opsForHash().get(“key”,“field”)</td></tr><tr><td>HMGET</td><td>key&nbsp; field&nbsp; [field …]</td><td>批量获取哈希表key中多个field键值</td><td>rt.opsForHash().multiGet(“key”,fieldList)</td></tr><tr><td>HDEL</td><td>key&nbsp; field&nbsp; [field …]</td><td>删除哈希表key中的field键值</td><td>rt.opsForHash().delete(“key”,“field1”,“field2”)</td></tr><tr><td>HLEN</td><td>key</td><td>返回哈希表key中field的数量</td><td></td></tr><tr><td>HGETALL</td><td>key</td><td>返回哈希表key中所有的键值</td><td>rt.opsForHash().entries(“key”)</td></tr><tr><td>HINCRBY</td><td>key&nbsp; field&nbsp; num</td><td>为哈希表key中field键的值原子 +num</td><td></td></tr><tr><td>HEXISTS</td><td>key field</td><td></td><td>rt.opsForHash().hasKey(“key”,“field”)</td></tr><tr><td>HVALS</td><td>key</td><td></td><td>rt.opsForHash().values(“key”)</td></tr><tr><td>HKEYS</td><td>key</td><td></td><td>rt.opsForHash().keys(“key”)</td></tr><tr><td><em><strong>LIST</strong></em></td><td><strong>==LIST==</strong></td><td></td><td></td></tr><tr><td>LPUSH</td><td>key&nbsp; value [value …]</td><td>插入到列表的表头(最左边)</td><td>rt.opsForList().leftPush(“key”,“value”)<br><br>rt.opsForList().leftPushAll(“key”,valueList)</td></tr><tr><td>RPUSH</td><td>key&nbsp; value [value …]</td><td>插入到列表的表尾(最右边)</td><td>rt.opsForList().rightPush(“key”,“value”)<br><br>rt.opsForList().rightPushAll(“key”,valueList)</td></tr><tr><td>LPOP</td><td>key</td><td>移除并返回key列表的头元素</td><td>rt.opsForList().leftPop(“key”)</td></tr><tr><td>RPOP</td><td>key</td><td>移除并返回key列表的尾元素</td><td>rt.opsForList().rightPop(“key”)</td></tr><tr><td>LRANGE</td><td>key&nbsp; start&nbsp; stop</td><td>返回列表key中 start-stop 区间内的元素</td><td>rt.opsForList().range(“key”,start,end)</td></tr><tr><td>BLPOP</td><td>key&nbsp; [key …]&nbsp; timeout</td><td>从列表表头弹出一个元素，若没有则阻塞等待timeout秒，如果timeout=0，则一直等待</td><td></td></tr><tr><td>BRPOP</td><td>key&nbsp; [key …]&nbsp; timeout</td><td>从列表表尾弹出一个元素，若没有则阻塞等待timeout秒，如果timeout=0，则一直等待</td><td></td></tr><tr><td>LINDEX</td><td>key index</td><td></td><td>rt.opsForList().index(“key”, index)</td></tr><tr><td>LLEN</td><td>key</td><td></td><td>rt.opsForList().size(“key”)</td></tr><tr><td>LPUSHX</td><td>list node</td><td></td><td>rt.opsForList().leftPushIfPresent(“list”,“node”)</td></tr><tr><td>RPUSHX</td><td>list node</td><td></td><td>rt.opsForList().rightPushIfPresent(“list”,“node”)</td></tr><tr><td>LREM</td><td>list count value</td><td></td><td>rt.opsForList().remove(“list”,count,“value”)</td></tr><tr><td>LSET</td><td>key index value</td><td></td><td>rt.opsForList().set(“list”,index,“value”)</td></tr><tr><td><em><strong>SET</strong></em></td><td><strong>==SET==</strong></td><td></td><td></td></tr><tr><td>SADD</td><td>key&nbsp; member&nbsp; [member …]</td><td>往集合key中存入元素，元素存在则忽略</td><td>rt.boundSetOps(“key”).add(“member1”,…)<br><br>rt.opsForSet().add(“key”, set)</td></tr><tr><td>SREM</td><td>key&nbsp; member&nbsp; [member …]</td><td>从集合key中删除元素</td><td>rt.opsForSet().remove(“key”,“member1”,…)</td></tr><tr><td>SMEMBERS</td><td>key</td><td>获取集合key中所有元素</td><td>rt.opsForSet().members(“key”)</td></tr><tr><td>SCARD</td><td>key</td><td>获取集合key的元素个数</td><td>rt.opsForSet().size(“key”)</td></tr><tr><td>SISMEMBER</td><td>key&nbsp; member</td><td>判断member元素是否存在于集合key中</td><td>rt.opsForSet().isMember(“key”,“member”)</td></tr><tr><td>SRANDMEMBER</td><td>key&nbsp; [count]</td><td>从集合key中选出count个元素，元素不从key中删除</td><td>rt.opsForSet().randomMember(“key”,count)</td></tr><tr><td>SPOP</td><td>key&nbsp; [count]</td><td>从集合key中选出count个元素，元素从key中删除</td><td>rt.opsForSet().pop(“key”)</td></tr><tr><td>SINTER</td><td>key&nbsp; [key …]</td><td>交集运算</td><td>rt.opsForSet().intersect(“key1”,“key2”)</td></tr><tr><td>SINTERSTORE</td><td>destination&nbsp; key&nbsp; [key …]</td><td>将交集结果存入新集合destination中</td><td>rt.opsForSet().intersectAndStore(“key1”,“key2”,“des”)</td></tr><tr><td>SUNION</td><td>key&nbsp; [key …]</td><td>并集运算</td><td>rt.opsForSet().union(“key1”,“key2”)</td></tr><tr><td>SUNIONSTORE</td><td>destination&nbsp; key&nbsp; [key …]</td><td>将并集结果存入新集合destination中</td><td>rt.opsForSet().unionAndStore(“key1”,“key2”,“des”)</td></tr><tr><td>SDIFF</td><td>key&nbsp; [key …]</td><td>差集运算</td><td>rt.opsForSet().difference(“key1”,“key2”)</td></tr><tr><td>SDIFFSTORE</td><td>destination&nbsp; key&nbsp; [key …]</td><td>将差集结果存入新集合destination中</td><td>rt.opsForSet().differenceAndStore(“key1”,“key2”,“des”)</td></tr><tr><td><em><strong>ZSET</strong></em></td><td><strong>==ZSET==</strong></td><td></td><td></td></tr><tr><td>ZADD</td><td>key score member [[score member]…]</td><td>往有序集合key中加入带分值元素</td><td></td></tr><tr><td>ZREM</td><td>key member [member …]</td><td>从有序集合key中删除元素</td><td></td></tr><tr><td>ZSCORE</td><td>key member</td><td>返回有序集合key中元素member的分值</td><td></td></tr><tr><td>ZINCRBY</td><td>key num member</td><td>为有序集合key中元素member的分值原子 +num</td><td></td></tr><tr><td>ZCARD</td><td>key</td><td>返回有序集合key中元素个数</td><td></td></tr><tr><td>ZRANGE</td><td>key start stop [WITHSCORES]</td><td>正序获取有序集合key从start下标到stop下标的元素</td><td></td></tr><tr><td>ZREVRANGE</td><td>key start stop [WITHSCORES]</td><td>倒序获取有序集合key从start下标到stop下标的元素</td><td></td></tr><tr><td>ZUNIONSTORE</td><td>destkey numkeys key [key …]</td><td>并集计算</td><td></td></tr><tr><td>ZINTERSTORE</td><td>destkey numkeys key [key …]</td><td>交集计算</td><td></td></tr></tbody></table><hr><h2 id="应用场景">应用场景</h2><ul><li>STRING 应用场景<ul><li>单值缓存 | 对象缓存（Session共享）</li><li>分布式锁：SET product:10001 true ex 10 nx</li><li>计数器（分布式唯一ID，客户端批量获取，缺点：断电导致重复ID）</li></ul></li><li>HASH 应用场景<ul><li>对象缓存（大KEY问题）</li><li>购物车：用户id为key；商品id为field；商品数量为value</li><li>优点<ul><li>同类数据归类整合储存，方便数据管理</li><li>相比string操作消耗内存与cpu更小</li><li>相比string储存更节省空间</li></ul></li><li>缺点<ul><li>过期功能不能使用在field上，只能用在key上</li><li>Redis集群架构下不适合大规模使用</li></ul></li></ul></li><li>LIST 应用场景<ul><li>社交平台发文（时间顺序展示）</li><li>数据结构<ul><li>栈 = LPUSH + LPOP</li><li>队列 = LPUSH + RPOP</li><li>阻塞队列 = LPUSH + BRPOP</li></ul></li><li><img src="/static/IT/Redis/Redis-%E6%A0%B8%E5%BF%83%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8-1.png" alt=""></li></ul></li><li>SET 应用场景<ul><li>抽奖：SADD / SRANDMEMBER / SPOP / SMEMBERS</li><li>点赞：SADD / SREM / SISMEMBER / SMEMBERS / SCARD / SINTER</li><li>关注模型<ul><li>共同关注：SINTER</li><li>我关注的人也关注他：SISMEMBER</li><li>可能认识的人（朋友的朋友 - 我的朋友）：SDIFF</li></ul></li><li>商品筛选（多查询条件取交集）：SINTER</li></ul></li><li>ZSET 应用场景<ul><li>新闻排行榜<ul><li>点击：ZINCRBY&nbsp; hotNews:20190819&nbsp; 1 xxx文章</li><li>前十：ZREVRANGE&nbsp; hotNews:20190819&nbsp; 0&nbsp; 9&nbsp; WITHSCORES</li><li>七日榜单：ZUNIONSTORE hotNews:20190813-20190819&nbsp; 7</li><li>七日前十：ZREVRANGE hotNews:20190813-20190819&nbsp; 0&nbsp; 9&nbsp; WITHSCORES</li></ul></li><li><img src="/static/IT/Redis/Redis-%E6%A0%B8%E5%BF%83%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-IO%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8-2.png" alt=""></li></ul></li></ul><hr><h2 id="IO多路复用">IO多路复用</h2><ul><li>单线程：<ul><li>Redis 的单线程主要是指 Redis 的网络 IO 和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程。但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的</li><li>因为它所有的数据都在内存中，所有的运算都是内存级别的运算，而且单线程避免了多线程的切换性能损耗问题。正因为 Redis 是单线程，所以要小心使用耗时的指令(比如keys)，一不小心就可能会导致 Redis 卡顿</li></ul></li><li>Redis的IO多路复用：<ul><li>redis利用epoll来实现IO多路复用，将连接信息和事件放到队列中，依次放到文件事件分派器，事件分派器将事件分发给事件处理器</li></ul></li><li>查看redis支持的最大连接数，在redis.conf文件中可修改：<code>CONFIG GET maxclients</code></li><li>SCAN<ul><li>KEYS是全量遍历，当数据量大时，性能很差，应用 SCAN 取代（手动分段遍历，直到游标为0停止遍历）</li><li><code>SCAN 0 match jxch** count 100</code><ul><li>以0为游标，分段扫描以jxch开头的key，每次返回100个，然后以返回值为游标，继续扫描，直到返回的游标为0</li><li>注意，100是个参考值，实际返回的是接近100的数目，可能多也可能少（取决于HASH桶中有几个值，因为Redis的存储结构是哈希表）</li></ul></li><li>如果在scan的过程中如果有键的变化（增加、 删除、 修改） ，那么遍历效果可能会碰到如下问题<ul><li>新增的键可能没有遍历到（不会扫描已经遍历过的HASH桶）</li><li>遍历出了重复的键（发生了RE-HASH）</li></ul></li><li>SCAN并不能保证完整的遍历出来所有的键， 这些是我们在开发时需要考虑的，但在追求性能的情况下，往往并不要求一致性</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;五种数据结构：string hash list set zset
&lt;ul&gt;
&lt;li&gt;应用场景&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RedisTemplate 默认采用的是JDK的序列化，直接看的话key会有乱码，建议用 StringRedisTemplat</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-高并发缓存架构</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-gao-bing-fa-huan-cun-jia-gou/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-gao-bing-fa-huan-cun-jia-gou/</id>
    <published>2024-09-10T07:32:00.000Z</published>
    <updated>2024-09-10T07:33:31.246Z</updated>
    
    <content type="html"><![CDATA[<ul><li>缓存击穿&amp;缓存穿透&amp;缓存雪崩</li><li>热点KEY的缓存重建</li><li>双写不一致&amp;读写并发不一致</li><li>写多读多</li></ul><hr><ul><li>缓存击穿：缓存集体失效导致某一时刻大量请求打到数据库<ul><li>随机缓存失效时间</li><li>读延期：阻塞该数据的访问，直到缓存重新建立</li></ul></li><li>缓存穿透：请求数据库不存在的数据（缓存肯定也不存在）<ul><li>恶意攻击，并发请求不存在的商品ID，导致数据库宕机<ul><li>布隆过滤器：返回不存在则一定不存在，只能添加不能更新（定期重新初始化）</li><li>缓存空对象并设置失效时间（避免缓存被长时间占满）：避免布隆过滤器哈希碰撞</li></ul></li></ul></li><li>缓存雪崩：缓存层宕机或无法正常提供服务（超大并发）<ul><li>热点中的热点导致超大并发请求：本地缓存（多级缓存）<ul><li>热点探测系统维护缓存内容并通知更新本地缓存 ：分布式大数据实时计算</li><li>MQ或ZK通知更新本地缓存：数据短暂不一致，增加系统复杂度</li></ul></li><li>服务层限流熔断降级</li><li>提高缓存层可用性</li><li>提前压测</li></ul></li></ul><hr><ul><li>热点KEY的缓存重建  &amp; 冷门商品突然变热：大量线程同时重建缓存导致数据库宕机<ul><li>商品分布式锁双重检测：保证重建缓存时只查一次库<ul><li>使用trylock避免双重检测时长期等待</li></ul></li></ul></li><li>缓存数据不一致：ABA问题<ul><li>缓存与数据库双写不一致<ul><li><img src="/static/IT/Redis/Redis-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84-1.png" alt=""></li></ul></li><li>读写并发不一致（发生概率偏低，因为查的速度一般快于写）<ul><li><img src="/static/IT/Redis/Redis-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84-2.png" alt=""></li></ul></li><li>解决方案<ul><li>写库和查库（重建缓存）之前使用分布式读写锁</li><li>延时双删：写库后延时删两次缓存，不推荐用这种方式解决小概率事件浪费大量性能</li><li>canal：模拟MySQL从库监听binlog，更新缓存，引入中间件增加了系统复杂性</li></ul></li></ul></li><li>写多读多场景<ul><li>MQ直接操作数据库</li><li>缓存作为主存储，异步同步到数据库</li></ul></li><li>放入缓存的数据应该是对实时性、一致性要求不是很高的数据。切记不要为了用缓存，同时又要保证绝对的一致性做大量的过度设计和控制，增加系统复杂性！</li></ul><p><img src="/static/IT/Redis/Redis-%E9%AB%98%E5%B9%B6%E5%8F%91%E7%BC%93%E5%AD%98%E6%9E%B6%E6%9E%84-3.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;缓存击穿&amp;amp;缓存穿透&amp;amp;缓存雪崩&lt;/li&gt;
&lt;li&gt;热点KEY的缓存重建&lt;/li&gt;
&lt;li&gt;双写不一致&amp;amp;读写并发不一致&lt;/li&gt;
&lt;li&gt;写多读多&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;缓存击穿：缓存集体失效导致某一时刻大量请求打</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-底层数据结构</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-di-ceng-shu-ju-jie-gou/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-di-ceng-shu-ju-jie-gou/</id>
    <published>2024-09-10T07:16:00.000Z</published>
    <updated>2024-09-10T07:22:11.626Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Redis 应用场景</li><li>Redis 底层核心数据结构</li><li>Redis 渐进式rehash及动态扩容机制</li><li>BitMap 海量数据统计</li><li>GeoHash 经纬度计算</li></ul><hr><h2 id="Redis-应用场景">Redis 应用场景</h2><ul><li>缓存</li><li>计数器：可以对 String 进行自增自减运算，从而实现计数器功能。Redis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量</li><li>分布式ID生成（宕机后ID重复）：利用自增特性，一次请求一个大一点的步长如 incr 2000 ,缓存在本地使用，用完再请求</li><li>海量数据统计 - 位图（bitmap）：:存储是否参过某次活动，是否已读谋篇文章，用户是否为会员， 日活统计。</li><li>会话缓存：可以使用 Redis 来统一存储多台应用服务器的会话信息。当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性</li><li>分布式队列/阻塞队列：List 是一个双向链表，可以通过 lpush/rpush 和 rpop/lpop 写入和读取消息。可以通过使用brpop/blpop 来实现阻塞队列</li><li>分布式锁实现：在分布式场景下，无法使用基于进程的锁来对多个节点上的进程进行同步。可以使用 Redis 自带的 SETNX 命令实现分布式锁</li><li>热点数据存储：最新评论，最新文章列表，使用list 存储，ltrim取出热点数据，删除老数据</li><li>社交类需求：Set 可以实现交集，从而实现共同好友等功能，Set通过求差集，可以进行好友推荐，文章推荐</li><li>排行榜：sorted_set可以实现有序性操作，从而实现排行榜等功能</li><li>延迟队列：使用sorted_set，使用 【当前时间戳 + 需要延迟的时长】做score, 消息内容作为元素，调用zadd来生产消息，消费者使用zrangbyscore获取当前时间之前的数据做轮询处理。消费完再删除任务 <code>rem&nbsp; key&nbsp; member</code></li></ul><hr><h2 id="Redis-底层核心数据结构">Redis 底层核心数据结构</h2><p><img src="/static/IT/Redis/Redis-%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-1.png" alt="Redis 底层核心数据结构"></p><h3 id="String">String</h3><ul><li>Redis 的字符串类型使用&nbsp;<code>SDS</code>&nbsp;(Simple Dynamic String) 作为其底层实现<ul><li>预分配策略<ul><li>减少内存重新分配次数，提高性能，SDS 会在字符串增长时预先分配多余的空间</li></ul></li><li>空间缩减策略<ul><li>当字符串缩短或者释放时，SDS 并不会立即释放多余的空间，这样可以保留一些预分配的内存以供将来使用</li></ul></li><li>对于短字符串，SDS 会预先分配一小段过量的空间</li><li>对于长字符串，预分配的空间比例会减少，但依然会多分配一定比例的内存</li></ul></li></ul><p><img src="/static/IT/Redis/Redis-%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-2.png" alt=""><br><img src="/static/IT/Redis/Redis-%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-3.png" alt=""></p><h3 id="List">List</h3><ul><li>采用 quicklist（双端链表） 和 ziplist 作为 List 的底层实现</li><li>list-max-ziplist-size&nbsp; -2<ul><li>单个ziplist节点最大能存储&nbsp; 8kb&nbsp; ,超过则进行分裂,将数据存储在新的ziplist节点中</li></ul></li><li>list-compress-depth&nbsp; 1<ul><li>0 代表所有节点，都不进行压缩，1， 代表从头节点往后走一个，尾节点往前走一个不用压缩，其他的全部压缩，2，3，4 … 以此类推</li></ul></li><li>ziplist<ul><li>紧凑存储<ul><li><code>ziplist</code>&nbsp;通过连续分配一段内存块，紧凑存储多个元素，以减少内存开销</li><li>每个元素包括实际数据以及前序长度和当前长度的元数据，而非像&nbsp;<code>linkedlist</code>&nbsp;一样每个元素都包含独立的指针</li></ul></li><li>预分配和回收空间<ul><li><code>ziplist</code>&nbsp;并不具备类似&nbsp;<code>SDS</code>&nbsp;的空间预分配机制，但在插入和删除元素时会重新分配内存</li><li>如果新插入的元素使得现有内存不足，<code>ziplist</code>&nbsp;会重新分配足够的内存来容纳新的数据</li></ul></li><li>数据结构<ul><li><code>ziplist</code>&nbsp;由多部分组成：起始偏移量、数据节点、结尾标志等</li><li>每个数据节点由前向长度（prevlen）、编码和实际数据组成</li><li>节点之间通过偏移量而非指针联系，这使得&nbsp;<code>ziplist</code>&nbsp;更紧凑，但插入和删除操作相对复杂，因为需要移动大量数据</li></ul></li><li>内存复制和移动<ul><li>因为&nbsp;<code>ziplist</code>&nbsp;是一块连续内存，在插入或删除元素时需要进行内存复制和移动操作，这会影响性能，特别是在元素较多的情况下</li><li><ul><li>当列表增大到一定程度，Redis 会自动将其转换为更合适的数据结构</li></ul></li></ul></li><li>使用场景<ul><li>小数据量：<code>ziplist</code>&nbsp;适用于小型数据结构（小列表和小哈希）。它能够有效减少内存开销，并且在数据量较小的情况下，其操作效率也较高</li><li>内存敏感：在内存敏感的场景中，<code>ziplist</code>&nbsp;可以提供良好的内存使用效率</li></ul></li><li>限制条件<ul><li><code>list-max-ziplist-entries</code>&nbsp;和&nbsp;<code>list-max-ziplist-value</code>，分别限制了列表中节点的最大数量和节点值的最大长度，当&nbsp;<code>ziplist</code>&nbsp;超过这些限制时，Redis 会将其转换为其他数据结构</li></ul></li></ul></li></ul><p><img src="/static/IT/Redis/Redis-%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-4.png" alt=""><br><img src="/static/IT/Redis/Redis-%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-5.png" alt=""></p><h3 id="Hash">Hash</h3><ul><li>底层实现为一个字典( dict )</li><li>当数据量比较小，或者单个元素比较小时，底层用ziplist存储</li><li>hash-max-ziplist-entries&nbsp; 512<ul><li>ziplist 元素个数超过 512 ，将改为hashtable编码</li></ul></li><li>hash-max-ziplist-value&nbsp;&nbsp;&nbsp; 64<ul><li>单个元素大小超过 64 byte时，将改为hashtable编码</li></ul></li></ul><p><img src="/static/IT/Redis/Redis-%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-6.png" alt=""></p><h3 id="Set">Set</h3><ul><li>底层实现为一个value 为 null 的 字典( dict )</li><li>当数据可以用整形表示时，Set集合将被编码为intset数据结构</li><li>两个条件任意满足时Set将用hashtable存储数据<ul><li>元素个数大于 set-max-intset-entries<ul><li>set-max-intset-entries 512<ul><li>intset 能存储的最大元素个数，超过则用hashtable编码</li></ul></li></ul></li><li>元素无法用整形表示</li></ul></li><li>intset<ul><li>整数集合是一个有序的，存储整型数据的结构</li><li>整型集合在Redis中可以保存int16_t,int32_t,int64_t类型的整型数据</li><li>可以保证集合中不会出现重复数据</li></ul></li></ul><p><img src="/static/IT/Redis/Redis-%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-7.png" alt=""></p><h3 id="ZSet">ZSet</h3><ul><li>有序的，自动去重的集合数据类型</li><li>底层实现为 字典(dict) + 跳表(skiplist)<ul><li>字典用于快速进行元素唯一性判定，而 Skiplist 用于保持元素的有序排列。每个元素同时存在于字典和 Skiplist 中，这样保证了两种数据结构的优势</li></ul></li><li>当数据比较少时，用ziplist编码结构存储</li><li>zset-max-ziplist-entries&nbsp; 128<ul><li>元素个数超过128，将用skiplist编码</li></ul></li><li>zset-max-ziplist-value&nbsp;&nbsp;&nbsp;&nbsp; 64<ul><li>单个元素大小超过 64 byte，将用 skiplist编码</li></ul></li></ul><p><img src="/static/IT/Redis/Redis-%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-8.png" alt=""></p><ul><li>skiplist<ul><li>多级链表<ul><li>Skiplist 由多层链表组成，每层链表都是底层链表的子集。底层链表包含所有元素，而每一层链表以一定概率抽取上层的一部分节点</li><li>每个节点包含多个指向下一层节点的指针，指针数量与节点所在层数相关</li></ul></li><li>随机层数<ul><li>插入新元素时，会随机决定新节点的层数。通常使用几何分布来确定层数，这样每个节点以一定概率加入更高层。从而保证了平均情况下查询操作的时间复杂度是对数级的</li><li>Redis 中，使用 1/4 的概率选择更高层，所以每层链表中的元素期望数量是其下层链表的 1/4</li></ul></li><li>查找操作<ul><li>查找过程从顶层链表开始，逐层向下进行</li><li>在每一层，查找操作按顺序走链表，直到找到目标元素或者需要进入下一层</li><li>因为更高层链表中的元素较少，查找操作在平均情况下是 O(log n) 的复杂度</li></ul></li><li>插入和删除操作<ul><li>插入操作先进行查找，找到插入位置后，根据随机层数将新节点插入相应层</li><li>删除操作类似，先进行查找，找到目标节点后，更新相关指针进行删除</li><li>虽然最坏情况下时间复杂度是 O(n)，但由于层级结构和随机分布，平均性能表现为 O(log n)</li></ul></li><li><img src="/static/IT/Redis/Redis-%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-9.png" alt=""></li><li><img src="/static/IT/Redis/Redis-%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-10.png" alt=""></li></ul></li></ul><hr><ul><li>渐进式rehash及动态扩容机制：避免在一次性rehash操作过程中导致大规模停顿<ul><li>Redis的数据结构字典（dict）中存在两个哈希表&nbsp;<code>ht[0]</code>&nbsp;和&nbsp;<code>ht[1]</code><ul><li>其中&nbsp;<code>ht[0]</code>&nbsp;用于存储当前数据，当开始rehash时，会初始化&nbsp;<code>ht[1]</code>&nbsp;并逐步将&nbsp;<code>ht[0]</code>&nbsp;中的数据迁移到<code>ht[1]</code></li></ul></li><li>Redis检测到需要扩展或收缩哈希表时，会初始化一个新的哈希表&nbsp;<code>ht[1]</code>，其大小根据需要进行扩展或收缩</li><li>设置一个标志，表示当前正在进行rehash操作</li><li>逐步迁移数据：在每次对字典进行增删查改等普通操作时，Redis会将有限数量（如一个或多个）从旧的哈希表&nbsp;<code>ht[0]</code>&nbsp;迁移到新的哈希表&nbsp;<code>ht[1]</code>&nbsp;中。这些操作包括&nbsp;<code>get</code>,&nbsp;<code>set</code>,&nbsp;<code>del</code>&nbsp;等</li><li>在rehash进行期间，所有的查找、插入操作都会同时对&nbsp;<code>ht[0]</code>&nbsp;和&nbsp;<code>ht[1]</code>&nbsp;生效。如果一个键在&nbsp;<code>ht[0]</code>&nbsp;中不存在，则继续查找&nbsp;<code>ht[1]</code></li><li>当&nbsp;<code>ht[0]</code>&nbsp;的所有数据全部迁移到&nbsp;<code>ht[1]</code>&nbsp;后，删除旧哈希表&nbsp;<code>ht[0]</code>，并将&nbsp;<code>ht[1]</code>&nbsp;设置为新的数据哈希表</li></ul></li><li>尽管渐进式rehash极大地提高了Redis的实时性和性能，但在rehash过程中，哈希表的大小并不会立即固定，这可能导致一些特殊情况下的查询延迟增加<ul><li><code>active-rehashing</code>&nbsp;参数决定是否在后台进程中主动进行rehash操作，也可以在主操作线程中逐步完成rehash，以使得逐步迁移在后端缓解表操作压力</li></ul></li><li>BitMap ：本质是字符串<ul><li>GETBIT/SETBIT/BITOPS/BITCOUNT</li><li>连续登录统计，每天的bitmap进行与运算</li></ul></li></ul><hr><h2 id="GeoHash-经纬度计算">GeoHash 经纬度计算</h2><ul><li>将地理位置编码为一串简短的字母和数字<ul><li>将空间细分为网格形状的桶，这是所谓的z顺序曲线的众多应用之一，通常是空间填充曲线</li></ul></li><li>经纬度编码<ul><li>经度范围是东经180到西经180，纬度范围是南纬90到北纬90<ul><li>设定西经为负，南纬为负，所以地球上的经度范围就是<code>[-180， 180]</code>，纬度范围就是<code>[-90，90]</code></li><li>如果以本初子午线、赤道为界，地球可以分成4个部分</li></ul></li><li>如果纬度范围<code>[-90°, 0°)</code>用二进制0代表，<code>（0°, 90°]</code>用二进制1代表，经度范围<code>[-180°, 0°)</code>用二进制0代表，<code>（0°, 180°]</code>用二进制1代表，那么地球可以分成如下(左图 )4个部分<ul><li><img src="/static/IT/Redis/Redis-%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-11.png" alt=""></li></ul></li></ul></li><li>通过GeoHash算法，可以将经纬度的二维坐标变成一个可排序、可比较的的字符串编码<ul><li>每个字符代表一个区域，并且前面的字符是后面字符的父区域</li><li><img src="/static/IT/Redis/Redis-%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-12.png" alt=""></li><li>纬度产生的编码为1011 1000 1100 0111 1001，经度产生的编码为1101 0010 1100 0100 0100</li><li>偶数位放经度，奇数位放纬度，把2串编码组合生成新串 11100 11101 00100 01111 00000 01101 01011 00001</li><li>最后使用用0-9、b-z（去掉a, i, l, o）这32个字母进行base32编码（将编码转换成经纬度的解码算法与之相反）<ul><li>首先将11100 11101 00100 01111 00000 01101 01011 00001转成十进制 28，29，4，15，0，13，11，1</li><li>十进制对应的编码就是wx4g0ec1</li></ul></li></ul></li><li>优点<ul><li>GeoHash利用Z阶曲线进行编码，Z阶曲线可以将二维所有点都转换成一阶曲线。地理位置坐标点通过编码转化成一维值，利用有序数据结构如B树、SkipList等，均可进行范围搜索。因此利用GeoHash算法查找邻近点比较快</li></ul></li><li>缺点<ul><li>Z 阶曲线有一个比较严重的问题，虽然有局部保序性，但是它也有突变性。在每个 Z 字母的拐角，都有可能出现顺序的突变</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Redis 应用场景&lt;/li&gt;
&lt;li&gt;Redis 底层核心数据结构&lt;/li&gt;
&lt;li&gt;Redis 渐进式rehash及动态扩容机制&lt;/li&gt;
&lt;li&gt;BitMap 海量数据统计&lt;/li&gt;
&lt;li&gt;GeoHash 经纬度计算&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-持久化</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-chi-jiu-hua/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-chi-jiu-hua/</id>
    <published>2024-09-10T07:09:00.000Z</published>
    <updated>2024-09-10T07:10:39.549Z</updated>
    
    <content type="html"><![CDATA[<ul><li>RDB：dump.rdb （二进制文件）<ul><li>配置文件：bgsave 方式<ul><li>save 60 1000 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关闭RDB只需要将所有的save保存策略注释掉即可<ul><li>60 秒内有至少有 1000 个键被改动， 则进行一次RDB持久化</li></ul></li></ul></li><li>命令：覆盖原有rdb快照文件<ul><li>save：同步</li><li>bgsave：异步，写时复制 - COW机制<ul><li>bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据<ul><li>在生成子进程执行调用fork函数时会有短暂阻塞</li></ul></li><li>如果主线程要修改一块数据，那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据</li></ul></li></ul></li><li>缺点：宕机后，服务器将丢失最近写入、且仍未保存到快照中的数据</li></ul></li><li>AOF（append-only file）：appendonly.aof（resp协议格式）<ul><li>将修改的每一条指令记录进文件appendonly.aof中（先写入os cache，每隔一段时间fsync到磁盘）</li><li>配置文件<ul><li>appendonly yes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 开启AOF模式</li><li>appendfsync always&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   每条命令都fsync一次，拉低性能</li><li>appendfsync everysec&nbsp;&nbsp;    每秒fsync一次，推荐，缺点是宕机后会丢失1秒的数据，但可以从数据库恢复</li><li>appendfsync no&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;     让操作系统决定fsync的时机，快但不安全</li><li>auto‐aof‐rewrite‐min‐size 64mb&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AOF文件超过64M时，重写AOF文件（整合命令）</li><li>auto‐aof‐rewrite‐percentage 100&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  自上一次重写后文件大小增长了100%则再次触发重写</li></ul></li><li>命令：bgrewriteaof&nbsp; （fork出一个子进程去做）</li><li>缺点：体积大，恢复慢</li></ul></li><li>混合持久化：RDB+AOF<ul><li>配置文件（必须先开启AOF）：aof‐use‐rdb‐preamble yes</li><li>将重写这一刻之前的内存做RDB快照处理，并且将RDB快照内容和增量的AOF修改内存数据的命令（生成RDB过程中产生的命令）存在一起，都写入新的AOF文件</li><li>新的文件一开始不叫appendonly.aof，等到重写完新的AOF文件才会进行改名，覆盖原有的AOF文件，完成新旧两个AOF文件的替换</li><li>于是在 Redis 重启的时候，可以先加载 RDB 的内容，然后再重放增量 AOF 日志就可以完全替代之前的AOF 全量文件重放，因此重启效率大幅得到提升</li><li><img src="/static/IT/Redis/Redis-%E6%8C%81%E4%B9%85%E5%8C%96-1.png" alt="混合持久化"></li></ul></li><li>数据备份策略：<ol><li>写crontab定时调度脚本，每小时都copy一份rdb或aof的备份到一个目录中去，仅仅保留最近48小时的备份</li><li>每天都保留一份当日的数据备份到一个目录中去，可以保留最近1个月的备份</li><li>每次copy备份的时候，删除一些旧备份</li><li>每天晚上将当前机器上的备份复制一份到其他机器上，以防机器损坏</li></ol></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;RDB：dump.rdb （二进制文件）
&lt;ul&gt;
&lt;li&gt;配置文件：bgsave 方式
&lt;ul&gt;
&lt;li&gt;save 60 1000 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;关闭RDB只需要将所有的save保存策略注释掉即可
&lt;ul&gt;
&lt;li</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Seata-特性</title>
    <link href="https://jxch.github.io/2024/09/10/architect/seata/seata-te-xing/"/>
    <id>https://jxch.github.io/2024/09/10/architect/seata/seata-te-xing/</id>
    <published>2024-09-10T06:28:00.000Z</published>
    <updated>2024-09-10T06:36:47.448Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Seata-组件</li><li>2PC</li><li>Seata-AT</li><li>Seata-XA</li><li>Seata-TCC</li><li>Seata-SAGA：业务流（最终一致性，不保证事务的隔离）</li><li>Seata-集成</li></ul><hr><h2 id="Seata-组件">Seata-组件</h2><ul><li>核心组件：TC&nbsp;为单独部署的&nbsp;Server&nbsp;服务端，TM&nbsp;和&nbsp;RM&nbsp;为嵌入到应用中的&nbsp;Client&nbsp;客户端<ul><li>TC&nbsp;(Transaction&nbsp;Coordinator)&nbsp;-&nbsp;事务协调者：维护全局和分支事务的状态，驱动全局事务提交或回滚</li><li>TM&nbsp;(Transaction&nbsp;Manager)&nbsp;-&nbsp;事务管理器：定义全局事务的范围，开始全局事务、提交或回滚全局事务</li><li>RM&nbsp;(Resource&nbsp;Manager)&nbsp;-&nbsp;资源管理器 ：管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚</li></ul></li><li>分布式事务的生命周期<ul><li>TM&nbsp;请求&nbsp;TC&nbsp;开启一个全局事务。TC&nbsp;会生成一个&nbsp;XID&nbsp;作为该全局事务的编号。XID会在微服务的调用链路中传播，保证将多个微服务的子事务关联在一起</li><li>RM&nbsp;请求&nbsp;TC&nbsp;将本地事务注册为全局事务的分支事务，通过全局事务的&nbsp;XID&nbsp;进行关联</li><li>TM&nbsp;请求&nbsp;TC&nbsp;告诉&nbsp;XID&nbsp;对应的全局事务是进行提交还是回滚</li><li>TC&nbsp;驱动&nbsp;RM&nbsp;们将&nbsp;XID&nbsp;对应的自己的本地事务进行提交还是回滚</li></ul></li><li>使用 <code>@GlobalTransactional</code> 开启分布式事务（XA模式需要同时使用<code>@Transcational</code>）</li><li>使用 <code>@GlobalLock</code> + <code>select ... for update</code> 实现全局事务的读隔离</li></ul><hr><h2 id="2PC">2PC</h2><ul><li>2PC 两阶段提交<ol><li>TM通知各个RM准备提交它们的事务分支。如果RM判断自己进行的工作可以被提交，那就对工作内容进行持久化，再给TM肯定答复；要是发生了其他情况，那给TM的都是否定答复</li><li>TM根据阶段1各个RM&nbsp;prepare的结果，决定是提交还是回滚事务。如果所有的RM都prepare成功，那么TM通知所有的RM进行提交；如果有RM&nbsp;prepare失败的话，则TM通知所有RM回滚自己的事务分支</li></ol></li><li>ACID：两阶段提交方案下全局事务的ACID特性，是依赖于RM的。一个全局事务内部包含了多个独立的事务分支，这一组事务分支要么都成功，要么都失败。各个事务分支的ACID特性共同构成了全局事务的ACID特性。也就是将单个事务分支支持的ACID特性提升一个层次到分布式事务的范畴</li><li>2PC存在的问题<ul><li>同步阻塞问题：2PC&nbsp;中的参与者是阻塞的。在第一阶段收到请求后就会预先锁定资源，一直到&nbsp;commit&nbsp;后才会释放</li><li>单点故障：一旦协调者TM发生故障，参与者RM会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作</li><li>数据不一致：若协调者第二阶段发送提交请求时崩溃，可能部分参与者收到commit请求提交了事务，而另一部分参与者未收到commit请求而放弃事务，从而造成数据不一致的问题</li></ul></li></ul><p><img src="/static/IT/Seata/Seata-%E7%89%B9%E6%80%A7-1.png" alt=""></p><hr><h2 id="Seata-AT">Seata-AT</h2><ul><li>AT（强一致性，适合并发量不高的场景）：业务无侵入，是一种改进后的两阶段提交（一阶段提交释放资源，二阶段日志回滚补偿 - undo_log 表）<ol><li>业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源（不会一直持有资源的锁）<ul><li>对业务sql进行解析，转换成undolog，并同时入库</li><li><img src="/static/IT/Seata/Seata-%E7%89%B9%E6%80%A7-2.png" alt=""></li></ul></li><li>提交异步化，非常快速地完成；回滚通过一阶段的回滚日志进行反向补偿<ul><li>分布式事务操作成功，则TC通知RM异步删除undolog<ul><li><img src="/static/IT/Seata/Seata-%E7%89%B9%E6%80%A7-3.png" alt=""></li></ul></li><li>分布式事务操作失败，TM向TC发送回滚请求，RM&nbsp;收到协调器TC发来的回滚请求，通过&nbsp;XID&nbsp;和&nbsp;Branch&nbsp;ID&nbsp;找到相应的回滚日志记录，通过回滚记录生成反向的更新&nbsp;SQL&nbsp;并执行，以完成分支的回滚<ul><li>问题：如果事务进行期间，其他线程修改了这行数据，导致undo log中前置镜像的值和库表实际值不一致，就会导致回滚失败，其他事务也会被拒绝，必须人工介入修改库表数据后删除undo log</li><li><img src="/static/IT/Seata/Seata-%E7%89%B9%E6%80%A7-4.png" alt=""></li></ul></li></ul></li></ol></li><li>TM 位于事务最顶层：下游服务如果处理了异常导致上有TM所在的服务无法捕获异常，就会导致事务失效的情况<ul><li>Feign的服务降级之后一定要重新抛出异常</li><li><img src="/static/IT/Seata/Seata-%E7%89%B9%E6%80%A7-5.png" alt=""></li></ul></li></ul><hr><h2 id="Seata-XA">Seata-XA</h2><ul><li>XA（强一致性，适用于中间件场景）：一直持有资源的锁，不需要undo_log表<ul><li>配置方式：<code>seata.data‐source‐proxy‐mode=XA</code></li><li>从编程模型上，XA&nbsp;模式与&nbsp;AT&nbsp;模式基本上完全一致，只是由于依赖本地事务（代理数据源-XADataSource），<code>@GlobalTransactional</code>必须配合<code>@Transactional</code>使用，否则无效</li><li><img src="/static/IT/Seata/Seata-%E7%89%B9%E6%80%A7-6.png" alt=""></li></ul></li><li>AT和XA模式数据源代理机制对比<ul><li><img src="/static/IT/Seata/Seata-%E7%89%B9%E6%80%A7-7.png" alt=""></li></ul></li></ul><hr><h2 id="Seata-TCC">Seata-TCC</h2><ul><li>TCC（Try-Confirm-Cance）：最终一致性，适合高并发事务场景，比如金融领域，不会一直持有资源的锁<ul><li>侵入式的分布式事务解决方案（需要手动实现try, commit, cancel）：完全不依赖数据库，能够实现跨数据库、跨应用资源管理</li><li>SEATA的TCC模式就是手工的AT模式，它允许你自定义两阶段的处理逻辑而不依赖AT模式的undo_log</li></ul></li><li>资源预留（Try）、确认操作（Confirm）、取消操作（Cancel）<ul><li>Try：对业务资源的检查并预留</li><li>Confirm：对业务处理进行提交，即&nbsp;commit&nbsp;操作，只要&nbsp;Try&nbsp;成功，那么该步骤一定成功</li><li>Cancel：对业务处理进行取消，即回滚操作，该步骤回对&nbsp;Try&nbsp;预留的资源进行释放</li><li><img src="/static/IT/Seata/Seata-%E7%89%B9%E6%80%A7-8.png" alt=""></li></ul></li><li>一阶段&nbsp;prepare&nbsp;行为；二阶段&nbsp;commit&nbsp;或&nbsp;rollback&nbsp;行为<ul><li><img src="/static/IT/Seata/Seata-%E7%89%B9%E6%80%A7-9.png" alt=""></li><li>try-commit：try&nbsp;阶段首先进行预留资源，然后在&nbsp;commit&nbsp;阶段扣除资源</li><li>try-cancel：try&nbsp;阶段首先进行预留资源，预留资源时扣减库存失败导致全局事务回滚，在&nbsp;cancel&nbsp;阶段释放资源</li></ul></li><li>空回滚、幂等、悬挂：Seata都已经封装好了，基于日志表 tcc_fence_log（对tcc的执行顺序进行日志记录，就可以识别这三个问题）<ul><li>空回滚：try 因为网络问题没有调用成功，按理说不需要 cancel，但是 cancel 了<ul><li>造成数据不一致：在try中扣减，在cancel增加，try没有执行，而cancel执行了</li><li><img src="/static/IT/Seata/Seata-%E7%89%B9%E6%80%A7-10.png" alt=""></li></ul></li><li>幂等：&nbsp;TC&nbsp;重复进行二阶段提交，Confirm/Cancel&nbsp;接口需要支持幂等处理，即不会产生资源重复提交或者重复释放<ul><li>由于网络问题，事务提交后没有响应到服务端，导致服务端发动重试机制，产生重复提交，重复释放同理</li><li><img src="/static/IT/Seata/Seata-%E7%89%B9%E6%80%A7-11.png" alt=""></li></ul></li><li>悬挂：try 方法被网络阻塞，导致空回滚，但是空回滚后 try 方法被成功执行了<ul><li><img src="/static/IT/Seata/Seata-%E7%89%B9%E6%80%A7-12.png" alt=""></li></ul></li></ul></li><li>Seata解决方案：在tcc_fence_log表中记录状态status <code>tried：1 committed：2 rollbacked：3 suspended：4</code><ul><li>try 执行后是 tried<ul><li>解决空回滚问题：当执行 cancel 时，检测到当前没有记录，就进行空回滚，并置为 suspended<ul><li>解决悬挂问题：try 执行时检测到现在是 suspended 状态，就放弃执行</li></ul></li></ul></li><li>confirm 执行后是 committed；cancel 执行后是 rollbacked<ul><li>解决幂等问题：当重复进行二阶段调用时，当前状态已经是二阶段状态了</li></ul></li><li>通过检查目前事务的状态，可以轻松避免空回滚、幂等、悬挂的问题</li></ul></li></ul><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token comment">// @LocalTCC 适用于SpringCloud+Feign模式下的TCC，@LocalTCC一定需要注解在接口上</span><span class="token annotation punctuation">@LocalTCC</span><span class="token keyword">public</span> <span class="token keyword">interface</span> <span class="token class-name">OrderService</span> <span class="token punctuation">{</span><span class="token comment">/*  * 定义了分支事务的 resourceId，commit和 cancel 方法 * name = 该tcc的bean名称,全局唯一 * commitMethod = commit 为二阶段确认方法 * rollbackMethod = rollback 为二阶段取消方法 * BusinessActionContextParameter注解，传递参数到二阶段中 * useTCCFence：用于解决TCC幂等，悬挂，空回滚问题，需增加日志表tcc_fence_log */</span><span class="token annotation punctuation">@TwoPhaseBusinessAction</span><span class="token punctuation">(</span>name <span class="token operator">=</span> <span class="token string">"prepareSaveOrder"</span><span class="token punctuation">,</span> commitMethod <span class="token operator">=</span> <span class="token string">"commit"</span><span class="token punctuation">,</span> rollbackMethod <span class="token operator">=</span> <span class="token string">"rollback"</span><span class="token punctuation">,</span> useTCCFence <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">)</span><span class="token keyword">boolean</span> <span class="token function">deduct</span><span class="token punctuation">(</span><span class="token annotation punctuation">@BusinessActionContextParameter</span><span class="token punctuation">(</span>paramName <span class="token operator">=</span> <span class="token string">"commodityCode"</span><span class="token punctuation">)</span> <span class="token class-name">String</span> commodityCode<span class="token punctuation">,</span>  <span class="token annotation punctuation">@BusinessActionContextParameter</span><span class="token punctuation">(</span>paramName <span class="token operator">=</span> <span class="token string">"count"</span><span class="token punctuation">)</span> <span class="token keyword">int</span> count<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">boolean</span> <span class="token function">commit</span><span class="token punctuation">(</span><span class="token class-name">BusinessActionContext</span> actionContext<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">boolean</span> <span class="token function">rollback</span><span class="token punctuation">(</span><span class="token class-name">BusinessActionContext</span> actionContext<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="Seata-集成">Seata-集成</h2><ul><li>db存储模式+Nacos(注册&amp;配置中心)方式部署<ul><li><img src="/static/IT/Seata/Seata-%E7%89%B9%E6%80%A7-13.png" alt=""></li></ul></li><li>将Seata&nbsp;Server注册到Nacos，修改conf/application.yml文件<ul><li>确保client与server的注册处于同一个namespace和group，不然会找不到服务</li></ul></li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">registry</span><span class="token punctuation">:</span><span class="token comment"># support: nacos, eureka, redis, zk, consul, etcd3, sofa</span><span class="token key atrule">type</span><span class="token punctuation">:</span> nacos<span class="token key atrule">nacos</span><span class="token punctuation">:</span><span class="token key atrule">application</span><span class="token punctuation">:</span> seata‐server<span class="token key atrule">server‐addr</span><span class="token punctuation">:</span> 127.0.0.1<span class="token punctuation">:</span><span class="token number">8848</span><span class="token key atrule">group</span><span class="token punctuation">:</span> SEATA_GROUP<span class="token key atrule">namespace</span><span class="token punctuation">:</span> 7e838c12‐8554‐4231‐82d5‐6d93573ddf32<span class="token key atrule">cluster</span><span class="token punctuation">:</span> default<span class="token key atrule">username</span><span class="token punctuation">:</span>password<span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>配置Nacos配置中心地址，修改conf/application.yml文件</li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">seata</span><span class="token punctuation">:</span><span class="token key atrule">config</span><span class="token punctuation">:</span><span class="token comment"># support: nacos, consul, apollo, zk, etcd3</span><span class="token key atrule">type</span><span class="token punctuation">:</span> nacos<span class="token key atrule">nacos</span><span class="token punctuation">:</span><span class="token key atrule">server‐addr</span><span class="token punctuation">:</span> 127.0.0.1<span class="token punctuation">:</span><span class="token number">8848</span><span class="token key atrule">namespace</span><span class="token punctuation">:</span> 7e838c12‐8554‐4231‐82d5‐6d93573ddf32<span class="token key atrule">group</span><span class="token punctuation">:</span> SEATA_GROUP<span class="token key atrule">data‐id</span><span class="token punctuation">:</span> seataServer.properties<span class="token key atrule">username</span><span class="token punctuation">:</span>password<span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>上传 seataServer.properties 到 Nacos （设定db模式）<ul><li>修改：<a href="https://github.com/apache/incubator-seata/blob/v1.5.1/script/config-center/config.txt">https://github.com/apache/incubator-seata/blob/v1.5.1/script/config-center/config.txt</a></li><li>seata是通过jdbc的executeBatch来批量插入全局锁的<ul><li>连接参数中的rewriteBatchedStatements为true时，在执行executeBatch，并且操作类型为insert时，jdbc驱动会把对应的SQL优化成<code>insert&nbsp;into&nbsp;()&nbsp;values&nbsp;(),&nbsp;()</code>的形式来提升批量插入的性能（插入性能为原来的10倍多）</li></ul></li></ul></li></ul><pre class="line-numbers language-prop" data-language="prop"><code class="language-prop">store.mode=dbstore.db.driverClassName=com.mysql.jdbc.Driverstore.db.url=jdbc:mysql://127.0.0.1:3306/seata?useUnicode=true&amp;rewriteBatchedStatements=truestore.db.user=rootstore.db.password=root<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>配置事务分组<ul><li><code>seataServer.properties</code> 里的 <code>service.vgroupMapping.default_tx_group=default</code> 与 Seata&nbsp;Server 注册到 Nacos 的 <code>registry.nacos.cluster=default</code> 必须保持一致</li><li>要与client配置的事务分组一致 <code>seata.tx-service-group=default_tx_group</code>（和<code>service.vgroupMapping.</code>的后缀对应）</li></ul></li><li>启动Seata&nbsp;Server<ul><li>seata‐<a href="http://server.sh">server.sh</a>&nbsp;‐p&nbsp;8091&nbsp;‐h&nbsp;127.0.0.1&nbsp;‐m&nbsp;db<ul><li>-m 事务日志存储方式，支持file,db,redis，默认file</li><li>-n 指定seata-server节点ID</li></ul></li></ul></li></ul><hr><ul><li>事务分组如何找到后端Seata集群（TC）<ul><li>客户端中配置了事务分组，SpringBoot 通过 seata.tx-service-group&nbsp;配置</li><li>客户端会通过用户配置的配置中心去寻找 service.vgroupMapping&nbsp;.[事务分组配置项]，取得配置项的值就是TC集群的名称，SpringBoot 通过 <code>seata.service.vgroup-mapping.事务分组名=集群名称</code> 配置</li><li>拿到集群名称程序通过一定的前后缀+集群名称去构造服务名</li><li>拿到服务名去相应的注册中心去拉取相应服务名的服务列表，获得后端真实的TC服务列表</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Seata-组件&lt;/li&gt;
&lt;li&gt;2PC&lt;/li&gt;
&lt;li&gt;Seata-AT&lt;/li&gt;
&lt;li&gt;Seata-XA&lt;/li&gt;
&lt;li&gt;Seata-TCC&lt;/li&gt;
&lt;li&gt;Seata-SAGA：业务流（最终一致性，不保证事务的隔离）&lt;/li&gt;
&lt;li&gt;Seata</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Seata" scheme="https://jxch.github.io/tags/Seata/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper-Leader选举</title>
    <link href="https://jxch.github.io/2024/09/10/architect/zookeeper/zookeeper-leader-xuan-ju/"/>
    <id>https://jxch.github.io/2024/09/10/architect/zookeeper/zookeeper-leader-xuan-ju/</id>
    <published>2024-09-10T06:14:00.000Z</published>
    <updated>2024-09-10T06:15:53.680Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/static/IT/Zookeeper/Zookeeper-Leader%E9%80%89%E4%B8%BE-1.png" alt="启动或leader宕机选举leader流程"></p><p><img src="/static/IT/Zookeeper/Zookeeper-Leader%E9%80%89%E4%B8%BE-2.png" alt="leader选举多层队列架构"></p><p>整个zookeeper选举底层可以分为选举应用层和消息传输层，应用层有自己的队列统一接收和发送选票，传输层也设计了自己的队列，但是按发送的机器分了队列，避免给每台机器发送消息时相互影响，比如某台机器如果出问题发送不成功则不会影响对正常机器的消息发<br>送</p><hr><p><img src="/static/IT/Zookeeper/Zookeeper-Leader%E9%80%89%E4%B8%BE-3.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/static/IT/Zookeeper/Zookeeper-Leader%E9%80%89%E4%B8%BE-1.png&quot; alt=&quot;启动或leader宕机选举leader流程&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/static/IT/Zookeep</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Zookeeper" scheme="https://jxch.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper-curator</title>
    <link href="https://jxch.github.io/2024/09/10/architect/zookeeper/zookeeper-curator/"/>
    <id>https://jxch.github.io/2024/09/10/architect/zookeeper/zookeeper-curator/</id>
    <published>2024-09-10T06:12:00.000Z</published>
    <updated>2024-09-10T06:13:00.620Z</updated>
    
    <content type="html"><![CDATA[<ul><li>工厂类创建客户端：CuratorFrameworkFactory<ul><li>connectionString：服务器地址列表；逗号分隔</li><li>retryPolicy：重试策略<ul><li>可以通过判断服务器返回的&nbsp;keeperException&nbsp;的状态代码来判断是否进行重试处理<ul><li>OK&nbsp;表示一切操作都没有问题</li><li>SYSTEMERROR&nbsp;表示系统或服务端错误</li></ul></li><li>ExponentialBackoffRetry 重试一组次数，重试之间的睡眠时间增加</li><li>RetryNTimes 重试最大次数</li><li>RetryOneTime 只重试一次</li><li>RetryUntilElapsed 在给定的时间结束之前重试</li></ul></li><li>sessionTimeoutMs&nbsp;会话超时时间；作用在服务端<ul><li>用来设置该条会话在&nbsp;ZooKeeper&nbsp;服务端的失效时间</li></ul></li><li>connectionTimeoutMs&nbsp;客户端创建会话的超时时间；&nbsp;作用在客户端<ul><li>用来限制客户端发起一个会话连接到接收&nbsp;ZooKeeper&nbsp;服务端应答的时间</li></ul></li></ul></li><li>创建节点<ul><li><code>curatorFramework.create().withMode(CreateMode.PERSISTENT).forPath</code></li></ul></li><li>一次性创建带层级结构的节点<ul><li><code>curatorFramework.create().creatingParentsIfNeeded().forPath</code></li></ul></li><li>获取数据<ul><li><code>curatorFramework.getData().forPath</code></li></ul></li><li>更新节点<ul><li><code>curatorFramework.setData().forPath</code></li></ul></li><li>删除节点<ul><li><code>curatorFramework.delete().guaranteed().deletingChildrenIfNeeded().forPath</code></li><li>guaranteed：起到一个保障删除成功的作用<ul><li>只要该客户端的会话有效，就会在后台持续发起删除请求，直到该数据节点在&nbsp;ZooKeeper&nbsp;服务端被删除</li></ul></li><li>deletingChildrenIfNeeded：系统在删除该数据节点的时候会以递归的方式直接删除其子节点，以及子节点的子节点</li></ul></li><li>异步接口 BackgroundCallback，用来处理服务器端返回来的信息，默认在&nbsp;EventThread&nbsp;中调用，也可以自定义线程池<ul><li><code>curatorFramework.getData().inBackground(BackgroundCallback).forpath</code></li><li>指定线程池<ul><li><code>curatorFramework.getData().inBackground(BackgroundCallback,ExecutorService).forpath</code></li></ul></li></ul></li><li>Curator&nbsp;监听器 CuratorListener<ul><li>针对&nbsp;background&nbsp;通知和错误通知。使用此监听器之后，调用 inBackground&nbsp;方法会异步获得监听</li></ul></li><li>Curator&nbsp;Caches<ul><li>Curator&nbsp;引入了&nbsp;Cache&nbsp;来实现对&nbsp;Zookeeper&nbsp;服务端事件监听</li><li>Cache&nbsp;事件监听可以理解为一个本地缓存视图与远程&nbsp;Zookeeper&nbsp;视图的对比过程</li><li>Cache&nbsp;提供了反复注册的功能<ul><li>Cache&nbsp;分为两类注册类型：节点监听和子节点监听</li></ul></li><li>NodeCache&nbsp;对某一个节点进行监听，可以通过注册监听器来实现，对当前节点数据变化的处理<ul><li><code>NodeCache&nbsp;nodeCache&nbsp;=&nbsp;new&nbsp;NodeCache(curatorFramework,&nbsp;"/node")</code></li><li><code>nodeCache.getListenable().addListener()</code></li><li><code>&nbsp;nodeCache.start();</code></li></ul></li><li>PathChildrenCache&nbsp;会对子节点进行监听，但是不会对二级子节点进行监听，可以通过注册监听器来实现，对当前节点的子节点数据变化的处理<ul><li><code>var&nbsp;pathChildrenCache&nbsp;=&nbsp;new&nbsp;PathChildrenCache(curatorFramework,&nbsp;PATH,&nbsp;true);</code></li><li><code>pathChildrenCache.getListenable().addListener</code></li><li><code>pathChildrenCache.start(true);</code> 如果设置为true则在首次启动时就会缓存节点内容到Cache中</li></ul></li><li>TreeCache&nbsp;使用一个内部类TreeNode来维护这个一个树结构。并将这个树结构与ZK节点进行了映射。所以TreeCache&nbsp;可以监听当前节点下所有节点的事件；可以通过注册监听器来实现，对当前节点的子节点，及递归子节点数据变化的处理<ul><li><code>TreeCache&nbsp;treeCache&nbsp;=&nbsp;new&nbsp;TreeCache(curatorFramework,&nbsp;TREE_CACHE);</code></li><li><code>treeCache.getListenable().addListener</code></li><li><code>treeCache.start();</code></li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;工厂类创建客户端：CuratorFrameworkFactory
&lt;ul&gt;
&lt;li&gt;connectionString：服务器地址列表；逗号分隔&lt;/li&gt;
&lt;li&gt;retryPolicy：重试策略
&lt;ul&gt;
&lt;li&gt;可以通过判断服务器返回的&amp;nbsp;keeper</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Zookeeper" scheme="https://jxch.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper-分布式锁&amp;注册中心</title>
    <link href="https://jxch.github.io/2024/09/10/architect/zookeeper/zookeeper-fen-bu-shi-suo-zhu-ce-zhong-xin/"/>
    <id>https://jxch.github.io/2024/09/10/architect/zookeeper/zookeeper-fen-bu-shi-suo-zhu-ce-zhong-xin/</id>
    <published>2024-09-10T06:06:00.000Z</published>
    <updated>2024-09-10T06:09:56.563Z</updated>
    
    <content type="html"><![CDATA[<ul><li>分布式锁<ul><li>非公平锁（羊群效应）</li><li>公平锁：临时顺序&nbsp;znode</li><li>Curator InterProcessMutex 可重入锁</li></ul></li><li>注册中心</li></ul><hr><h2 id="分布式锁">分布式锁</h2><ul><li>非公平锁（羊群效应）：所有的锁请求者都&nbsp;watch&nbsp;锁持有者，当代表锁持有者的&nbsp;znode&nbsp;被删除以后，所有的锁请求者都会通知到，但是只有一个锁请求者能拿到锁<ul><li><img src="/static/IT/Zookeeper/Zookeeper-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81-%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83-1.png" alt="非公平锁"></li></ul></li><li>公平锁：临时顺序&nbsp;znode<ul><li><img src="/static/IT/Zookeeper/Zookeeper-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81-%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83-2.png" alt="公平锁"></li></ul></li><li>Curator InterProcessMutex 可重入锁<ul><li><img src="/static/IT/Zookeeper/Zookeeper-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81-%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83-3.png" alt="Curator InterProcessMutex"></li><li>优点：具备高可用、可重入、阻塞锁特性，可解决失效死锁问题，使用起来也较为简单</li><li>缺点：因为需要频繁的创建和删除节点，性能上不如Redis</li><li>在高性能、高并发的应用场景下，不建议使用ZooKeeper的分布式锁</li><li>由于ZooKeeper的高可用性，因此在并发量不是太高的应用场景中，还是推荐使用ZooKeeper的分布式锁</li></ul></li></ul><hr><h2 id="注册中心">注册中心</h2><ul><li>spring‐cloud‐starter‐zookeeper‐discovery -&gt; ZookeeperDiscoveryClientConfiguration<ul><li>整合 feign 进行服务调用</li></ul></li></ul><p><img src="/static/IT/Zookeeper/Zookeeper-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81-%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83-4.png" alt="注册中心"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;分布式锁
&lt;ul&gt;
&lt;li&gt;非公平锁（羊群效应）&lt;/li&gt;
&lt;li&gt;公平锁：临时顺序&amp;nbsp;znode&lt;/li&gt;
&lt;li&gt;Curator InterProcessMutex 可重入锁&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;注册中心&lt;/li&gt;
&lt;/ul&gt;
</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Zookeeper" scheme="https://jxch.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper-特性</title>
    <link href="https://jxch.github.io/2024/09/10/architect/zookeeper/zookeeper-te-xing/"/>
    <id>https://jxch.github.io/2024/09/10/architect/zookeeper/zookeeper-te-xing/</id>
    <published>2024-09-10T05:59:00.000Z</published>
    <updated>2024-09-10T06:03:42.972Z</updated>
    
    <content type="html"><![CDATA[<ul><li>CP架构</li><li>常见命令</li><li>数据结构</li><li>监听通知机制</li><li>节点特性</li><li>ACL权限控制</li><li>集群</li><li>四字命令</li><li>Leader&nbsp;选举原理</li><li>数据同步流程</li></ul><hr><h2 id="CP架构">CP架构</h2><ul><li>CAP&nbsp;理论指出对于一个分布式计算系统来说，不可能同时满足以下三点<ul><li>一致性：在分布式环境中，一致性是指数据在多个副本之间是否能够保持一致的特性，等同于所有节点访问同一份最新的数据副本。在一致性的需求下，当一个系统在数据一致的状态下执行更新操作后，应该保证系统的数据仍然处于一致的状态</li><li>可用性：每次请求都能获取到正确的响应，但是不保证获取的数据为最新数据</li><li>分区容错性：分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供服务，除非是整个网络环境都发生了故障</li></ul></li><li>一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition&nbsp;tolerance）这三项中的两项<ul><li>P&nbsp;是必须的，因此只能在&nbsp;CP&nbsp;和&nbsp;AP&nbsp;中选择，zookeeper&nbsp;保证的是&nbsp;CP</li></ul></li><li>BASE&nbsp;理论：BASE&nbsp;是&nbsp;Basically&nbsp;Available(基本可用)、Soft-state(软状态)&nbsp;和&nbsp;Eventually&nbsp;Consistent(最终一致性)&nbsp;三个短语的缩写<ul><li>基本可用：在分布式系统出现故障，允许损失部分可用性（服务降级、页面降级）</li><li>软状态：允许分布式系统出现中间状态。而且中间状态不影响系统的可用性。这里的中间状态是指不同的&nbsp;data&nbsp;replication（数据备份节点）之间的数据更新可以出现延时的最终一致性</li><li>最终一致性：data&nbsp;replications&nbsp;经过一段时间达到一致性</li></ul></li><li>Zookeeper 写入是强一致性，读取是顺序一致性（版本号）</li><li>ZooKeeper本质上是一个分布式的小文件存储系统（Zookeeper=文件系统+监听机制）<ul><li>是一个基于观察者模式设计的分布式服务管理框架</li></ul></li></ul><hr><h2 id="常见命令">常见命令</h2><ul><li>ls 查看当前&nbsp;znode&nbsp;的子节点&nbsp;[可监听]<ul><li>-w:&nbsp;监听子节点变化</li><li>-s:&nbsp;节点状态信息（时间戳、版本号、数据大小等）</li><li>-R:&nbsp;表示递归的获取</li></ul></li><li>create 创建节点<ul><li>-s&nbsp;:&nbsp;创建有序节点</li><li>-e&nbsp;:&nbsp;创建临时节点</li><li>-c&nbsp;:&nbsp;创建一个容器节点</li><li>[-t&nbsp;ttl]&nbsp;:&nbsp;创建一个TTL节点，&nbsp;-t&nbsp;时间（单位毫秒）</li><li>data&nbsp;:&nbsp;节点的数据，可选，如果不使用时，节点数据就为null</li><li>acl&nbsp;:&nbsp;访问控制</li></ul></li><li>get 获取节点数据信息<ul><li>-s:&nbsp;节点状态信息（时间戳、版本号、数据大小等）</li><li>-w:&nbsp;监听节点变化</li></ul></li><li>set 设置节点数据<ul><li>-s: 表示节点为顺序节点</li><li>-v:&nbsp;指定版本号</li></ul></li><li>getAcl 获取节点的访问控制信息<ul><li>-s:&nbsp;节点状态信息（时间戳、版本号、数据大小等）</li></ul></li><li>setAcl 设置节点的访问控制列表<ul><li>-s: 节点状态信息（时间戳、版本号、数据大小等）</li><li>-v: 指定版本号</li><li>-R: 递归的设置</li></ul></li><li>stat 查看节点状态信息</li><li>delete  删除某一节点，只能删除无子节点的节点<ul><li>-v: 表示节点版本号</li></ul></li><li>deleteall 递归的删除某一节点及其子节点</li><li>setquota 对节点增加限制<ul><li>-n: 表示子节点的最大个数</li><li>-b: 数据值的最大长度，-1表示无限制</li></ul></li></ul><hr><h2 id="数据结构">数据结构</h2><ul><li>ZooKeeper的数据模型是层次模型，层次模型常见于文件系统。层次模型和key-value模型是两种主流的数据模型。ZooKeeper使用文件系统模型主要基于以下两点考虑<ul><li>文件系统的树形结构便于表达数据之间的层次关系</li><li>文件系统的树形结构便于为不同的应用分配独立的命名空间 (&nbsp;namespace&nbsp;)</li></ul></li><li>ZooKeeper的层次模型称作Data&nbsp;Tree，Data&nbsp;Tree的每个节点叫作Znode<ul><li>每一个&nbsp;ZNode&nbsp;默认能够存储&nbsp;1MB&nbsp;的数据</li><li>每个&nbsp;ZNode&nbsp;都可以通过其路径唯一标识</li><li>每个节点都有一个版本(version)，版本从0开始计数</li><li><img src="/static/IT/Zookeeper/Zookeeper-%E7%89%B9%E6%80%A7-1.png" alt=""></li></ul></li><li>节点分类<ul><li>持久节点 (PERSISTENT):&nbsp;这样的znode在创建之后即使发生ZooKeeper集群宕机或者client宕机也不会丢失</li><li>临时节点 (EPHEMERAL&nbsp;):&nbsp;client宕机或者client在指定的timeout时间内没有给ZooKeeper集群发消息，这样的znode就会消失</li><li>持久顺序节点 (PERSISTENT_SEQUENTIAL):&nbsp;znode除了具备持久性znode的特点之外，名字具备顺序性</li><li>临时顺序节点 (EPHEMERAL_SEQUENTIAL):&nbsp;znode除了具备临时性znode的特点之外，名字具备顺序性</li><li>Container节点&nbsp;(3.5.3版本新增)：Container容器节点，当容器中没有任何子节点，该容器节点会被zk定期删除（定时任务默认60s&nbsp;检查一次)<ul><li>和持久节点的区别是&nbsp;ZK&nbsp;服务端启动后，会有一个单独的线程去扫描，所有的容器节点，当发现容器节点的子节点数量为&nbsp;0&nbsp;时，会自动删除该节点</li><li>可以用于&nbsp;leader&nbsp;或者锁的场景中</li></ul></li><li>TTL节点:&nbsp;&nbsp;带过期时间节点，默认禁用<ul><li>在zoo.cfg中添加&nbsp;<code>extendedTypesEnabled=true&nbsp;</code>开启</li><li>ttl 不能用于临时节点</li></ul></li></ul></li><li>节点状态信息<ul><li>cZxid&nbsp;：Znode创建的事务id</li><li>ctime：节点创建时的时间戳</li><li>mZxid&nbsp;：Znode被修改的事务id，即每次对znode的修改都会更新mZxid<ul><li>对于zk来说，每次的变化都会产生一个唯一的事务id，zxid（ZooKeeper&nbsp;Transaction&nbsp;Id）</li><li>通过zxid，可以确定更新操作的先后顺序</li><li>如果zxid1小于zxid2，说明zxid1操作先于zxid2发生</li><li>zxid对于整个zk都是唯一的，即使操作的是不同的znode</li></ul></li><li>pZxid:&nbsp;表示该节点的子节点列表最后一次修改的事务ID<ul><li>只有子节点列表变更了才会变更pzxid，子节点内容变更不会影响pzxid<ul><li>添加子节点或删除子节点就会影响子节点列表</li><li>但是修改子节点的数据内容则不影响该ID</li></ul></li></ul></li><li>mtime：节点最新一次更新发生时的时间戳</li><li>cversion&nbsp;：子节点的版本号<ul><li>当znode的子节点有变化时，cversion&nbsp;的值就会增加1</li></ul></li><li>dataVersion：数据版本号<ul><li>每次对节点进行set操作，dataVersion的值都会增加1（即使设置的是相同的数据）</li><li>可有效避免了数据更新时出现的先后顺序问题</li></ul></li><li>ephemeralOwner<ul><li>如果该节点为临时节点,&nbsp;ephemeralOwner值表示与该节点绑定的session&nbsp;id<ul><li>在client和server通信之前,首先需要建立连接,该连接称为session</li><li>连接建立后,如果发生连接超时、授权失败,或者显式关闭连接,连接便处于closed状态,&nbsp;此时session结束</li></ul></li><li>如果不是,&nbsp;ephemeralOwner值为0 (持久节点)</li></ul></li><li>dataLength：&nbsp;数据的长度</li><li>numChildren：子节点的数量（只统计直接子节点的数量）</li></ul></li></ul><hr><h2 id="监听通知机制">监听通知机制</h2><ul><li>watcher 机制<ul><li>一个Watch事件是一个一次性的触发器<ul><li>当被设置了Watch的数据发生了改变的时候，则服务器将这个改变发送给设置了Watch的客户端，以便通知它们</li></ul></li><li>Zookeeper采用了&nbsp;Watcher机制实现数据的发布订阅功能<ul><li>多个订阅者可同时监听某一特定主题对象，当该主题对象的自身状态发生变化时例如节点内容改变、节点下的子节点列表改变等，会实时、主动通知所有订阅者</li></ul></li><li>watcher机制事件上与观察者模式类似，也可看作是一种观察者模式在分布式场景下的实现方式</li></ul></li><li>watcher 的过程<ul><li>客户端向服务端注册watcher</li><li>服务端事件发生触发watcher</li><li>客户端回调watcher得到触发事件情况</li></ul></li><li>Zookeeper中的watch机制，必须客户端先去服务端注册监听，这样事件发送才会触发监听，通知给客户端</li><li>支持的事件类型<ul><li>None:&nbsp;连接建立事件</li><li>NodeCreated：&nbsp;节点创建</li><li>NodeDeleted：&nbsp;节点删除</li><li>NodeDataChanged：节点数据变化</li><li>NodeChildrenChanged：子节点列表变化</li><li>DataWatchRemoved：节点监听被移除</li><li>ChildWatchRemoved：子节点监听被移除</li></ul></li><li>特性<ul><li>一次性触发：watcher是一次性的，一旦被触发就会移除，再次使用时需要重新注册</li><li>客户端顺序回调：watcher回调是顺序串行执行的，只有回调后客户端才能看到最新的数据状态。一个watcher回调逻辑不应该太多，以免影响别的watcher执行</li><li>轻量级：WatchEvent是最小的通信单位，结构上只包含通知状态、事件类型和节点路径，并不会告诉数据节点变化前后的具体内容</li><li>时效性：watcher只有在当前session彻底失效时才会无效，若在session有效期内快速重连成功，则watcher依然存在，仍可接收到通知</li></ul></li><li>使用场景<ul><li>master-worker 机制</li><li>基于版本号的条件更新<ul><li><img src="/static/IT/Zookeeper/Zookeeper-%E7%89%B9%E6%80%A7-2.png" alt=""></li></ul></li></ul></li></ul><hr><h2 id="节点特性">节点特性</h2><ul><li>同一级节点&nbsp;key&nbsp;名称是唯一的</li><li>创建节点时，必须要带上全路径</li><li>session&nbsp;关闭，临时节点清除</li><li>自动创建顺序节点</li><li>watch&nbsp;机制，监听节点变化<ul><li>监听事件被单次触发后，事件就失效了</li></ul></li><li>永久性 Watch（<code>addWatch&nbsp;[‐m&nbsp;mode]&nbsp;path</code>）：是Zookeeper&nbsp;3.6.0版本新增的功能<ul><li>在被触发之后，仍然保留，可以继续监听ZNode上的变更</li><li>针对指定节点添加事件监听，支持两种模式<ul><li>PERSISTENT，持久化订阅，针对当前节点的修改和删除事件，以及当前节点的子节点的删除和新增事件</li><li>PERSISTENT_RECURSIVE，持久化递归订阅，在PERSISTENT的基础上，增加了子节点修改的事件触发，以及子节点的子节点的数据变化都会触发相关事件</li></ul></li></ul></li><li>delete&nbsp;命令只能一层一层删除</li><li>deleteall&nbsp;命令递归删除</li><li>应用场景：适用于存储和协同相关的关键数据，不适合用于大数据量存储<ul><li>注册中心</li><li>数据发布/订阅（常用于实现配置中心）<ul><li>数据量小的KV</li><li>数据内容在运行时会发生动态变化</li><li>集群机器共享，配置一致</li><li>推拉结合<ul><li>服务端会推给注册了监控节点的客户端&nbsp;Watcher&nbsp;事件通知</li><li>客户端获得通知后，然后主动到服务端拉取最新的数据</li></ul></li></ul></li><li>统一集群管理</li><li>负载均衡</li><li>命名服务</li><li>分布式协调/通知</li><li>集群管理</li><li>Master选举</li><li>分布式锁</li><li>分布式队列</li></ul></li></ul><hr><h2 id="ACL权限控制">ACL权限控制</h2><ul><li>zookeeper 的 ACL（Access Control List，访问控制表）权限可以针对节点设置相关读写等权限</li><li>zookeeper 的 acl 通过&nbsp;<code>[scheme:id:permissions]</code>&nbsp;来构成权限列表<ul><li>scheme：授权的模式，代表采用的某种权限机制<ul><li>包括 world、auth、digest、ip、super 几种</li></ul></li><li>id：授权对象，代表允许访问的用户<ul><li>如果我们选择采用 IP 方式，使用的授权对象可以是一个 IP 地址或 IP 地址段</li><li>而如果使用 Digest 或 Super 方式，则对应于一个用户名</li><li>如果是 World 模式，是授权系统中所有的用户</li></ul></li><li>permissions：授权的权限，权限组合字符串，由 cdrwa 组成，其中每个字母代表支持不同权限<ul><li>创建权限 create©、删除权限 delete(d)、读权限 read®、写权限 write(w)、管理权限admin(a)。</li></ul></li></ul></li></ul><table><thead><tr><th>模式</th><th>描述</th></tr></thead><tbody><tr><td>world</td><td>授权对象只有一个anyone，代表登录到服务器的所有客户端都能对该节点执行某种权限</td></tr><tr><td>ip</td><td>对连接的客户端使用IP地址认证方式进行认证</td></tr><tr><td>auth</td><td>使用以添加认证的用户进行认证</td></tr><tr><td>digest</td><td>使用用户:密码方式验证</td></tr></tbody></table><table><thead><tr><th>权限类型</th><th>ACL简写</th><th>描述</th></tr></thead><tbody><tr><td>read</td><td>r</td><td>读取节点及显示子节点列表的权限</td></tr><tr><td>write</td><td>w</td><td>设置节点数据的权限</td></tr><tr><td>create</td><td>c</td><td>创建子节点的权限</td></tr><tr><td>delete</td><td>d</td><td>删除子节点的权限</td></tr><tr><td>admin</td><td>a</td><td>设置该节点ACL权限的权限</td></tr></tbody></table><table><thead><tr><th>授权命令</th><th>用法</th><th>描述</th></tr></thead><tbody><tr><td>getAcl</td><td>getAcl path</td><td>读取节点的ACL</td></tr><tr><td>setAcl</td><td>setAcl path acl</td><td>设置节点的ACL</td></tr><tr><td>create</td><td>create path data acl</td><td>创建节点时设置ACL</td></tr><tr><td>addAuth</td><td>addAuth scheme auth</td><td>添加认证用户，类似于登录操作</td></tr></tbody></table><ul><li>setAcl<ul><li><code>set Acl /name world:anyone:cdwa</code></li></ul></li><li>auth授权模式<ul><li>创建用户 <code>addauth digest fox:123456</code></li><li><code>setAcl /name auth:fox:123456:cdrwa</code></li><li>密码加密<ul><li><code>echo -n fox:123456 | openssl dgst -binary -sha1 | openssl base64</code></li><li><code>setAcl /name auth:fox:ZsWwgmtnTnx1usRF1voHFJAYGQU=:cdrwa</code></li></ul></li></ul></li><li>digest授权模式<ul><li><code>setAcl /tuling/fox digest:fox:ZsWwgmtnTnx1usRF1voHFJAYGQU=:cdrwa</code></li></ul></li><li>IP授权模式<ul><li><code>setAcl /node-ip ip:192.168.109.128:cdwra</code></li><li><code>create /node-ip data ip:192.168.109.128:cdwra</code><ul><li>多个指定IP可以通过逗号分隔<ul><li><code>setAcl /node-ip ip:IP1:rw,ip:IP2:a</code></li></ul></li></ul></li></ul></li><li>Super 超级管理员模式<ul><li>这是一种特殊的Digest模式， 在Super模式下超级管理员用户可以对Zookeeper上的节点进行任何的操作</li><li>需要在启动脚本上通过添加JVM 参数开启<ul><li><code>-Dzookeeper.DigestAuthenticationProvider.superDigest=admin:&lt;base64encoded(SHA1(123456))</code></li></ul></li></ul></li></ul><hr><h2 id="集群">集群</h2><ul><li>集群角色<ul><li>Leader：&nbsp;领导者<ul><li>事务请求（写操作）的唯一调度者和处理者，保证集群事务处理的顺序性</li><li>集群内部各个服务器的调度者</li><li>对于create、setData、delete等有写操作的请求，则要统一转发给leader处理，leader需要决定编号、执行操作，这个过程称为事务</li></ul></li><li>Follower：跟随者<ul><li>处理客户端非事务（读操作）请求（可以直接响应）</li><li>转发事务请求给 Leader</li><li>参与集群 Leader 选举投票</li></ul></li><li>Observer：观察者<ul><li>对于非事务请求可以独立处理（读操作）</li><li>对于事务性请求会转发给 leader 处理</li><li>Observer 节点接收来自 leader 的 inform 信息，更新自己的本地存储</li><li>不参与提交和选举投票</li><li>在不影响集群事务处理能力的前提下提升集群的非事务处理能力</li><li>Observer 应用场景<ul><li>提升集群的读性能</li><li>跨数据中心部署<ul><li>比如需要部署一个北京和香港两地都可以使用的zookeeper集群服务，并且要求北京和香港客户的读请求延迟都很低。解决方案就是把香港的节点都设置为observer</li></ul></li></ul></li></ul></li></ul></li><li>集群架构<ul><li><img src="/static/IT/Zookeeper/Zookeeper-%E7%89%B9%E6%80%A7-3.png" alt=""></li><li>leader节点可以处理读写请求</li><li>follower只可以处理读请求</li><li>follower在接到写请求时会把写请求转发给leader来处理</li></ul></li><li>Zookeeper数据一致性保证<ul><li>全局可线性化 (Linearizable) 写入：先到达leader的写请求会被先处理，leader决定写请求的执行顺序</li><li>客户端FIFO顺序：来自给定客户端的请求按照发送顺序执行</li></ul></li><li>集群搭建<ul><li>修改zoo.cfg配置，添加server节点配置：<code>server.A=B:C:D</code><ul><li><code>dataDir=/data/zookeeper</code></li><li><code>server.1=192.168.65.156:2888:3888</code></li><li>A&nbsp;是一个数字，表示这个是第几号服务器<ul><li>集群模式下配置一个文件&nbsp;myid，这个文件在&nbsp;dataDir&nbsp;目录下，这个文件里面有一个数据&nbsp;就是&nbsp;A&nbsp;的值，Zookeeper&nbsp;启动时读取此文件，拿到里面的数据与&nbsp;zoo.cfg&nbsp;里面的配置信息比较从而判断到底是哪个server</li></ul></li><li>B&nbsp;是这个服务器的地址</li><li>C&nbsp;是这个服务器Follower与集群中的Leader服务器交换信息的端口</li><li>D&nbsp;是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader<ul><li>而这个端口就是用来执行选举时服务器相互通信的端口</li></ul></li></ul></li><li>创建&nbsp;myid&nbsp;文件，配置服务器编号</li><li>启动 zookeeper&nbsp;server 集群 <code>bin/zkServer.sh&nbsp;start</code></li></ul></li></ul><hr><h2 id="四字命令">四字命令</h2><ul><li>zookeeper 支持某些特定的四字命令与其交互，用户获取 zookeeper 服务的当前状态及相关信息<ul><li>用户在客户端可以通过 telenet 或者 nc（netcat） 向 zookeeper 提交相应的命令</li></ul></li><li>开启四字命令<ul><li>在 zoo.cfg 文件里加入配置项让这些指令放行<ul><li><code>4lw.commands.whitelist=*</code></li></ul></li><li>在 zk 的启动脚本 <a href="http://zkServer.sh">zkServer.sh</a> 中新增放行指令（添加ＶＭ环境变量）<ul><li><code>ZOOMAIN="-Dzookeeper.4lw.commands.whitelist=* ${ZOOMAIN}"</code></li></ul></li></ul></li><li><code>echo [command] | nc [ip] [port]</code><ul><li>stat 命令用于查看 zk 的状态信息<ul><li><code>echo stat | nc 192.168.65.156 2181</code></li></ul></li></ul></li></ul><table><thead><tr><th>四字命令</th><th>功能描述</th></tr></thead><tbody><tr><td>conf</td><td>3.3.0版本引入的。打印出服务相关配置的详细信息。</td></tr><tr><td>cons</td><td>3.3.0版本引入的。列出所有连接到这台服务器的客户端全部连接/会话详细信息。包括"接受/发送"的包数量、会话id、操作延迟、最后的操作执行等等信息。</td></tr><tr><td>crst</td><td>3.3.0版本引入的。重置所有连接的连接和会话统计信息。</td></tr><tr><td>dump</td><td>列出那些比较重要的会话和临时节点。这个命令只能在leader节点上有用。</td></tr><tr><td>envi</td><td>打印出服务环境的详细信息。</td></tr><tr><td>reqs</td><td>列出未经处理的请求</td></tr><tr><td>ruok</td><td>测试服务是否处于正确状态。如果确实如此，那么服务返回"imok"，否则不做任何相应。</td></tr><tr><td>stat</td><td>输出关于性能和连接的客户端的列表。</td></tr><tr><td>srst</td><td>重置服务器的统计。</td></tr><tr><td>srvr</td><td>3.3.0版本引入的。列出连接服务器的详细信息</td></tr><tr><td>wchs</td><td>3.3.0版本引入的。列出服务器watch的详细信息。</td></tr><tr><td>wchc</td><td>3.3.0版本引入的。通过session列出服务器watch的详细信息，它的输出是一个与watch相关的会话的列表。</td></tr><tr><td>wchp</td><td>3.3.0版本引入的。通过路径列出服务器watch的详细信息。它输出一个与session相关的路径。</td></tr><tr><td>mntr</td><td>3.4.0版本引入的。输出可用于检测集群健康状态的变量列表</td></tr></tbody></table><hr><h2 id="Leader-选举原理">Leader&nbsp;选举原理</h2><ul><li>zookeeper&nbsp;的&nbsp;leader&nbsp;选举存在两个阶段<ul><li>一个是服务器启动时&nbsp;leader&nbsp;选举</li><li>另一个是运行过程中&nbsp;leader&nbsp;服务器宕机</li></ul></li><li>重要的参数<ul><li>服务器&nbsp;ID(myid)：编号越大在选举算法中权重越大</li><li>事务&nbsp;ID(zxid)：值越大说明数据越新，权重越大</li><li>逻辑时钟(epoch-logicalclock)：同一轮投票过程中的逻辑时钟值是相同的，每投完一次值会增加</li></ul></li><li>选举状态<ul><li>LOOKING:&nbsp;竞选状态</li><li>FOLLOWING:&nbsp;随从状态，同步&nbsp;leader&nbsp;状态，参与投票</li><li>OBSERVING:&nbsp;观察状态，同步&nbsp;leader&nbsp;状态，不参与投票</li><li>LEADING:&nbsp;领导者状态</li></ul></li><li>服务器启动时的&nbsp;leader&nbsp;选举<ul><li>每个节点启动的时候都&nbsp;LOOKING&nbsp;观望状态，接下来就开始进行选举主流程<ul><li>第一台服务器&nbsp;server1启动时，无法进行&nbsp;leader&nbsp;选举</li><li>当第二台服务器&nbsp;server2&nbsp;启动时，两台机器可以相互通信，进入&nbsp;leader&nbsp;选举过程</li></ul></li><li><img src="/static/IT/Zookeeper/Zookeeper-%E7%89%B9%E6%80%A7-4.png" alt=""><ol><li>每台&nbsp;server&nbsp;发出一个投票<ol><li>由于是初始情况，server1&nbsp;和&nbsp;server2&nbsp;都将自己作为&nbsp;leader&nbsp;服务器进行投票</li><li>每次投票包含所推举的服务器myid、zxid、epoch，使用（myid，zxid）表示</li><li>此时&nbsp;server1&nbsp;投票为（1,0），server2&nbsp;投票为（2,0），然后将各自投票发送给集群中其他机器</li></ol></li><li>接收来自各个服务器的投票<ol><li>集群中的每个服务器收到投票后，首先判断该投票的有效性</li><li>如检查是否是本轮投票（epoch）、是否来自&nbsp;LOOKING&nbsp;状态的服务器</li></ol></li><li>分别处理投票<ol><li>针对每一次投票，服务器都需要将其他服务器的投票和自己的投票进行对比<ol><li>优先比较&nbsp;epoch</li><li>检查&nbsp;zxid，zxid&nbsp;比较大的服务器优先作为&nbsp;leader</li><li>如果&nbsp;zxid&nbsp;相同，那么就比较&nbsp;myid，myid&nbsp;较大的服务器作为&nbsp;leader&nbsp;服务器</li></ol></li></ol></li><li>统计投票<ol><li>每次投票后，服务器统计投票信息，判断是否有过半机器接收到相同的投票信息</li><li>server1、server2&nbsp;都统计出集群中有两台机器接受了（2,0）的投票信息，此时已经选出了&nbsp;server2&nbsp;为&nbsp;leader&nbsp;节点</li></ol></li><li>改变服务器状态<ol><li>一旦确定了&nbsp;leader，每个服务器响应更新自己的状态</li><li>如果是&nbsp;follower，那么就变更为&nbsp;FOLLOWING，如果是&nbsp;Leader，变更为&nbsp;LEADING</li><li>此时&nbsp;server3继续启动，直接加入变更自己为&nbsp;FOLLOWING</li></ol></li></ol></li></ul></li><li>运行过程中的&nbsp;leader&nbsp;选举：当集群中&nbsp;leader&nbsp;服务器出现宕机或者不可用情况时，整个集群无法对外提供服务，进入新一轮的&nbsp;leader&nbsp;选举<ol><li>变更状态：leader&nbsp;挂后，其他非&nbsp;Oberver服务器将自身服务器状态变更为&nbsp;LOOKING</li><li>每个&nbsp;server&nbsp;发出一个投票：在运行期间，每个服务器上&nbsp;zxid&nbsp;可能不同</li><li>处理投票：规则同启动过程</li><li>统计投票：与启动过程相同</li><li>改变服务器状态：与启动过程相同</li></ol></li></ul><hr><h2 id="数据同步流程">数据同步流程</h2><ul><li>在&nbsp;Zookeeper&nbsp;中，主要依赖&nbsp;ZAB&nbsp;协议来实现分布式数据一致性</li><li>ZAB&nbsp;协议分为两部分：消息广播；崩溃恢复</li><li>消息广播<ul><li><img src="/static/IT/Zookeeper/Zookeeper-%E7%89%B9%E6%80%A7-5.png" alt=""></li><li>Zookeeper&nbsp;使用单一的主进程&nbsp;Leader&nbsp;来接收和处理客户端所有事务请求</li><li>并采用&nbsp;ZAB&nbsp;协议的原子广播协议，将事务请求以&nbsp;Proposal&nbsp;提议广播到所有&nbsp;Follower&nbsp;节点</li><li>当集群中有过半的Follower&nbsp;服务器进行正确的&nbsp;ACK&nbsp;反馈<ul><li>那么Leader就会再次向所有的&nbsp;Follower&nbsp;服务器发送commit&nbsp;消息，将此次提案进行提交</li></ul></li><li>这个过程可以简称为&nbsp;2pc&nbsp;事务提交</li><li>注意&nbsp;Observer&nbsp;节点只负责同步&nbsp;Leader&nbsp;数据，不参与&nbsp;2PC&nbsp;数据同步过程</li></ul></li><li>崩溃恢复<ul><li>在正常情况消息下广播能运行良好，但是一旦&nbsp;Leader&nbsp;服务器出现崩溃，或者由于网络原理导致&nbsp;Leader&nbsp;服务器失去了与过半&nbsp;Follower&nbsp;的通信，那么就会进入崩溃恢复模式</li><li>需要选举出一个新的&nbsp;Leader&nbsp;服务器</li><li>在这个过程中可能会出现两种数据不一致性的隐患，需要&nbsp;ZAB&nbsp;协议的特性进行避免<ul><li>Leader&nbsp;服务器将消息&nbsp;commit&nbsp;发出后，立即崩溃</li><li>Leader&nbsp;服务器刚提出&nbsp;proposal&nbsp;后，立即崩溃</li></ul></li><li>ZAB&nbsp;协议的恢复模式使用了以下策略<ul><li>选举&nbsp;zxid&nbsp;最大的节点作为新的&nbsp;leader</li><li>新&nbsp;leader&nbsp;将事务日志中尚未提交的消息进行处理</li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;CP架构&lt;/li&gt;
&lt;li&gt;常见命令&lt;/li&gt;
&lt;li&gt;数据结构&lt;/li&gt;
&lt;li&gt;监听通知机制&lt;/li&gt;
&lt;li&gt;节点特性&lt;/li&gt;
&lt;li&gt;ACL权限控制&lt;/li&gt;
&lt;li&gt;集群&lt;/li&gt;
&lt;li&gt;四字命令&lt;/li&gt;
&lt;li&gt;Leader&amp;nbsp;选举原理</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Zookeeper" scheme="https://jxch.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper-ZAB</title>
    <link href="https://jxch.github.io/2024/09/09/architect/zookeeper/zookeeper-zab/"/>
    <id>https://jxch.github.io/2024/09/09/architect/zookeeper/zookeeper-zab/</id>
    <published>2024-09-09T02:41:00.000Z</published>
    <updated>2024-09-09T02:44:46.657Z</updated>
    
    <content type="html"><![CDATA[<ul><li>ZAB</li><li>消息广播</li><li>崩溃恢复</li><li>数据同步</li></ul><hr><h2 id="ZAB">ZAB</h2><ul><li>整个Zookeeper就是一个多节点分布式一致性算法的实现，底层采用的实现协议是ZAB</li><li>ZAB&nbsp;协议全称：Zookeeper&nbsp;Atomic&nbsp;Broadcast（Zookeeper&nbsp;原子广播协议）<ul><li>ZAB是Paxos算法的一种简化实现</li></ul></li><li>ZAB&nbsp;协议是为分布式协调服务&nbsp;Zookeeper&nbsp;专门设计的一种支持&nbsp;崩溃恢复&nbsp;和&nbsp;原子广播&nbsp;的协议<ul><li>当&nbsp;Leader&nbsp;服务可以正常使用，就进入消息广播模式</li><li>当&nbsp;Leader&nbsp;不可用时，则进入崩溃恢复模式</li></ul></li><li>基于该协议，Zookeeper&nbsp;实现了一种&nbsp;主备模式&nbsp;的系统架构来保持集群中各个副本之间数据一致性</li><li><img src="/static/IT/Zookeeper/Zookeeper-ZAB-1.png" alt=""><ul><li>所有客户端写入数据都是写入到Leader节点</li><li>然后，由&nbsp;Leader&nbsp;复制到Follower节点中，从而保证数据一致性</li><li>复制过程类似两阶段提交 (2PC)<ul><li>ZAB&nbsp;只需要&nbsp;Follower(含leader自己的ack)&nbsp;有一半以上返回&nbsp;Ack&nbsp;信息就可以执行提交<ul><li>大大减小了同步阻塞；也提高了可用性</li></ul></li></ul></li></ul></li></ul><h2 id="消息广播">消息广播</h2><ul><li>ZAB&nbsp;协议的消息广播过程使用的是一个原子广播协议，类似一个&nbsp;两阶段提交过程</li><li>对于客户端发送的写请求，全部由&nbsp;Leader&nbsp;接收，Leader&nbsp;将请求封装成一个事务&nbsp;Proposal，将其发送给所有&nbsp;Follwer</li><li>然后，根据所有&nbsp;Follwer&nbsp;的反馈，如果超过半数 (含leader自己) 成功响应，则执行&nbsp;commit&nbsp;操作</li><li><img src="/static/IT/Zookeeper/Zookeeper-ZAB-2.png" alt=""><ul><li>Leader&nbsp;在收到客户端请求之后，会将这个请求封装成一个事务，并给这个事务分配一个全局递增的唯一&nbsp;ID，称为事务ID（ZXID），ZAB&nbsp;协议需要保证事务的顺序，因此必须将每一个事务按照&nbsp;ZXID&nbsp;进行先后排序然后处理，主要通过消息队列实现</li><li>在&nbsp;Leader&nbsp;和&nbsp;Follwer&nbsp;之间还有一个消息队列，用来解耦他们之间的耦合，解除同步阻塞</li><li>zookeeper集群中为保证所有进程能够有序的顺序执行，只能是&nbsp;Leader&nbsp;服务器接受写请求，即使是&nbsp;Follower&nbsp;服务器接受到客户端的写请求，也会转发到&nbsp;Leader&nbsp;服务器进行处理，Follower只能处理读请求</li><li>ZAB协议规定了如果一个事务在一台机器上被处理(commit)成功，那么应该在所有的机器上都被处理成功，哪怕机器出现故障崩溃</li></ul></li></ul><h2 id="崩溃恢复">崩溃恢复</h2><ul><li>当&nbsp;Leader&nbsp;崩溃，即进入崩溃恢复模式（崩溃即：Leader&nbsp;失去与过半&nbsp;Follwer&nbsp;的联系）<ul><li>Leader&nbsp;在复制数据给所有&nbsp;Follwer&nbsp;之后，还没来得及收到Follower的ack返回就崩溃</li><li>Leader&nbsp;在收到&nbsp;ack&nbsp;并提交了自己，同时发送了部分&nbsp;commit&nbsp;出去之后崩溃</li></ul></li><li>针对这些问题，ZAB&nbsp;定义了&nbsp;2&nbsp;个原则<ul><li>ZAB&nbsp;协议确保丢弃那些只在&nbsp;Leader&nbsp;提出/复制，但没有提交的事务</li><li>ZAB&nbsp;协议确保那些已经在&nbsp;Leader&nbsp;提交的事务最终会被所有服务器提交</li></ul></li><li>所以，ZAB&nbsp;设计了下面这样一个选举算法：能够确保提交已经被&nbsp;Leader&nbsp;提交的事务，同时丢弃已经被跳过的事务<ul><li>让&nbsp;Leader&nbsp;选举算法能够保证新选举出来的&nbsp;Leader&nbsp;服务器拥有集群中所有机器&nbsp;ZXID&nbsp;最大的事务<ul><li>可以省去&nbsp;Leader&nbsp;服务器检查事务的提交和丢弃工作的这一步操作</li></ul></li></ul></li></ul><h2 id="数据同步">数据同步</h2><ul><li>当崩溃恢复之后，需要在正式工作之前（接收客户端请求），Leader&nbsp;服务器首先确认事务是否都已经被过半的&nbsp;Follwer&nbsp;提交了，即是否完成了数据同步。目的是为了保持数据一致</li><li>当&nbsp;Follwer&nbsp;服务器成功同步之后，Leader&nbsp;会将这些服务器加入到可用服务器列表中</li><li>实际上，Leader&nbsp;服务器处理或丢弃事务都是依赖着&nbsp;ZXID&nbsp;的<ul><li><img src="/static/IT/Zookeeper/Zookeeper-ZAB-3.png" alt=""></li><li>在&nbsp;ZAB&nbsp;协议的事务编号&nbsp;ZXID&nbsp;设计中，ZXID&nbsp;是一个&nbsp;64&nbsp;位的数字<ul><li>其中低&nbsp;32&nbsp;位可以看作是一个简单的递增的计数器，针对客户端的每一个事务请求，Leader&nbsp;都会产生一个新的事务&nbsp;Proposal&nbsp;并对该计数器进行&nbsp;+&nbsp;1&nbsp;操作</li><li>而高&nbsp;32&nbsp;位则代表了&nbsp;Leader&nbsp;服务器上取出本地日志中最大事务&nbsp;Proposal&nbsp;的&nbsp;ZXID，并从该&nbsp;ZXID&nbsp;中解析出对应的&nbsp;epoch&nbsp;值 (leader选举周期)，当一轮新的选举结束后，会对这个值加一，并且事务id又从0开始自增</li></ul></li><li>高&nbsp;32&nbsp;位代表了每代&nbsp;Leader&nbsp;的唯一性，低&nbsp;32&nbsp;代表了每代&nbsp;Leader&nbsp;中事务的唯一性<ul><li>同时，也能让&nbsp;Follwer&nbsp;通过高&nbsp;32&nbsp;位识别不同的&nbsp;Leader。简化了数据恢复流程</li></ul></li></ul></li><li>当&nbsp;Follower&nbsp;连接上&nbsp;Leader&nbsp;之后，Leader&nbsp;服务器会根据自己服务器上最后被提交的&nbsp;ZXID&nbsp;和&nbsp;Follower&nbsp;上的&nbsp;ZXID&nbsp;进行比对，比对结果要么回滚，要么和&nbsp;Leader&nbsp;同步</li></ul><hr><p><img src="/static/IT/Zookeeper/Zookeeper-ZAB-4.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;ZAB&lt;/li&gt;
&lt;li&gt;消息广播&lt;/li&gt;
&lt;li&gt;崩溃恢复&lt;/li&gt;
&lt;li&gt;数据同步&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;ZAB&quot;&gt;ZAB&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;整个Zookeeper就是一个多节点分布式一致性算法的实现，底层采用的实现协</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Zookeeper" scheme="https://jxch.github.io/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>RocketMQ-整体架构</title>
    <link href="https://jxch.github.io/2024/09/09/architect/rocketmq/rocketmq-zheng-ti-jia-gou/"/>
    <id>https://jxch.github.io/2024/09/09/architect/rocketmq/rocketmq-zheng-ti-jia-gou/</id>
    <published>2024-09-09T02:33:00.000Z</published>
    <updated>2024-09-09T02:37:37.979Z</updated>
    
    <content type="html"><![CDATA[<ul><li>NameServer的结构</li><li>Broker的结构</li><li>Netty服务注册框架</li><li>RocketMQ的同步结果推送与异步结果推送</li><li>Broker心跳注册过程</li><li>Producer发送消息过程</li><li>Consumer拉取消息过程</li><li>文件存储</li><li>延迟消息</li><li>长轮询机制</li></ul><hr><h2 id="NameServer的结构">NameServer的结构</h2><ul><li>NameServer的核心作用<ul><li>一是维护Broker的服务地址并进行及时的更新</li><li>二是给Producer和Consumer提供服务获取Broker列表</li></ul></li><li>NameServer的启动入口为NamesrvStartup类的main方法</li><li>整个NameServer的核心就是一个NamesrvController对象</li></ul><p><img src="/static/IT/RocketMQ/RocketMQ-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84-1.png" alt=""></p><hr><h2 id="Broker的结构">Broker的结构</h2><ul><li>Broker是整个RocketMQ的业务核心，所有消息存储、转发这些最为重要的业务都是在Broker中进行处理的</li><li>Broker启动的入口在BrokerStartup这个类的main方法</li><li>重点也是围绕一个BrokerController对象</li><li>在BrokerStartup.createBrokerController方法中可以看到Broker的几个核心配置<ul><li>BrokerConfig</li><li>NettyServerConfig  ：Netty服务端占用了10911端口。同样也可以在配置文件中覆盖</li><li>NettyClientConfig</li><li>MessageStoreConfig</li></ul></li></ul><p><img src="/static/IT/RocketMQ/RocketMQ-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84-2.png" alt=""></p><hr><h2 id="Netty服务注册框架">Netty服务注册框架</h2><ul><li>RocketMQ使用Netty框架提供了一套基于服务码的服务注册机制，让各种不同的组件都可以按照自己的需求，注册自己的服务方法</li><li>Netty的所有远程通信功能都由remoting模块实现。RemotingServer模块里包含了RPC的服务端RemotingServer以及客户端RemotingClient</li><li>RocketMQ基于Netty保持客户端与服务端的长连接Channel</li><li>所有的请求都封装成RemotingCommand对象。而每个处理消息的服务逻辑，会封装成一个NettyRequestProcessor对象</li><li>服务端和客户端都维护了一个processorTable，这是个HashMap，key是服务码requestCode，value是对应的运行单元<ul><li>Pair&lt;NettyRequestProcessor, ExecutorService&gt;  类型，包含了处理逻辑Prcessor和执行线程池ExecutorService</li></ul></li><li>服务端的注册BrokerController.registerProcessor() ，客户端的服务注册见MQClientAPIImpl。NameServer则会注册一个大的DefaultRequestProcessor，统一处理所有的服务</li><li>服务注册流程：</li></ul><p><img src="/static/IT/RocketMQ/RocketMQ-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84-3.png" alt=""></p><ul><li>NameServer会维护Broker的路由列表，并对路由列表进行实时更新</li><li>BrokerController.this.registerBrokerAll方法会发起向NameServer注册心跳。启动时会立即注册，同时也会启动一个线程池，以10秒延迟，默认30秒的间隔持续向NameServer发送心跳</li><li>BrokerController.this.registerBrokerAll这个方法就是注册心跳的入口</li><li>然后，在NameServer中也会启动一个定时任务，扫描不活动的Broker。具体观察NamesrvController.initialize方法</li></ul><p><img src="/static/IT/RocketMQ/RocketMQ-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84-4.png" alt=""></p><hr><h2 id="RocketMQ的同步结果推送与异步结果推送">RocketMQ的同步结果推送与异步结果推送</h2><ul><li>RocketMQ的RemotingServer服务端，会维护一个responseTable，这是一个线程同步的Map结构。 key为请求的ID，value是异步的消息结果 ConcurrentMap&lt;Integer  , ResponseFuture&gt;</li><li>处理同步请求(NettyRemotingAbstract#invokeSyncImpl)时，处理的结果会存入responseTable，通过ResponseFuture提供一定的服务端异步处理支持，提升服务端的吞吐量。 请求返回后，立即从responseTable中移除请求记录</li><li>处理异步请求(NettyRemotingAbstract#invokeAsyncImpl)时，处理的结果依然会存入responsTable，等待客户端后续再来请求结果。但是他保存的依然是一个ResponseFuture，也就是在客户端请求结果时再去获取真正的结果<ul><li>另外，在RemotingServer启动时，会启动一个定时的线程任务，不断扫描responseTable，将其中过期的response清除掉</li></ul></li></ul><hr><h2 id="Producer发送消息过程">Producer发送消息过程</h2><ul><li>Producer有两种<ul><li>普通发送者：DefaultMQProducer。只负责发送消息，发送完消息，就可以停止了<ul><li>DefaultMQProducer只需要构建一个Netty客户端，往Broker发送消息就行了</li><li>异步回调只是在Producer接收到Broker的响应后自行调整流程，不需要提供Netty服务</li></ul></li><li>事务消息发送者： TransactionMQProducer。支持事务消息机制。需要在事务消息过程中提供事务状态确认的服务，这就要求事务消息发送者虽然是一个客户端，但是也要完成整个事务消息的确认机制后才能退出<ul><li>TransactionMQProducer由于需要在事务消息机制中给Broker提供状态确认服务，所以在发送消息的同时，还需要保持连接，提供服务</li><li>TransactionMQProducer的启动过程中，会往RemotingClient中注册相应的Processor，这样RemotingServer和RemotingClient之间就可以通过channel进行双向的服务请求了</li></ul></li></ul></li><li>整个Producer的流程，大致分两个步骤<ul><li>start方法，进行一大堆的准备工作</li><li>各种各样的send方法，进行消息发送</li></ul></li><li>重点关注以下几个问题<ul><li>Producer的核心启动流程以及两种消息发送者的区别<ul><li>所有Producer的启动过程，最终都会调用DefaultMQProducerImpl#start方法<ul><li>在start方法中的通过一个mQClientFactory对象，启动生产者的一大堆重要服务</li></ul></li><li>所有客户端的启动流程是固定的，不同客户端的区别只是在于他们在启动前注册的一些信息不同<ul><li>生产者注册到producerTable，消费者注册到consumerTable，管理控制端注册到adminExtTable</li></ul></li></ul></li><li>Producer如何管理Borker路由信息<ul><li>Producer需要拉取Broker列表，然后跟Broker建立连接等等很多核心的流程，其实都是在发送消息时建立的。因为在启动时，还不知道要拉取哪个Topic的Broker列表呢</li><li>对NameServer的地址管理，则是散布在启动和发送的多个过程当中，并且NameServer地址可以通过一个Http服务来获取</li><li>Send方法中，首先需要获得Topic的路由信息。这会从本地缓存中获取，如果本地缓存中没有，就从NameServer中去申请 DefaultMQProducerImpl#tryToFindTopicPublishInfo 方法</li><li><img src="/static/IT/RocketMQ/RocketMQ-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84-5.png" alt=""></li></ul></li><li>Producer的负载均衡<ul><li>Producer在获取路由信息后，会选出一个MessageQueue去发送消息</li><li>这个选MessageQueue的方法就是一个索引自增然后取模的方式</li><li>然后根据MessageQueue再找所在的Broker，往Broker发送请求</li></ul></li><li>在发送Netty请求时，如何制定Broker<ul><li>实际上是指定的MessageQueue，而不是Topic。Topic只是用来找MessageQueue</li></ul></li></ul></li></ul><hr><h2 id="Consumer拉取消息过程">Consumer拉取消息过程</h2><ul><li>消费者也是有两种，推模式消费者（用的最多）和拉模式消费者</li><li>消费者组之间有集群模式和广播模式两种消费模式</li><li>消费者端的负载均衡的原理。即消费者是如何绑定消费队列的，哪些消费策略到底是如何落地的</li><li>在推模式的消费者中，MessageListenerConcurrently 和MessageListenerOrderly这两种消息监听器的处理逻辑到底有什么不同，为什么后者能保持消息顺序</li><li>DefaultMQPushConsumer.start作为入口。最终消费者的启动过程，跟生产者一样，也交由了mQClientFactory<ul><li>pullMessageService主要处理拉取消息服务；rebalanceService主要处理客户端的负载均衡</li></ul></li><li>客户端负载均衡策略<ul><li>在消费者示例的start方法中，启动RebalanceService，这个是客户端进行负载均衡策略的启动服务。他只负责根据负载均衡策略获取当前客户端分配到的MessageQueue</li><li>五种负载策略，可以由Consumer的allocateMessageQueueStrategy属性来选择<ul><li>这个属性可以在DefaultMQPushConsumer的构造方法当中指定。默认是AllocateMessageQueueAveragely策略</li><li>最常用的是AllocateMessageQueueAveragely平均分配和AllocateMessageQueueAveragelyByCircle平均轮询分配<ul><li>平均分配是把MessageQueue按组内的消费者个数平均分配</li><li>平均轮询分配就是把MessageQueue按组内的消费者一个一个轮询分配</li></ul></li></ul></li></ul></li><li>并发消费与顺序消费的过程<ul><li>消费的过程依然是在DefaultMQPushConsumerImpl的 consumeMessageService中</li><li>他有两个子类ConsumeMessageConcurrentlyService和ConsumeMessageOrderlyService</li><li>最主要的差别是ConsumeMessageOrderlyService会在消费前把队列锁起来，优先保证拉取同一个队列里的消息</li><li>消费过程的入口在DefaultMQPushConsumerImpl的pullMessage中定义的PullCallback中</li></ul></li><li>RocketMQ消息消费方式分别为集群模式、广播模式</li><li>消息队列负载由RebalanceService线程默认每隔20s进行一次消息队列负载<ul><li>根据当前消费者组内消费者个数与主题队列数量按照某一种负载算法进行队列分配</li><li>分配原则为同一个消费者可以分配多个消息消费队列</li><li>同一个消息消费队列同一个时间只会分配给一个消费者</li></ul></li><li>消息拉取由PullMessageService线程根据RebalanceService线程创建的拉取任务进行拉取<ul><li>默认每次拉取一批消息(可以由业务指定，默认是1)，提交给消费者消费线程后继续下一次消息拉取</li><li>如果消息消费过慢产生消息堆积会触发消息消费拉取流控</li></ul></li><li>并发消息消费指消费线程池中的线程可以并发对同一个消息队列的消息进行消费<ul><li>消费成功后，取出消息队列中最小的消息偏移量作为消息消费进度偏移量存储在于消息消费进度存储文件中</li><li>集群模式消息消费进度存储在Broker（消息服务器）</li><li>广播模式消息消费进度存储在消费者端</li></ul></li><li>RocketMQ不支持任意精度的定时调度消息，只支持自定义的消息延迟级别<ul><li>可通过在broker配置文件中设置messageDelayLevel</li></ul></li><li>顺序消息一般使用集群模式，是指对消息消费者内的线程池中的线程对消息消费队列只能串行消费<ul><li>与并发消息消费最本质的区别是消息消费时必须成功锁定消息消费队列</li><li>在Broker端会存储消息消费队列的锁占用情况</li></ul></li><li>拉模式核心服务类： PullMessageService<ul><li>PullRequest里有messageQueue和processQueue，其中messageQueue负责拉取消息，拉取到后，将消息存入processQueue，进行处理。 存入后就可以清空messageQueue，继续拉取了</li><li><img src="/static/IT/RocketMQ/RocketMQ-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84-6.png" alt=""></li></ul></li></ul><hr><h2 id="文件存储">文件存储</h2><ul><li>Broker接收到消息后是如何把消息进行存储的</li><li>最终存储的文件<ul><li>commitLog：消息存储目录</li><li>config：运行期间一些配置信息</li><li>consumerqueue：消息消费队列存储目录</li><li>index：消息索引文件存储目录</li><li>abort：如果存在改文件寿命Broker非正常关闭</li><li>checkpoint：文件检查点，存储CommitLog文件最后一次刷盘时间戳、 consumerquueue最后一次刷盘时间，index索引文件最后一次刷盘时间戳</li></ul></li><li>messageStore就是负责消息存储的核心组件</li><li>消息存储的入口在：DefaultMessageStore.putMessage</li><li>commitLog写入<ul><li>CommitLog的doAppend方法就是Broker写入消息的实际入口</li><li>这个方法最终会把消息追加到MappedFile映射的一块内存里，并没有直接写入磁盘</li><li>写入消息的过程是串行的，一次只会允许一个线程写入</li></ul></li><li>分发ConsumeQueue和IndexFile<ul><li>当CommitLog写入一条消息后，在DefaultMessageStore的start方法中，会启动一个后台线程reputMessageService每隔1毫秒就会去拉取CommitLog中最新更新的一批消息，然后分别转发到ComsumeQueue和IndexFile里去</li><li>如果服务异常宕机，会造成CommitLog和ConsumeQueue、IndexFile文件不一致，有消息写入CommitLog后，没有分发到索引文件，这样消息就丢失了</li><li>DefaultMappedStore的load方法提供了恢复索引文件的方法，入口在load方法</li></ul></li><li>文件同步刷盘与异步刷盘：CommitLog.submitFlushRequest<ul><li>同步刷盘也是使用异步机制实现的<ul><li>刷盘是一个很重的操作，所以，RocketMQ即便是同步刷盘，也要对刷盘次数精打细算<ul><li>单条消息，那么直接将commitlog刷盘即可</li><li>批量消息，RockeMQ会先收集这一批次消息的刷盘请求，再进行一次统一的刷盘操作</li><li>一批消息有可能会跨两个commitlog文件，所以在刷盘时，要严格计算commitlog文件的刷盘次数</li></ul></li></ul></li><li>异步刷盘<ul><li>通过RocketMQ自己实现的一个CountDownLatch2提供了线程阻塞，使用CAS来驱动CountDownLatch2的countDown操作</li><li>每来一个消息就启动一次CAS，成功后，调用一次countDown</li><li>这个CountDownLatch2在CountDownLatch的基础上，增加实现了reset功能，实现了对象的重用</li></ul></li><li>TransientStorePoolEnable。如果开启了堆外内存，会在启动时申请一个跟CommitLog文件大小一致的堆外内存，这部分内存就可以确保不会被交换到虚拟内存中</li></ul></li><li>CommigLog主从复制：CommitLog.submitReplicaRequest<ul><li>RocketMQ整体是基于Netty实现的网络请求，而在主从复制这一块，却放弃了Netty框架，转而使用更轻量级的Java的NIO来构建</li><li>在主要的HAService中，会在启动过程中启动三个守护进程<ul><li>其中与Master相关的是acceptSocketService和groupTransferService<ul><li>acceptSocketService主要负责维护Master与Slave之间的TCP连接</li><li>groupTransferService主要与主从同步复制有关</li></ul></li><li>而slave相关的则是haClient</li></ul></li></ul></li><li>过期文件删除：DefaultMessageStore.addScheduleTask -&gt; DefaultMessageStore.this.cleanFilesPeriodically<ul><li>Broker会启动后台线程，每60秒，检查CommitLog、ConsumeQueue文件<ul><li>然后对超过72小时的数据进行删除</li><li>默认情况下， RocketMQ只会保存3天内的数据</li><li>可以通过fileReservedTime来配置</li></ul></li><li>他删除时，并不会检查消息是否被消费了</li></ul></li><li>整个文件存储的核心入口在DefaultMessageStore的start方法中</li></ul><p><img src="/static/IT/RocketMQ/RocketMQ-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84-7.png" alt=""></p><hr><h2 id="延迟消息">延迟消息</h2><ul><li>延迟消息的核心使用方法就是在Message中设定一个MessageDelayLevel参数，对应18个延迟级别</li><li>然后Broker中会创建一个默认的Schedule_Topic主题，这个主题下有18个队列，对应18个延迟级别</li><li>消息发过来之后，会先把消息存入Schedule_Topic主题中对应的队列<ul><li>等延迟时间到了，再转发到目标队列，推送给消费者进行消费</li></ul></li><li>延迟消息的处理入口在scheduleMessageService， 他会在broker启动时也一起加载</li><li>消息写入：CommitLog.putMessage<ul><li>在CommitLog写入消息时，会判断消息的延迟级别，然后修改Message的Topic和Queue，达到转储Message的目的</li></ul></li><li>消息转储到目标Topic：scheduleMessageService<ul><li>这个服务只在master节点上启动，而在slave节点上会主动关闭这个服务</li><li>由于RocketMQ的主从节点支持切换，所以就需要考虑这个服务的幂等性<ul><li>在节点切换为slave时就要关闭服务，切换为master时就要启动服务</li><li>即便节点多次切换为master，服务也只启动一次：通过一个CAS操作来保证服务的启动状态</li><li>这个CAS操作还保证了在后面，同一时间只有一个DeliverDelayedMessageTimerTask执行</li></ul></li><li>ScheduleMessageService会每隔1秒钟执行一个executeOnTimeup任务，将消息从延迟队列中写入正常Topic中：ScheduleMessageService -&gt; DeliverDelayedMessageTimerTask.executeOnTimeup</li></ul></li></ul><p><img src="/static/IT/RocketMQ/RocketMQ-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84-8.png" alt=""></p><hr><h2 id="长轮询机制">长轮询机制</h2><ul><li>RocketMQ对消息消费者提供了Push推模式和Pull拉模式两种消费模式<ul><li>但是这两种消费模式的本质其实都是Pull拉模式</li><li>Push模式可以认为是一种定时的Pull机制</li></ul></li><li>长轮询机制简单来说，就是当Broker接收到Consumer的Pull请求时，判断如果没有对应的消息，不用直接给Consumer响应 (给响应也是个空的，没意义)，而是就将这个Pull请求给缓存起来<ul><li>当Producer发送消息过来时，增加一个步骤去检查是否有对应的已缓存的Pull请求，如果有，就及时将请求从缓存中拉取出来，并将消息通知给Consumer</li></ul></li></ul><p><img src="/static/IT/RocketMQ/RocketMQ-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84-9.png" alt=""></p><hr><p><img src="/static/IT/RocketMQ/RocketMQ-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84-10.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;NameServer的结构&lt;/li&gt;
&lt;li&gt;Broker的结构&lt;/li&gt;
&lt;li&gt;Netty服务注册框架&lt;/li&gt;
&lt;li&gt;RocketMQ的同步结果推送与异步结果推送&lt;/li&gt;
&lt;li&gt;Broker心跳注册过程&lt;/li&gt;
&lt;li&gt;Producer发送消息过程</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="RocketMQ" scheme="https://jxch.github.io/tags/RocketMQ/"/>
    
  </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PA &amp; CODING</title>
  
  <subtitle>求仁得仁</subtitle>
  <link href="https://jxch.github.io/atom.xml" rel="self"/>
  
  <link href="https://jxch.github.io/"/>
  <updated>2023-07-10T06:53:28.204Z</updated>
  <id>https://jxch.github.io/</id>
  
  <author>
    <name>钱不寒</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>11.数据高可用</title>
    <link href="https://jxch.github.io/2023/07/10/architect/dian-shang-xi-tong/11-shu-ju-gao-ke-yong/"/>
    <id>https://jxch.github.io/2023/07/10/architect/dian-shang-xi-tong/11-shu-ju-gao-ke-yong/</id>
    <published>2023-07-10T05:59:06.000Z</published>
    <updated>2023-07-10T06:53:28.204Z</updated>
    
    <content type="html"><![CDATA[<p>缓存不命中：把<strong>全量数据</strong>都放在Redis 集群中，处理读请求的时候，只需要读取Redis，而不用访问数据库。很多大型互联网公司都在使用这种方法<br>更新缓存：</p><ul><li>启动一个更新缓存的服务接收数据变更的消息队列中的消息，然后注意解决消息的<strong>可靠性</strong>问题即可，这种方式实现起来很简单，也没有什么侵入性。</li><li>使用 Binlog 实时更新Redis 缓存（Canal）<ul><li>由于在整个缓存更新链路上，减少了一个收发消息队列的环节，从MySQL 更新到Redis 更新的时延变得更短，出现故障的可能性也更低，这也是为什么很多大型互联网企业更青睐于采用这种方案的原因。</li></ul></li></ul><h1>Canal 详解</h1><p><font color="green">它通过模拟MySOL主从复制的交互协议，<strong>把自己伪装成一个MySOL的从节点</strong>，向 MySOL 主节点发送dump 请求。</font>MySOL 收到请求后，就会向 Canal 开始推送Binlog，Canal 解析Binlog 字节流之后，将其转换为便于读取的结构化数据，供下游程序订阅使用。<br>Canal有个服务端，在模拟MySOL从节点获得数据库服务器的数据后，我们可以使用一个包含Canal Client的服务程序获得Canal服务端解析出的数据，也可以通过配置让Canal服务端直接将数据发送给MQ，当然我们的Canal Client 程序经过数据处理后也可以发送给MQ。不管是经过Canal Client程序还是直接发给MQ，接下来还可由第三方的服务或者存储系统进行后续处理。</p><h2 id="配置">配置</h2><ol><li><strong>先开启 Binlog 写入功能，配置binlog-format为ROW模式</strong></li><li><strong>给Canal 设置一个用来复制数据的MySQL账号</strong></li><li><font color="green"><strong>修改canal.properties 文件，比较关键的是canal.destinations</strong></font></li><li><font color="green"><strong>修改instance.properties</strong></font><ol><li><strong>MySQL 主服务的连接配置</strong></li><li><strong>要对哪些相关的业务表进行监视</strong></li></ol></li></ol><h2 id="跨系统实时数据同步">跨系统实时数据同步</h2><p>如果是按照关键字搜索，放在ES 中会比放在MySQL 中更合适。所以在大规模系统中，对于海量数据的处理原则都是根据业务对数据查询的需求反过来确定选择什么数据库、如何组织数据结构、如何分片数据等之类的问题，这样才能获得最优的查询性能。在大型互联网企业中、其核心业务数据，以不同的数据结构和存储方式，保存几十甚至上百份，都是非常正常的。</p><p>当然为了能够支撑下游的众多数据库，从 Canal 出来的Binlog 数据肯定不能直接写入下游的众多数据库中。原因也很明显：一是写不过来；二是下游的每个数据库，在写入之前可能还要处理一些数据转换和过滤的工作。所以一般我们会增加一个<font color="green"><strong>消息队列来解耦上下游</strong></font>。</p><h1>更换数据库</h1><p>既不能长时间停止服务，也不能丢失数据</p><h2 id="不停机更换数据库">不停机更换数据库</h2><p>我们在设计迁移方案的时候，<font color="green"><strong>一定要保证每一步都是可逆的</strong></font>。也就是必须保证，每执行完一个步骤,一旦出现任何问题，都能快速回滚到上一个步骤。</p><ol><li>首先要做的一点是，<font color="green"><strong>把旧库的数据全部复制到新库中</strong></font>。因为旧库还在服务线上业务，所以<strong>不断会有数据写入旧库</strong>，我们不仅要向新库复制数据，<font color="green"><strong>还要保证新旧两个库的数据是实时同步的</strong></font>。所以，需要用一个同步程序来实现新旧两个数据库的实时同步。可以使用Binlog 实现两个异构数据库之间数据的实时同步。<font color="green"><strong>这一步不需要回滚，因为这里只增加了一个新库和一个同步程序</strong></font>，对系统的旧库和程序没有任何改变。<font color="green"><strong>即使新上线的同步程序影响到了旧库，停掉同步程序也就可以了。</strong></font></li><li>改造DAO层<ol><li><font color="green"><strong>支持双写新旧两个库</strong></font>，并且预留<strong>热切换开关</strong>，能通过开关控制三种写状态:只写旧库、只写新库和同步双写</li><li><font color="green"><strong>支持读取新旧两个库</strong></font>，同样预留<strong>热切换开关</strong>，控制读取旧库还是新库</li></ol></li><li><font color="green">然后上线新版服务，这个时候服务仍然是<strong>只读写旧库</strong>，不读写新库。<strong>让新版服务稳定运行至少一到两周的时间</strong></font>，其间我们不仅要验证新版服务的稳定性，还要<font color="green"><strong>验证新旧两个库中的数据是否保持一致</strong></font>。这个过程中，如果新版服务<font color="green"><strong>出现任何问题，都要立即下线新版服务，回滚到旧版本的服务</strong></font>。<ul><li><font color="green"><strong>稳定一段时间之后，就可以开启订单服务的双写开关了</strong>。<strong>开启双写开关的同时，需要停掉同步程序</strong></font>。这里有一个需要特别注意的问题是，这里双写的业务逻辑，一定是<font color="green"><strong>先写旧库，再写新库，并且以旧库的结果为准</strong></font>。</li><li><font color="green"><strong>如果旧库写成功，新库写失败，则返回成功</strong>，但这个时候要记录日志，后续我们会根据这个日志来验证新库是否还有问题。<strong>如果旧库写失败，则直接返回失败</strong>，同时也不再写新库了。<strong>不能让新库影响到现有业务的可用性和数据准确性</strong>。如果<strong>出现任何问题都要关闭双写</strong>,回滚到只读写旧库的状态。</font></li><li>切换到双写之后,新库与旧库的数据可能会出现不一致的问题。<ul><li>停止同步程序和开启双写，这两个过程很难做到无缝衔接</li><li>双写的第略也不能保证新旧库的强一致性。对于这个问题,我们需要上线一个<font color="green"><strong>比对和补偿的程序</strong></font>，用于比对旧库最近的数据变更，然后检查新库中的数据是否一致，如果不一致，则需要进行补偿</li></ul></li><li>开启双写之后，还需要<strong>稳定运行至少几周的时间</strong>，并且在这期间我们需要不断地检查，以确保不能有旧库写成功、新库写失败的问题。如果在几周之后比对程序发现新旧两个库的数据没有不一致的情况，那就可以认为新旧两个库的数据一直都是保持同步的。</li></ul></li><li>接下来就可以用类似<font color="green"><strong>灰度发布的方式把读请求逐步切换到新库上</strong>。同样，运行期间如果<strong>出现任何问题，都要再切回到旧库</strong></font>。</li><li><font color="green">将<strong>全部读请求都切换到新库上之后，其实读写请求已经全部切换到新库上了</strong></font>，虽然实际的切换已经完成,但后续还有需要收尾的步骤。<ul><li><font color="green">**再稳定一段时间之后，就可以停掉比对程序，把订单服务的写状态改为只写新库。至此，旧库就可以下线了。**注意，在整个迁移过程中,<strong>只有这个步骤是不可逆的</strong>。</font>由于这一步的主要操作就是摘掉已经不再使用的旧库，因此对于正在使用的新库并不会有什么影响，实际出问题的可能性已经非常小了。</li><li>双写切换为新库单写这一步不可逆的主要原因是，一旦切换为新库单写，旧库的数据与新库的就不一致了，这种情况是无法再切换回旧库的。所以问题的关键是，切换为新库单写后，需要保证旧库的数据能与新库保持同步。<font color="green">这时双写需要增加一种过渡状态：**从双写以旧库为准过渡到双写以新库为准。然后把比对和补偿程序反过来，用新库的数据补偿旧库的数据。**这样就可以做到一旦出现问题，就直接切回到旧库上。</font>但是这样做一般成本比较高。</li></ul></li><li>至此们完成了在线更换数据库的全部流程。<font color="green">双写版本的服务也完成了它的历史使命，<strong>可以在下一次升级订单服务版本的时候下线双写功能</strong>。</font></li></ol><p><font color="green">数据表的变更，如果只是新增表，这个很简单，一般直接回退到旧版本程序即可；但如果牵涉到<strong>表字段的变化</strong>就麻烦些，但是也可以采用类似的思路，<strong>双写新旧表并设计热切换开关</strong>。</font></p><h2 id="实现比对和补偿程序">实现比对和补偿程序</h2><p>这个比对和补偿程序的实现难点在于，我们要比对的是两个随时都在变化的数据厍中的数据。</p><p>像订单这类时效性比较强的数据，是比较容易进行比对和补偿的。因为订单一旦完成之后，几乎就不会再改变了，比对和补偿程序就可以根据订单完成时间，每次只比对这个时间窗口内完成的订单。补偿的逻辑也很简单，<font color="green"><strong>发现不一致的情况后，直接用旧库的订单数据覆盖新库的订单数据就可以了</strong></font>。<br>这样，切换双写期间，对于少量不一致的订单数据，等到订单完成之后，补偿程序会将其修正。后续在双写的时候只要新库不是频繁写入失败，就可以保证两个库的数据完全一致。<br>比较麻烦的是更一般的情况，比如像商品信息之类的数据，随时都有可能会发生变化。如果数据上带有更新时间,那么比对程序就可以利用这个更新时间，<font color="green"><strong>每次从旧库中读取一个更新时间窗口内的数据，到新库中查找具有相同主键的数据进行比对</strong></font>，如果发现数据不一致，则还要比对一下更新时间。</p><ul><li><font color="green"><strong>如果新库数据的更新时间晚于旧库数据，那么很可能是比对期间数据发生了变化，这种情况暂时不要补偿，放到下个时间窗口继续进行比对即可。</strong></font></li><li><font color="green"><strong>时间窗口的结束时间不要选取当前时间，而是要比当前时间早一点，比如1分钟之前，这样就可以避免比对正在写入的数据了。</strong></font></li></ul><p><strong>如果数据没带时间戳信息，那就只能从旧库中读取Binlog</strong>，获取数据变化信息后到新库中查找对应的数据进行比对和补偿。</p><h1>安全地实现数据备份和恢复</h1><p>一般来说，由存储系统导致的比较严重的损失主要有两种情况。</p><ol><li><strong>数据丢失造成的直接财产损失</strong>。比如订单数据丢失造成了大量的坏账。为了避免这种损失，系统需要保证数据的<strong>高可靠性</strong>。</li><li><strong>存储系统的损坏</strong>，造成整个业务系统停止服务而带来的损失。比如，电商系统停服期间造成的收入损失。为了避免这种损失，系统需要保证存储服务的<strong>高可用性</strong>。</li></ol><p>保证数据安全，最简单且有效的方法就是<strong>定期备份数据</strong>，这样无论因为出现何种问题而导致的数据损失，都可以通过备份来恢复数据。</p><p>最简单的备份方式就是<strong>全量备份</strong>。备份的时候把所有的数据复制一份，存放到文件中，恢复的时候再把文件中的数据复制回去，这样就可以保证恢复之后,数据库中的数据与备份时的数据是完全一样的。在 MySQL 中，我们可以使用mysqldump 命令执行全量备份。</p><p><strong>不过全量备份的代价非常高</strong>：首先备份文件包含了数据库中的所有数据，占用的磁盘空间非常大；其次,每次备份操作都要拷贝大量的数据，备份过程中会占用数据库服务器大量的CPU和磁盘IO 资源、同时为了保证数据一致性，备份过程中很有可能会锁表。这此都会导致在备份期间数据库本身的性能严重下降。所以我们不能频繁地对数据库执行全量备份操作。</p><p>一般来说，在生产系统中每天执行一次全量备份就已经是非常频繁的了。这就意味着，如果数据库中的数据丢失了就只能恢复到最近一次全量备份的那个时间点，这个时间点之后的数据是无法找回的。也就是说，因为全量备份的代价比较高不能频繁地执行备份操作,所以<strong>全量备份不能做到完全无损的恢复</strong>。</p><p>相比于全量备份，<strong>增量备份每次只用备份相对于上一次备份发生了变化的那部分数据，所以增量备份的速度更快。</strong></p><p>MySQL 自带的 Binlog 就是一种实时的增量备份工具。Binlog 所记录的就是MySQL 数据变更的操作日志。开启 Binlog 之后，MySQL 中数据的每次更新操作，都会记录到Binlog 中。Binlog 是可以回放的，回放Binlog，就相当于是把之前对数据库中所有数据的更新操作，都按顺序重新执行一遍，回放完成之后数据自然就恢复了。这就是Binlog 增量备份的基本原理。很多数据库都有类似于MySQL Binlog 的日志工具，原理也与Binlog 相同，备份和恢复的方法也与之类似。<font color="green"><strong>通过定期的全量备份配合 Binlog，我们可以把数据恢复到任意一个时间点</strong></font></p><p>在执行备份和恢复的时候，需要特别注意如下两个要点：</p><ol><li>“<font color="green"><strong>不要把所有的鸡蛋放在同一个篮子中</strong></font>”，无论是全量备份还是Binlog，都不要与数据库存放在同一个服务器上。最好能存放到不同的机房，甚至不同城市离得越远越好。这样即使出现机房着火、光缆被挖断甚至地震也不怕数据丢失。</li><li>在回放 Binlog 的时候，<font color="green"><strong>指定的起始时间可以比全量备份的时间稍微提前一点儿</strong></font>，这样可以确保全量备份之后的所有操作都在恢复的Binlog 范围内，从而保证数据恢复的完整性。<ul><li><font color="green"><strong>为了确保回放的幂等性，需要将Binlog的格式设置为ROW格式</strong></font></li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;缓存不命中：把&lt;strong&gt;全量数据&lt;/strong&gt;都放在Redis 集群中，处理读请求的时候，只需要读取Redis，而不用访问数据库。很多大型互联网公司都在使用这种方法&lt;br&gt;
更新缓存：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;启动一个更新缓存的服务接收数据变更的消息队列中的消息</summary>
      
    
    
    
    <category term="电商系统" scheme="https://jxch.github.io/categories/%E7%94%B5%E5%95%86%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="缓存" scheme="https://jxch.github.io/tags/%E7%BC%93%E5%AD%98/"/>
    
  </entry>
  
  <entry>
    <title>10.高并发缓存</title>
    <link href="https://jxch.github.io/2023/07/10/architect/dian-shang-xi-tong/10-gao-bing-fa-huan-cun/"/>
    <id>https://jxch.github.io/2023/07/10/architect/dian-shang-xi-tong/10-gao-bing-fa-huan-cun/</id>
    <published>2023-07-10T00:35:31.000Z</published>
    <updated>2023-07-10T04:43:38.230Z</updated>
    
    <content type="html"><![CDATA[<p>缓存大体可以分为三类: 客户端缓存；服务端缓存；网络中的缓存。<br>根据规模和部署方式缓存也可以分为:单体缓存；缓存集群；分布式缓存。</p><p><strong>缓存一定是离用户越近越好</strong></p><p>缓存的分类：</p><ul><li>客户端缓存</li><li>页面缓存</li><li>浏览器缓存</li><li>APP缓存</li><li>网络缓存</li><li>Web代理缓存（Nginx）</li><li>边缘缓存（CDN）</li><li>服务端缓存</li><li>数据库缓存（innodb_buffer_pool_size）</li><li>应用级缓存（Caffeine，Ehcache）<ul><li>缓存预热</li></ul></li><li>平台级缓存（Redis）</li></ul><h1>缓存的数据一致性</h1><p>缓存更新方案：</p><ol><li>先更新缓存，再更新数据库（一般不考虑）<ul><li>更新缓存成功，更新数据库出现异常了，导致缓存数据与数据库数据完全不一致，而且很难察觉，因为缓存中的数据一直都存在。</li></ul></li><li>先更新数据库，再更新缓存（一般不考虑）<ul><li>原因跟第一个一样，数据库更新成功了，缓存更新失败，同样会出现数据不一致问题。</li></ul></li><li>先删除缓存，后更新数据库（不建议）<ul><li>A进程删除缓存后，还没来得及更新数据库，就被B进程查询数据库（缓存为空）后再次更新了缓存（相当于没删）<ul><li>解决方案：延时双删（删缓存 - 写数据库 - 休眠1s再删缓存）</li></ul></li><li>A进程删除缓存、写数据库后，主从同步时，被B从从库查数据（缓存为空，从库复制还未完成），再次更新了缓存（相当于没删）<ul><li>解决方案：1. 延时双删；2. 更新redis的查询操作，指向主库查询</li></ul></li><li>异步双删，增加吞吐量</li><li>不推荐的原因：1. 缓存穿透到DB；2. 休眠时间不好设置</li></ul></li><li><font color="green"><strong>先更新数据库，后删除缓存</strong></font><ul><li>依然有并发问题：进程A查询数据库（缓存为空）后写入缓存前，被进程B更新了数据库，导致缓存与DB不一致<ul><li>概率极低，因为需要让进程B写数据库的操作耗时比进程读数据库还短，而读库操作一般比写库要快</li><li>解决方案：1. 设置缓存失效时间；2. 异步延时删除；3. <font color="green"><strong>使用Canal监听binlog日志，更新缓存</strong></font></li></ul></li></ul></li></ol><p>删除缓存失败的解决方案：</p><ol><li>使用MQ再次删除</li><li>使用canal采集binlog日志，发送到MQ中，然后通过ACK机制确认处理删除缓存</li></ol><h2 id="缓存更新的设计模式">缓存更新的设计模式</h2><ul><li>Cache Aside （最常用，因为强一致的实现性能往往较差）：缓存失效时由调用方加载（上面的第四种方案）</li><li>Read/Write Through：把更新数据库操作由缓存自己代理了</li><li>Read Through：缓存失效时由用缓存服务自己来加载</li><li>Write Through：当有数据更新的时候，如果没有命中缓存，直接更新数据库，然后返回。如果命中了缓存，则更新缓存，然后再由Cache自己更新数据库。</li><li>Write Behind Caching：，在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库<ul><li>数据不是强一致性的，而且可能会丢失</li><li>实现复杂</li></ul></li></ul><h2 id="缓存数据一致性">缓存数据一致性</h2><ol><li>本地缓存<ul><li>设置失效时间（毛刺现象）</li><li><font color="green"><strong>设置双缓存</strong></font><ul><li>只有两份本地缓存都没有，才会到远程获得。</li><li>正式缓存是最后一次写入后经过固定时间过期</li><li>备份缓存是设置最后一次访问后经过固定时间过期</li><li>异步刷新：正式缓存只有无效才会被重新写入，备份缓存无论是否无效都会重新写入</li></ul></li><li>强制本地缓存失效和手动刷新本地缓存（CacheManagerController）</li></ul></li><li>Redis使用canal更新缓存</li></ol><h1>Redis集群</h1><h2 id="Redis-Cluster">Redis Cluster</h2><p>每个集群的槽数都是固定的16 384(即16×1024)个，客户端可以连接集群的任意一个节点来访问集群的数据，如果数据不在当前这个节点上，那就向客户端返回一个重定向的命令。支持主从复制（内部通过选举来确定有主节点的存活），支持读写分离（需要客户端的支持）。</p><h2 id="大厂不用Redis-Cluster构建集群">大厂不用Redis Cluster构建集群</h2><p>Redis Cluster 采用了一种去中心化的流言（Gossip）协议来传播集群配置的变化。传播速度比较慢，而且是集群规模越大，传播的速度就越慢。</p><p>比如说10个节点的状态信息约1kb。同时redis集群内节点，每秒都在发ping消息。在这种情况下，一个总节点数为200的Redis集群，默认情况下，这时ping/pong消息占用带宽达到25M，这还只是槽的范围是0 ～16383 的情况。其次redis 的集群主节点越多，心跳包的消息体内携带的数据越多。如果节点过1000 个，也会导致<strong>网络拥堵</strong>。因此redis 作者，不建议redis cluster 节点数量超过<strong>1000个</strong>。</p><h3 id="构建超大规模集群">构建超大规模集群</h3><h4 id="代理">代理</h4><p>比较常用的方法是采用一种基于代理的方式，即在客户端和Redis 节点之间</p><ol><li>负责在客户端和Redis 节点之间转发请求和响应。客户端只与代理服务打交道，代理收到客户端的请求之后，会转发到对应的Redis 节点上，节点返回的响应再经由代理转发返回给客户端。</li><li>负责监控集群中所有Redis 节点的状态，如果发现存在问题节点，就及时进行主从切换。</li><li>维护集群的元数据，这个元数据主要是集群所有节点的主从信息,以及槽和节点的关系映射表。像开源的Redis 集群方案twemproxy 和 Codis，采用的都是代理服务这种架构。</li></ol><p>优点：对客户端透明,从客户端的视角来看,整个集群就像是一个超大容量的单节点Redis 一样。除此之外,由于分片算法是受代理服务控制的，因此扩容比较方便，新节点加入集群后，直接修改代理服务中的元数据就可以完成扩容。</p><p>缺点：由于增加了一层代理转发，因此每次数据访问的链路变得更长了，这必然会导致一定的性能损失。而且代理服务本身也是集群的单点。当然，我们可以把代理服务也做成一个集群来解决单点问题，那样集群就更复杂了。</p><h4 id="客户端">客户端</h4><p><font color="green"><strong>把代理服务的寻址功能前移到客户端中。</strong></font>客户端在发起请求之前,首先会查询元数据，客户端可以自行缓存元数据，这样访问性能基本上就与单机版的Redis 一样了。如果某个分片的主节点宕机了，就会选举新的主节点，并更新元数据中的信息。对集样的扩容操作也比较简单，除了必须完成数据的迁移工作之外，再更新一下元数据就可以了。</p><p>当然元数据服务仍然是一个单点，但是它的数据量不大，访问量也不大，相对来说比较容易实现。利用已有的ZooKeeper、etcd 甚至MySQL 都可以被用来实现上述元数据服务。定制客户端的Redis 集群方案应该是最适合超大规模Redis 集群的方案，在性能、弹性、高可用等几个方面的表现都非常好，缺点是整个架构比较复杂，客户端不能通用，需要开发定制化的Redis 客户端，所以往往只有规模足够大的企业才能负担得起高昂的定制开发成本。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;缓存大体可以分为三类: 客户端缓存；服务端缓存；网络中的缓存。&lt;br&gt;
根据规模和部署方式缓存也可以分为:单体缓存；缓存集群；分布式缓存。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;缓存一定是离用户越近越好&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;缓存的分类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;客户端</summary>
      
    
    
    
    <category term="电商系统" scheme="https://jxch.github.io/categories/%E7%94%B5%E5%95%86%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="缓存" scheme="https://jxch.github.io/tags/%E7%BC%93%E5%AD%98/"/>
    
  </entry>
  
  <entry>
    <title>09.分布式事务</title>
    <link href="https://jxch.github.io/2023/07/10/architect/dian-shang-xi-tong/09-fen-bu-shi-shi-wu/"/>
    <id>https://jxch.github.io/2023/07/10/architect/dian-shang-xi-tong/09-fen-bu-shi-shi-wu/</id>
    <published>2023-07-09T23:15:43.000Z</published>
    <updated>2023-07-10T00:24:47.402Z</updated>
    
    <content type="html"><![CDATA[<h1>Seata架构</h1><ul><li>TC&nbsp;(Transaction&nbsp;Coordinator)&nbsp;-&nbsp;事务协调者：维护全局和分支事务的状态，驱动全局事务提交或回滚。</li><li>TM&nbsp;(Transaction&nbsp;Manager)&nbsp;-&nbsp;事务管理器：定义全局事务的范围：开始全局事务、提交或回滚全局事务。</li><li>RM&nbsp;(Resource&nbsp;Manager)&nbsp;-&nbsp;资源管理器：管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。</li></ul><p>其中，TC&nbsp;为单独部署的&nbsp;Server&nbsp;服务端，TM&nbsp;和&nbsp;RM&nbsp;为嵌入到应用中的&nbsp;Client&nbsp;客户端。</p><p>一个分布式事务的生命周期：<br>1.&nbsp;TM&nbsp;请求&nbsp;TC&nbsp;开启一个全局事务。TC&nbsp;会生成一个&nbsp;XID&nbsp;作为该全局事务的编号。XID会在微服务的调用链路中传播，保证将多个微服务的子事务关联在一起。<br>2.&nbsp;RM&nbsp;请求&nbsp;TC&nbsp;将本地事务注册为全局事务的分支事务，通过全局事务的&nbsp;XID&nbsp;进行关联。<br>3.&nbsp;TM&nbsp;请求&nbsp;TC&nbsp;告诉&nbsp;XID&nbsp;对应的全局事务是进行提交还是回滚。<br>4.&nbsp;TC&nbsp;驱动&nbsp;RM&nbsp;们将&nbsp;XID&nbsp;对应的自己的本地事务进行提交还是回滚。</p><h2 id="分库分表全局事务（ShardingSphere）">分库分表全局事务（ShardingSphere）</h2><p>不能简单的在全局事务发起方使用 <code>@GlobalTransactional</code>，因为seata找不到逻辑表对应的物理表，需要融入&nbsp;Apache&nbsp;ShardingSphere&nbsp;分布式事务</p><p>整合&nbsp;Seata&nbsp;AT&nbsp;事务时，需要将&nbsp;TM，RM&nbsp;和&nbsp;TC&nbsp;的模型融入&nbsp;Apache&nbsp;ShardingSphere&nbsp;的分布式事务生态中。&nbsp;在数据库资源上，Seata&nbsp;通过对接&nbsp;DataSource&nbsp;接口，让&nbsp;JDBC&nbsp;操作可以同&nbsp;TC&nbsp;进行远程通信。&nbsp;同样，Apache&nbsp;ShardingSphere&nbsp;也是面向&nbsp;DataSource&nbsp;接口，对用户配置的数据源进行聚合。&nbsp;因此，将&nbsp;DataSource&nbsp;封装为&nbsp;基于Seata&nbsp;的&nbsp;DataSource&nbsp;后，就可以将&nbsp;Seata&nbsp;AT&nbsp;事务融入到&nbsp;Apache&nbsp;ShardingSphere的分片生态中。</p><ol><li>引入依赖</li><li>配置seata.conf <pre class="line-numbers language-config" data-language="config"><code class="language-config">client&nbsp;{    application.id&nbsp;=&nbsp;tulingmall‐order‐curr    transaction.service.group&nbsp;=&nbsp;tuling‐order‐group}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li>开启全局事务配置:<pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token comment">// GlobalTransactional和ShardingTransactionType不能同时出现</span> <span class="token annotation punctuation">@ShardingTransactionType</span><span class="token punctuation">(</span><span class="token class-name">TransactionType</span><span class="token punctuation">.</span><span class="token constant">BASE</span><span class="token punctuation">)</span> <span class="token annotation punctuation">@Transactional</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li><li>关闭Seata数据源自动代理<pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">seata</span><span class="token punctuation">:</span>    <span class="token key atrule">enable‐auto‐data‐source‐proxy</span><span class="token punctuation">:</span> false <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><h1>柔性事务：可靠消息最终一致性</h1><h2 id="本地消息表方案">本地消息表方案</h2><ol><li>定时任务扫描日志<ul><li>启动独立的线程，定时对消息日志表中的消息进行扫描并发送至消息中间件，在消息中间件反馈发送成功后删除该消息日志，否则等待定时任务下一周期重试。</li></ul></li><li>消费消息（幂等控制）<ul><li>可以使用MQ的ack（即消息确认）机制，消费者监听MQ，如果消费者接收到消息并且业务处理完成后向MQ发送ack（即消息确认），此时说明消费者正常消费消息完成，MQ将不再向消费者推送消息，否则消费者会不断重试向消费者来发送消息。</li><li>由于消息会重复投递，需要实现幂等性。</li></ul></li></ol><h2 id="Rocketmq事务消息">Rocketmq事务消息</h2><p>RocketMQ事务消息设计则主要是为了<strong>解决Producer端的消息发送与本地事务执行的原子性问题</strong>，RocketMQ的设计中broker与producer端的双向通信能力，使得broker天生可以作为一个事务协调者存在；而RocketMQ本身提供的存储机制为事务消息提供了持久化能力；RocketMQ的高可用机制以及可靠消息设计则为事务消息在系统发生异常时依然能够保证达成事务的最终一致性。在RocketMQ&nbsp;4.3后实现了完整的事务消息，实际上其实是对本地消息表的一个封装，将本地消息表移动到了MQ内部，解决Producer端的消息发送与本地事务执行的原子性问题。</p><ol><li>Producer发送事务消息<ul><li>Producer（MQ发送方）发送事务消息至MQ&nbsp;Server，MQ&nbsp;Server将消息状态标记为Prepared（预览状态），注意此时这条消息消费者（MQ订阅方）是无法消费到的。</li></ul></li><li>MQ&nbsp;Server回应消息发送成功<ul><li>MQ&nbsp;Server接收到Producer发送的消息则回应发送成功表示MQ已接收到消息。</li></ul></li><li>Producer执行本地事务</li><li>消息投递<ul><li>若Producer本地事务执行成功则自动向MQ&nbsp;Server发送commit消息</li><li>MQ&nbsp;Server接收到commit消息后将“该消息”状态标记为可消费，此时MQ订阅方即正常消费消息；</li><li>若Producer&nbsp;本地事务执行失败则自动向MQ&nbsp;Server发送rollback消息，MQ&nbsp;Server接收到rollback消息后将删除“该消息”。</li><li>MQ订阅方消费消息，消费成功则向MQ回应ack，否则将重复接收消息。这里ack默认自动回应，即程序执行正常则自动回应ack。</li></ul></li><li>事务回查<ul><li>如果执行Producer端本地事务过程中，执行端挂掉，或者超时，MQ&nbsp;Server将会不停的询问同组的其他Producer来获取事务执行状态，这个过程叫事务回查。MQ&nbsp;Server会根据事务回查结果来决定是否投递消息。</li></ul></li></ol><p>对用户则来说，用户需要分别实现本地事务执行以及本地事务回查方法（<code>RocketMQLocalTransactionListener</code>），因此只需关注本地事务的执行状态即可。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Seata架构&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;TC&amp;nbsp;(Transaction&amp;nbsp;Coordinator)&amp;nbsp;-&amp;nbsp;事务协调者：维护全局和分支事务的状态，驱动全局事务提交或回滚。&lt;/li&gt;
&lt;li&gt;TM&amp;nbsp;(Transaction&amp;n</summary>
      
    
    
    
    <category term="电商系统" scheme="https://jxch.github.io/categories/%E7%94%B5%E5%95%86%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="分布式事务" scheme="https://jxch.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>08.支付超时</title>
    <link href="https://jxch.github.io/2023/07/10/architect/dian-shang-xi-tong/08-zhi-fu-chao-shi/"/>
    <id>https://jxch.github.io/2023/07/10/architect/dian-shang-xi-tong/08-zhi-fu-chao-shi/</id>
    <published>2023-07-09T21:19:18.000Z</published>
    <updated>2023-07-09T22:14:41.423Z</updated>
    
    <content type="html"><![CDATA[<h1>延迟任务（不推荐）</h1><ol><li>下单时增加一个定时任务，在五分钟后对订单进行超时判断。</li><li>超时判断时，可以先去支付宝上查询订单支付状态。</li></ol><p>如果已支付，则判断订单是否正常结束，这是因为在用户完成扫码支付后，支付宝正常会往图灵电商发送支付成功的通知。但是这个通知是没有事务保证的，所以是非常有可能失败的，这时就需要在订单超时判断时对状态进行对齐。<br>如果未支付，则需要释放库存，取消本地订单，然后通知支付宝取消支付订单。</p><p><strong>五分钟时间太长了，而支付宝的通知又没有事务保证</strong>，通常企业中的做法并不会等到订单超时时才去查询订单状态，而是<strong>在后台会多次频繁查询支付宝支付状态</strong>，这样可以更及时的获得支付结果。例如五分钟超时时间，至少需要半分钟或者一分钟去查一次支付宝订单状态，如果支付成功了，就及时结束后续等待处理过程。如果没有完成支付，就再开启下一个定时任务，等待下次检查。</p><h1>RocketMQ事务消息（推荐）</h1><p>RocketMQ事务消息机制的核心是对消息状态进行不断的确认。循环确认的过程就正好可以用来改造，解决上面说的频繁任务调度的问题。这样就可以专注于开发业务逻辑，而不用关注频繁复杂的任务调度逻辑。</p><ol><li>支付宝预下单时发送事务消息：通知下游服务进行订单取消</li><li>发送消息后，会先执行本地事务。<ul><li>将订单ID放到Redis中，这样可以在后续进行支付状态检查时，快速找到对应的业务信息。</li><li>只要下单成功，就会返回<code>UNKOWN</code>状态，这样RocketMQ会在之后进行状态回查（检查支付宝支付状态）。</li></ul></li><li>然后在事务状态回查时<ul><li>会自行记录回查次数，超过最大次数就直接取消订单（<code>transactionCheckInterval</code>，默认回查15次，间隔60s，超过<code>ROLLBACK</code>）<ul><li>如果没有超过最大次数，就可以去支付宝中查询订单支付状态。<ul><li>如果已经支付完成，则返回<code>ROLLBACK</code>状态，消息取消，后续就不会再进行本地订单取消了。</li><li>如果未支付，则记录回查次数后，返回<code>UNKNOWN</code>状态，等待下次回查。</li></ul></li></ul></li></ul></li><li>如果订单已经超时（事务消息成功发送出去了）<ul><li>下游的消费者就会完成取消本地订单，释放库存等操作。</li></ul></li><li>如果本地订单已经取消，而支付宝支付状态已经成功，则退款</li></ol><h2 id="通过聚合支付进行分布式事务控制">通过聚合支付进行分布式事务控制</h2><p>如果在对接过程中，直接使用支付宝的二维码通知用户进行当面支付。而用户使用支付宝扫码支付的过程，电商都是完全不知道的，也就没有办法对用户的支付动作进行控制。比如如果电商本地的订单已经超时，就要阻止用户进行扫码支付。可以在支付宝的回调接口判断订单状态，如果订单式已关闭，则发起订单回退。这样显然效率是不高的。</p><p>在很多电商项目中，会采用聚合支付的方式，统一对接多个第三方支付方。用户的支付动作就不是直接与支付宝这样的第三方支付公司交互完成，而是要经过电商后台转发请求完成。这时，就可以通过添加一些分布式锁机制，保证整个支付业务是串行执行的，以<strong>防止在电商进行订单超时回退后，用户再次扫码支付</strong>。</p><h2 id="正向通知与反向通知">正向通知与反向通知</h2><p>当前是通过事务消息通知下游服务订单取消，这其实就是一种反向通知的方式。但是其实最直观的方式还是使用正向通知，即通过事务消息通知下游服务进行订单支付确认，这样这个下单的消息就容易扩展更多的下游消费者。订单下单确认是用户完成支付后，支付宝发起的通知来确认的。这时，如果<strong>订单确认的下游服务实现了幂等控制</strong>，就完全可以将事务消息机制改为正向通知。即在事务消息回查过程中，确认用户已经完成了支付，就发送消息通知下游服务订单支付成功。这样也可以防止支付宝通知丢失造成的订单状态缺失。</p><p>而用户订单超时判断，则可以在事务消息的状态回查过程中，通过记录回查次数判断。如果已经超时，则返回Rollback。同时启动另外一个消息生产者，往下游服务发送一个订单取消的消息，这样也是可以的。</p><h2 id="兜底补偿机制">兜底补偿机制</h2><p>对于订单超时后的回退处理，不光通过RocketMQ的事务消息进行了通知，另外也部署<strong>定时任务</strong>，批量回退超时的订单。<br>这其实就是一种事务消息的兜底补偿机制，以处理那些事务消息机制有可能漏处理的超时订单。在设计金融相关业务时，这种兜底策略会显得尤为重要。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;延迟任务（不推荐）&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;下单时增加一个定时任务，在五分钟后对订单进行超时判断。&lt;/li&gt;
&lt;li&gt;超时判断时，可以先去支付宝上查询订单支付状态。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果已支付，则判断订单是否正常结束，这是因为在用户完成扫码支付后，支付宝正</summary>
      
    
    
    
    <category term="电商系统" scheme="https://jxch.github.io/categories/%E7%94%B5%E5%95%86%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="支付超时" scheme="https://jxch.github.io/tags/%E6%94%AF%E4%BB%98%E8%B6%85%E6%97%B6/"/>
    
  </entry>
  
  <entry>
    <title>07.订单系统设计</title>
    <link href="https://jxch.github.io/2023/07/09/architect/dian-shang-xi-tong/07-ding-dan-xi-tong-she-ji/"/>
    <id>https://jxch.github.io/2023/07/09/architect/dian-shang-xi-tong/07-ding-dan-xi-tong-she-ji/</id>
    <published>2023-07-08T21:57:56.000Z</published>
    <updated>2023-07-09T20:26:33.964Z</updated>
    
    <content type="html"><![CDATA[<h1>重复下单问题（幂等）</h1><ol><li>用户在点击“提交订单”的按钮时，不小心点了两下</li><li>网络错误也有可能会导致重传，很多RPC框架和网关都拥有自动重试机制</li></ol><h2 id="font-color-green-主键唯一约束-font"><font color="green">主键唯一约束</font></h2><p>为订单系统增加一个“生成订单号”的服务,这个服务没有参数,返回值就是一个新的、全局唯一的订单号。在用户进入创建订单的页面时，<font color="green"><strong>前端页面会先调用这个生成订单号的服务得到一个订单号，在用户提交订单的时候，在创建订单的请求中带着这个订单号。</strong></font>这个订单号就是订单表的主键，这样，无论是用户原因,还是网络原因等各种情况导致的重试,这些重复请求中的订单号都是相同的。订单服务在订单表中插入数据的时候，<strong>这些重复的INSERT语句中的主键，都是同一个订单号</strong>。数据库的主键唯一约束特性就可以保证，只有一次INSERT 语句的执行是成功的,这样就实现了创建订单服务的幂等性。</p><p>还有一点需要注意的是，在具体实现时，如果是因为重复订单导致插入订单表的语句失败，那么订单服务就不要再把这个错误返回给前端页面了。否则，就有可能会出现用户点击创建订单按钮后，页面提示创建订单失败，而实际上订单已经创建成功了。正确的做法是,遇到这种情况,订单服务直接返回“订单创建成功”的响应即可。要做到这一点，可以捕获 <code>java.sql.SQLIntegrityConstraintViolationException</code> 或者 <code>org.springframework.dao.DuplicateKeyException</code> 来实现。</p><h1>订单ABA问题（幂等）</h1><p>比如连续两次更新订单信息，第一次的更新由于网络问题发生重试，又覆盖了第二次的更新，导致ABA问题</p><h2 id="font-color-green-版本戳-font"><font color="green">版本戳</font></h2><p>数据库表添加<strong>version字段</strong>，每次查询订单的时候，版本号需要随着订单数据返回给页面。<font color="green"><strong>页面在更新数据的请求时,需要把该版本号作为更新请求的参数再带回给订单更新服务</strong></font>。订单服务在更新数据的时候需要<strong>比较订单当前数据的版本号与消息中的版本号是否一致</strong>，如果不一致就拒绝更新数据。如果版本号一致，则还需要在更新数据的同时，把版本号<strong>加1</strong>。当然需要特别注意的是，“比较版本号、更新数据和把版本号加1”这个过程必须在同一个事务里面执行，只有这一系列操作具备<strong>原子性</strong>，才能真正保证并发操作的安全性。<code>UPDATE orders set tracking_number = 666,version = version + 1 WHERE version = ?;</code></p><h1>读写分离</h1><p>使用Redis 作为MySQL 的前置缓存，可以帮助MySQL 挡住绝大部分的查询请求。但是与用户相关的系统,即使同一个功能界面，用户看到的数据也是不一样的。使用缓存的效果就没有那么好了.随着系统的用户数量越来越多,穿透到MySQL 数据库中的读写请求也会越来越多</p><p>读写分离是提升 MySQL 并发能力的<strong>首选方案</strong>，当单个MySQL 无法满足要求的时候，只能用多个MySQL 实例来承担大量的读写请求。MySQL 与大部分常用的关系型数据库一样,都是典型的单机数据库,不支持分布式部署。用一个单机数据库的多个实例组成一个集群,提供分布式数据库服务,是一件非常困难的事情。一个简单且非常有效的是用多个具有相同数据的MySOL 实例来分担大量查询请求，也就是“读写分离”。很多系统，特别是互联网系统，数据的读写比例严重不均衡，<strong>读写比例一般在9:1</strong>到几十比1，即平均每发生几十次查询请求，才会有一次更新请求，那就是说数据库需要应对的绝大部分请求都是只读查询请求。</p><p><font color="gray">分布式存储系统支持分布式写是非常困难的，因为很难解决好数据一致性的问题。但分布式读相对来说就简单得多，能够把数据尽可能实时同步到只读实例上，它们就可以分担大量的查询请求了。读写分离的另一个好处是，实施起来相对比较简单。把使用单机MySQL 的系统升级为读写分离的多实例架构非常容易，一般不需要修改系统的业务逻辑，只需要简单修改DAO (Data Access Object,一般指应用程序中负责访问数据库的抽象层)层的代码,把对数据库的读写请求分开，请求不同的MySQL 实例就可以了。通过读写分离这样一个简单的存储架构升级，数据库支持的并发数量就可以增加几倍到十几倍。所以，当系统的用户数越来越多时，读写分离应该是首要考虑的扩容方案。</font></p><h2 id="数据不一致问题">数据不一致问题</h2><p>读写分离的一个副作用是，可能会存在数据不一致的问题。原因是数据库中的数据在主库完成更新后，是异步同步到每个从库上的，这个过程会有一个微小的时间差。正常情况下，主从延迟非常小，以几毫秒计。但即使是这样小的延迟，也会导致在某个时刻主库和从库上数据不一致的问题。应用程序需要能够接受并克服这种主从不一致的情况，否则就会引发一些由于主从延迟而导致的数据错误。</p><p>回顾我们的订单系统业务，用户对购物车发起商品结算创建订单，进入订单页，打开支付页面进行支付，支付完成后，按道理应该再返回到支付之前的订单页。但如果这时马上自动返回到订单页，就很有可能会出现订单状态还是显示“未支付”的问题。因为支付完成后，订单库的主库中订单状态已经更新了，但订单页查询的从库中这条订单记录的状态可能还未更新，如何解决这种问题呢？其实这个问题并没有特别好的技术手段来解决，所以可以看到，稍微上点规模的电商网站并不会支付完成后自动跳到到订单页，而是<font color="green"><strong>增加了一个支付完成页面</strong></font>，这个页面其实没有任何新的有效信息，就是告诉你支付成功的信息。如果想再查看一下刚刚支付完成的订单，需要手动选择，这样就能很好地规避主从同步延迟的问题。</p><p><strong>如果是那些数据更新后需要立刻查询的业务，这两个步骤可以放到一个数据库事务中，同一个事务中的查询操作也会被路由到主库</strong>，这样就可以规避主从不一致的问题了，还有一种解决方式则是<strong>对查询部分单独指定进行主库查询</strong>。总的来说，对于这种因为主从延迟而带来的数据不一致问题，并没有一种简单方便且通用的技术方案可以解决，对此，我们需要重新设计业务逻辑，<font color="green"><strong>尽量规避更新数据后立即去从库查询刚刚更新的数据</strong></font>。</p><h1>分库分表</h1><p>绝大部分电商企业的在线交易类业务，比如订单、支付相关的系统，还是无法离开MySQL的。原因是只有MySOL之类的关系型数据库，才能提供金融级的事务保证。目前的分布式事务的各种解法方案多少都有些不够完善。</p><h2 id="如何规划分库分表">如何规划分库分表</h2><p>以订单表为例，首先,我们需要思考的问题是，选择分库还是分表，或者两者都有，分库就是把数据拆分到不同的MySQL 数据库实例中，分表就是把数据拆分到一个数据库的多张表里面。在考虑到底是选择分厍还是分表之前,我们需要首先明确一个原则，那就是<strong>能小拆就小拆</strong>. 原因很简单，数据拆得越分散,并发和维护就越麻烦，系统出问题的概率也就越大。</p><p>遵循上面这个原则，还需要进一步了解，哪种情况适合分表，哪种情况适合分库。选择分厍或是分表的目的是解决如下两个问题。</p><ol><li>是为了解决因数据量太大而导致查询慢的问题。这里所说的“查询”，其实主要是事务中的查询和更新操作，因为只读的查询可以通过缓存和主从分离来解决。分表主要用于解决因数据量大而导致的查询慢的问题。</li><li>是为了应对高并发的问题。如果一个数据库实例撑不住，就把并发请求分散到多个实例中，所以分库可用于解决高并发的问题。</li></ol><p>简单地说，<strong>如果数据量太大，就分表；如果并发请求量高，就分库</strong>。一般情况下,我们的解决方案大都需要同时做分库分表,我们可以根据预估的并发量和数据量，分别计算应该拆分成多少个库以及多少张表。</p><h2 id="分库分表案例">分库分表案例</h2><h3 id="数据量">数据量</h3><p>在设计系统，我们预估订单的数量每个月订单2000W，一年的订单数可达2.4 亿。而每条订单的大小大致为1KB，按照我们在MySQL 中学习到的知识，为了让B+树的高度控制在一定范围，保证查询的性能，<strong>每个表中的数据不宜超过2000W</strong>。在这种情况下，为了存下2.4 亿的订单，我们似乎应该将订单表分为16（12 往上取最近的2 的幂）张表。</p><p>但是这样设计，有个问题，我们只考虑了订单表，没有考虑<strong>订单详情表</strong>。我们预估一张订单下的商品平均为10 个，那既是一年的订单详情数可以达到24亿，同样以每表2000W 记录计算，应该订单详情表为128（120 往上取最近的2的幂）张，而订单表和订单详情表虽然记录数上是一对一的关系，但是表之间还是一对一，也就是说订单表也要为128 张。经过再三分析，我们最终将订单表和订单详情表的张数定为32 张。这会导致订单详情表单表的数据量达到8000W（但是我们会对历史数据进行数据迁移，比如迁移到Mongodb或es）</p><h3 id="选择分片键">选择分片键</h3><p>选择分片链有一个最重要的参考因素是我们的业务是如何访问数据的？</p><p>比如我们把订单ID 作为分片键来诉分订单表。那么拆分之后,如果按照订单ID 来查询订单,就需要先根据订单ID 和分片算法,计算所要查的这个订单具体在哪个分片上，也就是哪个库的哪张表中，然后再去那个分片执行查询操作即可。但是当用户打开“我的订单”这个页面的时候，它的查询条件是用户ID，由于这里没有订单ID，因此我们无法知道所要查询的订单具体在哪个分片上，也就没法查了。如果要强行查询的话，那就只能把所有的分片都查询一遍，再合并查询结果，这个过程比较麻烦，而且性能很差，对分页也很不友好。</p><p>这个问题的解决办法是，在生成订单ID的时候，<font color="green"><strong>把用户ID的后几位作为订单ID的一部分。这样按订单ID查询的时候，就可以根据订单ID中的用户ID找到分片。</strong></font><strong>所以在我们的系统中订单ID从唯一ID服务获取ID后，还会将用户ID的后两位拼接，形成最终的订单ID。</strong></p><p>然而，系统对订单的查询万式，肯定不只是按订单ID 或按用户ID 查询两种方式。比如如果有商家希望查询自家家店的订单，有与订单相关的各种报表。对订单做了分库分表，就没法解决了。这个问题又该怎么解决呢？</p><p>一般的做法是，<font color="green"><strong>把订单里数据同步到其他存储系统中</strong></font>，然后在其他存储系统里解决该问题。比如可以再<strong>构建一个以店铺ID作为分片键的只读订单库，专供商家使用</strong>。或者数据同步到Hadoop分布式文件系统（HDFS）中，然后通过一些大数据技术生成与订单相关的报表。</p><p>在分片算法上，我们知道常用的有按范围，比如时间范围分片，哈希分片，查表法分片。一旦做了分库分表，就会极大地限制数据库的查询能力，原本很简单的查询，分库分表之后，可能就没法实现了。分库分表一定是在数据量和并发请求量大到所有招数都无效的情况下，我们才会采用的<strong>最后一招</strong>。</p><h3 id="具体实现">具体实现</h3><ol><li>纯手工方式：修改应用程序的DAO 层代码，定义多个数据源，在代码中需要访问数据库的每个地方指定每个数据库请求的数据源。</li><li>组件方式：使用像<font color="green"><strong>Sharding-JDBC</strong></font>这些组件集成在应用程序内，用于代理应用程序的所有数据库请求，并把请求自动路由到对应的数据库实例上。</li><li>代理方式: 在应用程序和数据库实例之间部署一组数据库代理实例,比如Atlas 或Sharding-Proxy。对于应用程序来说,数据库代理把自己伪装成一个单节点的MySQL 实例,应用程序的所有数据库请求都将发送给代理，代理分离请求，然后将分离后的请求转发给对应的数据库实例。</li></ol><p>在这三种方式中一般推荐第二种，使用分离组件的方式。采用这种方式,<strong>代码侵入非常少</strong>,同时还能兼顾性能和稳定性。<br>如果应用程序是一个逻辑非常简单的微服务,简单到只有几个SQL,或者应用程序使用的编程语言没有合适的读写分离组件,那么也可以考虑通过纯手工的方式。<br>不推荐使用代理方式（第三种方式），原因是代理方式加长了系统运行时数据库请求的调用链路,会造成一定的性能损失，而且代理服务本身也可能会出现故障和性能瓶颈等问题。代理方式有一个好处，对应用程序完全透明。</p><h1>归档历史数据</h1><p>所谓归档，也是一种拆分数据的策略。简单地说，就是把大量的历史订单移到另外一张历史订单表或数据存储中。为什这么做呢？订单数据有个特点：具备时间属性的，并且随着系统的运行，数据累计增长越来越多。但其实订单数据在使用上有个特点，最近的数据使用最频繁，超过一定时间的数据很少使用，这被称之为<font color="green"><strong>热尾效应</strong></font>。</p><p>因为新数据只占数据息量中很少的一部分，所以把新老数据分开之后，新数据的数据量就少很多，查询速度也会因此快很多。虽然与之前的总量相比，老数据没有减少太多，但是因为老数据很少会被访问到，所以即使慢一点儿也不会有太大的问题，而且还可以使用其他的存储系统提升查询速度。</p><p>这样拆分数据的另外一个好处是，拆分订单时，<strong>系统需要改动的代码非常少</strong>。对订单表的大部分操作都是在订单完成之前执行的，这些业务逻辑都是完全不用修改的。即使是像退货退款这类订单完成之后的操作，也是有时限的，这些业务逻辑也不需要修改,还是按照之前那样操作订单即可。</p><p><strong>基本上只有查询统计类的功能会查到历史订单</strong>，这些都需要稍微做些调整。按照查询条件中的时间范围，选择去订单表还是历史订单中查询就可以了。很多大型互联网电商在逐步发展壮大的过程中，长达数年的时间采用的都是这种订单拆分的方案，正如我们前面看到的京东就是如此。</p><h2 id="分布式事务">分布式事务</h2><p>考察迁移的过程，我们是逐表批次删除，对于每张订单表，先从MySQL 从获得指定批量的数据，写入MongoDB，再从MySQL 中删除已写入MongoDB 的部分，这里存在着一个多源的数据操作，为了保证数据的一致性，看起来似乎需要分布式事务。<font color="green"><strong>但是其实这里并不需要分布式事务，解决的关键在于写入订单数据到MongoDB 时，我们要记住同时写入当前迁入数据的最大订单ID，让这两个操作执行在同一个事务之中。这样，在MySQL 执行数据迁移时，总是去MongoDB 中获得上次处理的最大OrderId，作为本次迁移的查询起始ID。当然数据写入MongoDB 后，还要记得删除MySQL 中对应的数据。</strong></font></p><p>在这个过程中，我们需要注意的问题是，<strong>尽量不要影响线上的业务</strong>。迁移如此大量的数据，或多或少都会影响数据库的性能，因此应该尽量选择在<strong>闲时迁移</strong>而且每次数据库操作的记录数不宜太多。按照一般的经验，对MySQL 的操作的记录条数<strong>每次控制在10000以下是比较合适</strong>，在我们的系统中缺省是2000 条。更重要的是，迁<strong>移之前一定要做好备份</strong>，这样的话，即使不小心误操作了，也能用备份来恢复。</p><h2 id="如何批量删除大量数据">如何批量删除大量数据</h2><p>虽然我们是按时间迁出订单表中的数据，<font color="green"><strong>但是删除最好还是按ID来删除，并且同样要控制住每次删除的记录条数</strong></font>，太大的数量容易遇到错误。</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">delete</span> <span class="token keyword">from</span> ${orderTableName} o <span class="token keyword">WHERE</span> o<span class="token punctuation">.</span>id <span class="token operator">&gt;=</span> <span class="token comment">#{minOrder!d} and o.id &lt;= #{maxOrderId} </span><span class="token keyword">order</span> <span class="token keyword">by</span> id<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这样每次删除的时候，由于条件变成了主键比较，而在MySQL 的InnoDB 存储引擎中，表数据结构就是按照主键组织的一棵B+树，同时B+树本身就是有序的，因此优化后不仅查找变得非常快,而且也不需要再进行额外的排序操作了。</p><p>为什么要加一个排序的操作呢？因为<font color="green"><strong>按ID排序后，每批删除的记录基本上都是ID连续的一批记录</strong></font>，由于B+树的有序性，这些ID 相近的记录，在磁盘的物理文件上，大致也是存放在一起的，这样删除效率会比较高，也便于MySQL 回收页。</p><p>关于大批量删除数据，还有一个点需要注意一下，<font color="green"><strong>执行删除语句后，最好能停顿一小会，因为删除后肯定会牵涉到大量的B+树页面分裂和合并</strong></font>，这个时候MySQL的本身的负载就不小了，停顿一小会，可以让MySQL的负载更加均衡。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;重复下单问题（幂等）&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;用户在点击“提交订单”的按钮时，不小心点了两下&lt;/li&gt;
&lt;li&gt;网络错误也有可能会导致重传，很多RPC框架和网关都拥有自动重试机制&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;font-color-green-主键唯一约束-f</summary>
      
    
    
    
    <category term="电商系统" scheme="https://jxch.github.io/categories/%E7%94%B5%E5%95%86%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="订单系统" scheme="https://jxch.github.io/tags/%E8%AE%A2%E5%8D%95%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>06.微服务网关整合OAuth2.0授权中心</title>
    <link href="https://jxch.github.io/2023/07/09/architect/dian-shang-xi-tong/06-wei-fu-wu-wang-guan-zheng-he-oauth2-0-shou-quan-zhong-xin/"/>
    <id>https://jxch.github.io/2023/07/09/architect/dian-shang-xi-tong/06-wei-fu-wu-wang-guan-zheng-he-oauth2-0-shou-quan-zhong-xin/</id>
    <published>2023-07-08T19:07:10.000Z</published>
    <updated>2023-07-08T19:37:12.576Z</updated>
    
    <content type="html"><![CDATA[<ol><li>配置授权服务器（AuthorizationServerConfigurerAdapte - 授权码/密码）<ol><li>DB模式</li><li>内存模式</li></ol></li><li>SpringSecurity（WebSecurityConfigurerAdapter）</li><li>JWT：头部（header）、载荷（payload）与签名（signature 加盐）<ol><li>一次性验证、无状态认证</li><li>注销续约复杂，不适合会话管理</li></ol></li><li>JWT非对称加密（公钥私钥）</li><li>扩展JWT中的存储内容（TokenEnhancer）</li><li>接入网关服务（GlobalFilter）<ol><li>过滤不需要认证的url</li><li>校验token（需要从授权服务获取公钥）<ul><li>不能直接通过@LoadBalancer配置RestTemplate去获取公钥，因为Spring容器启动过程中@LoadBalancer还未生效</li></ul></li><li>校验通过后，从token中获取的用户登录信息存储到请求头中</li></ol></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ol&gt;
&lt;li&gt;配置授权服务器（AuthorizationServerConfigurerAdapte - 授权码/密码）
&lt;ol&gt;
&lt;li&gt;DB模式&lt;/li&gt;
&lt;li&gt;内存模式&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;SpringSecurity（WebSecurityCo</summary>
      
    
    
    
    <category term="电商系统" scheme="https://jxch.github.io/categories/%E7%94%B5%E5%95%86%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="授权中心" scheme="https://jxch.github.io/tags/%E6%8E%88%E6%9D%83%E4%B8%AD%E5%BF%83/"/>
    
  </entry>
  
  <entry>
    <title>05.分布式唯一ID</title>
    <link href="https://jxch.github.io/2023/07/09/architect/dian-shang-xi-tong/05-fen-bu-shi-wei-yi-id/"/>
    <id>https://jxch.github.io/2023/07/09/architect/dian-shang-xi-tong/05-fen-bu-shi-wei-yi-id/</id>
    <published>2023-07-08T18:02:08.000Z</published>
    <updated>2023-07-08T18:49:31.179Z</updated>
    
    <content type="html"><![CDATA[<ul><li><strong>全局唯一性</strong>：不能出现重复的ID号，既然是唯一标识，这是最基本的要求。</li><li><strong>趋势递增、单调递增</strong>：保证下一个ID一定大于上一个ID。</li><li><strong>信息安全</strong>：如果ID是连续的，恶意用户的扒取工作就非常容易做了，直接按照顺序下载指定URL 即可；如果是订单号就更危险了，竞对可以直接知道我们一天的单量。所以在一些应用场景下，会需要ID 无规则、不规则。</li></ul><h1>常见方法</h1><h2 id="UUID">UUID</h2><p>UUID(Universally Unique Identifier)的标准型式包含 32 个 16 进制数字，以连字号分为五段，<strong>形式为 8-4-4-4-12 的 32 个字符</strong>，示例：<br>550e8400-e29b-41d4-a716-446655440000，到目前为止业界一共有5种方式生成UUID，详情见IETF 发布的UUID 规范 A Universally Unique IDentifier (UUID) URN Namespace。</p><ul><li>优点：性能非常高，本地生成，没有网络消耗。</li><li>缺点：<ul><li>不易于存储：UUID 太长，16 字节128 位，通常以36 长度的字符串表示，很多场景不适用。</li><li>信息不安全：基于 MAC 地址生成 UUID 的算法可能会造成 <font color="red"><strong>MAC 地址泄露</strong></font>，这个漏洞曾被用于寻找梅丽莎病毒的制作者位置。</li></ul></li><li>ID 作为主键时在特定的环境会存在一些问题，比如做 DB 主键的场景下，UUID 就非常不适用<ul><li>MySQL 官方有明确的建议主键要尽量越短越好，36 个字符长度的 UUID 不符合要求。</li><li>对 MySQL 索引不利：如果作为数据库主键，在InnoDB 引擎下，UUID 的无序性可能会引起数据位置频繁变动，严重影响性能。在MySQL InnoDB 引擎中使用的是聚集索引，由于多数 RDBMS 使用 B-tree 的数据结构来存储索引数据，在主键的选择上面我们应该<font color="red"><strong>尽量使用有序的主键保证写入性能</strong></font>。</li></ul></li></ul><h2 id="雪花算法">雪花算法</h2><p>这种方案大致来说是一种以划分命名空间（UUID 也算，由于比较常见，所以单独分析）来生成ID 的一种算法，Snowflake 是Twitter 开源的分布式 ID 生成算法。Snowflake 把64-bit分别划分成多段</p><ul><li>第 0 位： 符号位（标识正负），始终为 0，没有用，不用管。</li><li>第 1~41 位 ：一共41位，用来表示<strong>时间戳</strong>，单位是毫秒，可以支撑 2 ^41 毫秒（约69 年）</li><li>第 42~52 位 ：一共10位，一般来说，<strong>前5位表示机房 ID，后5位表示机器ID</strong>（实际项目中可以根据实际情况调整），这样就可以区分不同集群/机房的节点，这样就可以表示32 个IDC，每个IDC 下可以有32 台机器。</li><li>第 53~64 位 ：一共12位，用来表示<strong>序列号</strong>。 序列号为自增值，代表单台机器每毫秒能够产生的最大 ID 数(2^12 = 4096),也就是说单台机器<strong>每毫秒最多可以生成4096个唯一ID</strong>。</li></ul><p>理论上snowflake 方案的QPS约为409.6w/s，这种分配方式可以保证在任何一个IDC的任何一台机器在任意毫秒内生成的ID都是不同的。</p><ul><li>优点：<ul><li>毫秒数在高位，自增序列在低位，整个ID都是趋势递增的。</li><li>不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的。</li><li>可以根据自身业务特性分配bit位，非常灵活。</li></ul></li><li>缺点：强依赖机器时钟，如果机器上<font color="red"><strong>时钟回拨</strong></font>，会导致发号重复或者服务会处于不可用状态。</li></ul><h2 id="Mongdb-ObjectID">Mongdb ObjectID</h2><p>它也可以算作是和snowflake类似方法，通过 “<strong>时间+机器码+pid+inc</strong>” 共12 个字节，通过4+3+2+3的方式最终标识成一个24长度的十六进制字符。</p><h2 id="Seata-UUID">Seata UUID</h2><p>Seata 内置了一个分布式 UUID 生成器，用于辅助生成全局事务 ID 和分支事务 ID，我们同样可以拿来使用，完整类名为：<code>io.seata.common.util.IdWorker</code></p><h2 id="数据库生成-MySQL">数据库生成 (MySQL)</h2><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> <span class="token identifier"><span class="token punctuation">`</span>sequence_id<span class="token punctuation">`</span></span> <span class="token punctuation">(</span> <span class="token identifier"><span class="token punctuation">`</span>id<span class="token punctuation">`</span></span> <span class="token keyword">bigint</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> <span class="token keyword">unsigned</span> <span class="token operator">NOT</span> <span class="token boolean">NULL</span> <span class="token keyword">AUTO_INCREMENT</span><span class="token punctuation">,</span> <span class="token identifier"><span class="token punctuation">`</span>stub<span class="token punctuation">`</span></span> <span class="token keyword">char</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">NOT</span> <span class="token boolean">NULL</span> <span class="token keyword">DEFAULT</span> <span class="token string">''</span><span class="token punctuation">,</span> <span class="token keyword">PRIMARY</span> <span class="token keyword">KEY</span> <span class="token punctuation">(</span><span class="token identifier"><span class="token punctuation">`</span>id<span class="token punctuation">`</span></span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token keyword">UNIQUE</span> <span class="token keyword">KEY</span> <span class="token identifier"><span class="token punctuation">`</span>stub<span class="token punctuation">`</span></span> <span class="token punctuation">(</span><span class="token identifier"><span class="token punctuation">`</span>stub<span class="token punctuation">`</span></span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">ENGINE</span><span class="token operator">=</span><span class="token keyword">InnoDB</span> <span class="token keyword">DEFAULT</span> <span class="token keyword">CHARSET</span><span class="token operator">=</span>utf8mb4<span class="token punctuation">;</span>stub<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>段无意义，只是为了占位，便于我们插入或者修改数据。并且，给 stub 字段创建了唯一索引，保证其唯一性。</p><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">BEGIN</span><span class="token punctuation">;</span><span class="token keyword">REPLACE</span> <span class="token keyword">INTO</span> sequence_id <span class="token punctuation">(</span>stub<span class="token punctuation">)</span> <span class="token keyword">VALUES</span> <span class="token punctuation">(</span><span class="token string">'stub'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">SELECT</span> LAST_INSERT_ID<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">COMMIT</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>插入数据这里，我们没有使用 insert into 而是使用 replace into 来插入数据。replace 是insert 的增强版，replace into 首先尝试插入数据到表中，</p><ol><li>如果发现表中已经有此行数据（根据主键或者唯一索引判断）则先删除此行数据，然后插入新的数据。</li><li>否则，直接插入新数据。</li></ol><ul><li>优点：非常简单，利用现有数据库系统的功能实现，成本小，有DBA 专业维护。ID号单调自增，存储消耗空间小。</li><li>缺点：<ul><li>支持的并发量不大</li><li>存在数据库单点问题（可以使用数据库集群解决，不过增加了复杂度）</li><li>ID 没有具体业务含义、安全问题（比如<font color="red"><strong>根据订单 ID 的递增规律就能推算出每天的订单量</strong></font>，商业机密啊！ ）</li><li>每次获取 ID 都要访问一次数据库（<font color="red"><strong>增加了对数据库的压力</strong></font>，获取速度也慢）</li></ul></li></ul><p>对于MySQL 性能问题，可用如下方案解决：在分布式系统中我们可以<strong>多部署几台机器，每台机器设置不同的初始值，且步长和机器数相等</strong>。比如有两台机器。设置步长step 为2，TicketServer1 的初始值为1（1，3，5，7，9，11…）、TicketServer2 的初始值为2（2，4，6，8，10…）。这是Flickr（雅虎旗下图片分享网站）团队在2010 年撰文介绍的一种主键生成策略（Ticket Servers: Distributed Unique Primary Keys on the Cheap ）。为了实现上述方案分别设置两台机器对应的参数，TicketServer1 从1 开始发号，TicketServer2 从2 开始发号，两台机器每次发号之后都递增2。假设我们要部署N 台机器，步长需设置为N，每台的初始值依次为0,1,2…N-1。</p><p>这种架构貌似能够满足性能的需求，但有以下几个缺点：</p><ul><li>系统<font color="red"><strong>水平扩展比较困难</strong></font>，比如定义好了步长和机器台数之后，如果要添加机器该怎么做？假设现在只有一台机器发号是1,2,3,4,5（步长是1），这个时候需要扩容机器一台。可以这样做：把第二台机器的初始值设置得比第一台超过很多，比如140（假设在扩容时间之内第一台不可能发到140），同时设置步长为2，那么这台机器下发的号码都是140 以后的偶数。然后摘掉第一台，把ID 值保留为奇数，比如7，然后修改第一台的步长为2。让它符合我们定义的号段标准，对于这个例子来说就是让第一台以后只能产生奇数。扩容方案看起来复杂吗？貌似还好，现在想象一下如果我们线上有100 台机器，这个时候要扩容该怎么做？简直是噩梦。所以系统水平扩展方案复杂难以实现。</li><li>ID <font color="red"><strong>没有了单调递增的特性</strong></font>，只能趋势递增，这个缺点对于一般业务需求不是很重要，可以容忍。</li><li><strong>数据库压力</strong>还是很大，每次获取ID 都得读写一次数据库，只能靠堆机器来提高性能。</li></ul><h2 id="Redis">Redis</h2><p>通过 Redis 的 incr 命令即可实现对 id 原子顺序递增。为了提高可用性和并发，我们可以使用 Redis Cluster。</p><p>除了高可用和并发之外，我们知道 Redis 基于内存，我们需要<strong>持久化数据</strong>，避免重启机器或者机器故障后数据丢失。很明显，Redis 方案性能很好并且生成的 ID 是有序递增的。不过，我们也知道，即使 Redis 开启了持久化，不管是快照（snapshotting，RDB）、只追加文件（append-only file, AOF）还是 RDB 和AOF 的混合持久化依然<font color="red"><strong>存在着丢失数据的可能，那就意味着产生的 ID 存在着重复的概率</strong></font>。</p><h1>分布式ID微服务</h1><h2 id="Leaf-segment">Leaf-segment</h2><p>Leaf-segment 方案，在使用数据库的方案上，做了如下改变：</p><ul><li>原 MySQL 方案每次获取 ID 都得读写一次数据库，造成数据库压力大。改为<strong>批量获取</strong>，每次获取一个segment(step 决定大小)号段的值。用完之后再去数据库获取新的号段，可以大大的减轻数据库的压力。</li><li><strong>各个业务不同的发号需求用 biz_tag 字段来区分</strong>，每个 biz-tag 的 ID 获取相互隔离，互不影响。如果以后有性能需求需要对数据库扩容，不需要上述描述的复杂的扩容操作，只需要对 biz_tag 分库分表就行。</li><li>biz_tag 用来区分业务，max_id 表示该 biz_tag 目前所被分配的 ID 号段的最大值，step 表示每次分配的号段长度。原来获取ID 每次都需要写数据库，现在只需要把step 设置得足够大，比如1000。那么只有当1000 个号被消耗完了之后才会去重新读写一次数据库。<strong>读写数据库的频率从1减小到了1/step</strong>。</li></ul><pre class="line-numbers language-sql" data-language="sql"><code class="language-sql"><span class="token keyword">Begin</span><span class="token keyword">UPDATE</span> <span class="token keyword">table</span> <span class="token keyword">SET</span> max_id<span class="token operator">=</span>max_id<span class="token operator">+</span>step <span class="token keyword">WHERE</span> biz_tag<span class="token operator">=</span>xxx<span class="token keyword">SELECT</span> tag<span class="token punctuation">,</span> max_id<span class="token punctuation">,</span> step <span class="token keyword">FROM</span> <span class="token keyword">table</span> <span class="token keyword">WHERE</span> biz_tag<span class="token operator">=</span>xxx<span class="token keyword">Commit</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li>优点：<ul><li>Leaf 服务可以很方便的<strong>线性扩展</strong>，性能完全能够支撑大多数业务场景。ID 号码是趋势递增的8byte 的64 位数字，满足上述数据库存储的主键要求。</li><li>容灾性高：Leaf 服务内部有<strong>号段缓存</strong>，即使DB 宕机，短时间内Leaf 仍能正常对外提供服务。</li><li>可以自定义 max_id 的大小，非常方便业务从原有的ID方式上迁移过来。</li></ul></li><li>缺点：<ul><li>ID 号码不够随机，能够<font color="red"><strong>泄露发号数量</strong></font>的信息，不太安全。</li><li>TP999 数据波动大，当号段使用完之后还是会在获取新号段时在更新数据库的I/O依然会存在着等待，tg999 数据会出现<font color="red"><strong>偶尔的尖刺</strong></font>。</li><li><strong>DB 宕机</strong>会造成整个系统不可用。</li></ul></li></ul><h3 id="双buffer优化">双buffer优化</h3><p><font color="gray">Leaf 取号段的时机是在号段消耗完的时候进行的，也就意味着号段临界点的ID 下发时间取决于下一次从DB取回号段的时间，并且在这期间进来的请求也会因为DB号段没有取回来，导致线程阻塞。如果请求DB 的网络和DB 的性能稳定，这种情况对系统的影响是不大的，但是假如取DB 的时候网络发生抖动，或者DB 发生慢查询就会导致整个系统的响应时间变慢。为此，希望DB 取号段的过程能够做到无阻塞，不需要在DB 取号段的时候阻塞请求线程，即当号段消费到某个点时就异步的把下一个号段加载到内存中。而不需要等到号段用尽的时候才去更新号段。这样做就可以很大程度上的降低系统的TP999 指标。</font><br>采用双buffer 的方式，Leaf 服务内部有两个号段缓存区segment。<strong>当前号段已下发10%时</strong>，如果下一个号段未更新，则<strong>另启一个更新线程去更新下一个号段</strong>。<strong>当前号段全部下发完后</strong>，如果下个号段准备好了则<strong>切换到下个号段为当前segment接着下发</strong>，循环往复。<br>通常<strong>推荐segment长度设置为服务高峰期发号QPS 的600 倍（10 分钟），这样即使DB 宕机，Leaf 仍能持续发号10-20 分钟不受影响</strong>。每次请求来临时都会判断下个号段的状态，从而更新此号段，所以偶尔的网络抖动不会影响下个号段的更新。</p><h3 id="Leaf-高可用容灾">Leaf 高可用容灾</h3><p>对于第三点“DB 可用性”问题，可以采用<strong>一主两从</strong>的方式，同时分机房部署，Master 和Slave 之间采用半同步方式同步数据。美团内部使用了奇虎360 的Atlas 数据库中间件（已开源，改名为DBProxy）做主从切换。当然这种方案在一些情况会退化成异步模式，甚至在非常极端情况下仍然会造成数据不一致的情况，但是出现的概率非常小。如果要保证100%的数据强一致，可以选择使用“类Paxos算法”实现的强一致MySQL 方案，如MySQL 5.7 中的MySQL Group Replication。但是运维成本和精力都会相应的增加，根据实际情况选型即可。</p><h2 id="Leaf-snowflake">Leaf-snowflake</h2><p>Leaf-segment 方案可以生成趋势递增的ID，同时ID 号是可计算的，不适用于订单ID 生成场景，比如竞对在两天中午12 点分别下单，通过订单id 号相减就能大致计算出公司一天的订单量，这个是不能忍受的。面对这一问题，美团提供了Leaf-snowflake 方案。</p><p>Leaf-snowflake 方案完全沿用snowflake 方案的bit 位设计，即是“1+41+10+12”的方式组装ID 号。对于workerID 的分配，当服务集群数量较小的情况下，完全可以手动配置。Leaf 服务规模较大，动手配置成本太高。所以<strong>使用Zookeeper 持久顺序节点的特性自动对snowflake节点配置wokerID</strong>。</p><p>Leaf-snowflake 是按照下面几个步骤启动的：</p><ul><li>启动Leaf-snowflake 服务，连接Zookeeper，在leaf_forever 父节点下检查自己是否已经注册过（是否有该顺序子节点）。</li><li>如果有注册过直接取回自己的workerID（zk 顺序节点生成的int 类型ID 号），启动服务。</li><li>如果没有注册过，就在该父节点下面创建一个持久顺序节点，创建成功后取回顺序号当做自己的workerID 号，启动服务。</li></ul><h3 id="弱依赖ZooKeeper">弱依赖ZooKeeper</h3><p>除了每次会去ZK 拿数据以外，也会在<strong>本机文件系统上缓存一个workerID文件</strong>。当ZooKeeper 出现问题，恰好机器出现问题需要重启时，能保证服务能够正常启动。这样做到了对三方组件的弱依赖。</p><h3 id="解决时钟问题">解决时钟问题</h3><p>首先在启动时，服务会进行检查：</p><ol><li>新节点通过检查综合对比其余Leaf 节点的系统时间来判断自身系统时间是否准确，具体做法是取所有运行中的Leaf-snowflake 节点的服务IP：Port，然后通过RPC 请求得到所有节点的系统时间，计算<strong>sum(time)/nodeSize</strong>，然后看本机时间与这个平均值是否在阈值之内来确定当前系统时间是否准确，准确正常启动服务，不准确认为本机系统时间发生大步长偏移，启动失败并报警。</li><li>在ZooKeeper 中登记过的老节点，同样会<strong>比较自身系统时间和ZooKeeper上本节点曾经的记录时间以及所有运行中的Leaf-snowflake节点的时间，不准确同样启动失败并报警</strong>。</li><li>在运行过程中，<strong>每隔一段时间节点都会上报自身系统时间写入ZooKeeper</strong>。</li></ol><p>在服务运行过程中，机器的NTP 同步也会造成秒级别的回退，由于强依赖时钟，对时间的要求比较敏感，美团建议有三种解决方案，</p><ol><li>可以<strong>直接关闭NTP同步</strong>；</li><li>在时钟回拨的时候<strong>直接不提供服务直接返回ERROR_CODE，等时钟追上</strong></li><li><strong>做一层重试，然后上报报警系统</strong>，更或者是发现有时钟回拨之后<strong>自动摘除本身节点并报警</strong></li></ol><p>从美团的实际运行情况来看，在2017 年闰秒出现那一次出现过部分机器回拨，由于Leaf-snowflake 的策略保证，成功避免了对业务造成的影响。<br>Leaf 在美团点评公司内部服务包含金融、支付交易、餐饮、外卖、酒店旅游猫眼电影等众多业务线。目前Leaf 的性能在4C8G 的机器上QPS 能压测到近<strong>5万/s</strong>，TP999 1ms，已经能够满足大部分的业务的需求。每天提供亿数量级的调用量。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;全局唯一性&lt;/strong&gt;：不能出现重复的ID号，既然是唯一标识，这是最基本的要求。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;趋势递增、单调递增&lt;/strong&gt;：保证下一个ID一定大于上一个ID。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;信息安全&lt;/st</summary>
      
    
    
    
    <category term="电商系统" scheme="https://jxch.github.io/categories/%E7%94%B5%E5%95%86%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="分布式唯一ID" scheme="https://jxch.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%94%AF%E4%B8%80ID/"/>
    
  </entry>
  
  <entry>
    <title>04.微服务全链路灰度发布</title>
    <link href="https://jxch.github.io/2023/07/08/architect/dian-shang-xi-tong/04-wei-fu-wu-quan-lian-lu-hui-du-fa-bu/"/>
    <id>https://jxch.github.io/2023/07/08/architect/dian-shang-xi-tong/04-wei-fu-wu-quan-lian-lu-hui-du-fa-bu/</id>
    <published>2023-07-08T08:59:10.000Z</published>
    <updated>2023-07-08T09:19:59.268Z</updated>
    
    <content type="html"><![CDATA[<p>灰度发布&nbsp;Gray&nbsp;Release（又名金丝雀发布&nbsp;Canary&nbsp;Release）。不停机旧版本，部署新版本，高比例流量（例如：95%）走旧版本，低比例流量（例如：5%）切换到新版本，通过监控观察无问题，逐步扩大范围，最终把所有流量都迁移到新版本上。属无损发布。</p><p>优点：灵活简单，不需要用户标记驱动。安全性高，新版本如果出现问题，只会发生在低比例的流量上<br>缺点：成本较高，需要部署稳定/灰度两套环境</p><p>微服务体系架构中，服务之间的依赖关系错综复杂，有时某个功能发版依赖多个服务同时升级上线。我们希望可以对这些服务的新版本同时进行小流量灰度验证，这就是微服务架构中特有的全链路灰度场景，通过构建从网关到整个后端服务的环境隔离来对多个不同版本的服务进行灰度验证。在发布过程中，我们只需部署服务的灰度版本，流量在调用链路上流转时，由流经的网关、各个中间件以及各个微服务来识别灰度流量，并动态转发至对应服务的灰度版本。<br>**无论是微服务网关还是微服务本身都需要识别流量，根据治理规则做出动态决策。当服务版本发生变化时，这个调用链路的转发也会实时改变。**相比于利用机器搭建的灰度环境，这种方案不仅可以节省大量的机器成本和运维人力，而且可以帮助开发者实时快速的对线上流量进行精细化的全链路控制。</p><h1>全链路灰度设计思路</h1><ol><li>链路上各个组件和服务能够根据请求流量特征进行<strong>动态路由</strong>。</li><li>需要对服务下的所有节点进行分组，能够区分版本。</li><li>需要对流量进行<strong>灰度标识、版本标识</strong>。</li><li>需要识别出不同版本的灰度流量。</li></ol><p>首先，需要支持动态路由功能，对于Spring&nbsp;Cloud、Dubbo开发框架，可以对出口流量实现自定义Filter，在该Filter中完成流量识别以及标签路由。同时需要借助分布式链路追踪技术完成流量标识链路传递以及流量自动染色。此外，需要引入一个中心化的流量治理平台，方便各个业务线的开发者定义自己的全链路灰度规则。实现全链路灰度的能力，无论是成本还是技术复杂度都是比较高的，以及后期的维护、扩展都是非常大的成本。</p><h2 id="标签路由">标签路由</h2><p>标签路由通过对服务下所有节点<strong>按照标签名和标签值不同进行分组</strong>，使得订阅该服务节点信息的服务消费端可以**按需访问该服务的某个分组，**即所有节点的一个子集。服务消费端可以使用服务提供者节点上的任何标签信息，根据所选标签的实际含义，消费端可以将标签路由应用到更多的业务场景中。</p><h2 id="节点打标">节点打标</h2><p><strong>给服务节点添加不同的标签</strong>：</p><ul><li><strong>K8S</strong>：在使用Kubernetes&nbsp;Service作为服务发现的业务系统中，服务提供者通过向ApiServer提交Service资源完成服务暴露，服务消费端监听与该Service资源下关联的Endpoint资源，从Endpoint资源中获取关联的业务Pod&nbsp;资源，读取上面的Labels数据并作为该节点的元数据信息。所以，我们<strong>只要在业务应用描述资源Deployment中的Pod模板中为节点添加标签即可</strong>。</li><li><strong>Nacos</strong>：一般是需要业务根据其使用的微服务框架来决定打标方式。如果Java应用使用的Spring&nbsp;Cloud微服务开发框架，我们可以为业务容器添加对应的环境变量来完成标签的添加操作。比如我们希望为节点添加版本灰度标，那么为业务容器添加<code>spring.cloud.nacos.discovery.metadata.version=gray</code>，这样框架向Nacos注册该节点时会为其添加一个标签<code>verison=gray</code>。</li></ul><h2 id="流量染色">流量染色</h2><p>为请求流量添加不同灰度标识来方便区分。我们可以在请求的源头上对流量进行染色，前端在发起请求时根据用户信息或者平台信息的不同对流量进行打标。如果前端无法做到，我们也可以在微服务网关上对匹配特定路由规则的请求动态添加流量标识。此外，流量在链路中流经灰度节点时，如果请求信息中不含有灰度标识，需要自动为其染色，接下来流量就可以在后续的流转过程中优先访问服务的灰度版本。<strong>保证请求链路上的各个组件能够识别出不同的灰度流量</strong>。</p><h2 id="分布式链路追踪">分布式链路追踪</h2><p>借助于分布式链路追踪思想，我们也可以传递一些自定义信息，比如灰度标识。<strong>保证灰度标识能够在链路中一直传递下去</strong>。</p><h1>框架</h1><ul><li><a href="https://github.com/Nepxion/Discovery">Discovery</a>：Spring</li><li><a href="https://help.aliyun.com/document_detail/359851.html">MSE</a>：阿里云平台</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;灰度发布&amp;nbsp;Gray&amp;nbsp;Release（又名金丝雀发布&amp;nbsp;Canary&amp;nbsp;Release）。不停机旧版本，部署新版本，高比例流量（例如：95%）走旧版本，低比例流量（例如：5%）切换到新版本，通过监控观察无问题，逐步扩大范围，最终把所有流量都</summary>
      
    
    
    
    <category term="电商系统" scheme="https://jxch.github.io/categories/%E7%94%B5%E5%95%86%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="微服务灰度发布" scheme="https://jxch.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83/"/>
    
  </entry>
  
  <entry>
    <title>03.微服务架构拆分</title>
    <link href="https://jxch.github.io/2023/07/08/architect/dian-shang-xi-tong/03-wei-fu-wu-jia-gou-chai-fen/"/>
    <id>https://jxch.github.io/2023/07/08/architect/dian-shang-xi-tong/03-wei-fu-wu-jia-gou-chai-fen/</id>
    <published>2023-07-08T04:48:20.000Z</published>
    <updated>2023-07-08T08:41:33.230Z</updated>
    
    <content type="html"><![CDATA[<h1>微服务拆分时机</h1><p>微服务不仅仅是技术的升级，更是开发方式、组织架构、开发观念的转变。</p><ul><li><strong>业务规模</strong>：业务模式得到市场的验证，需要进一步加快脚步快速占领市场，这时业务的规模变得越来越大，按产品生命周期来划分（导入期、成长期、成熟期、衰退期）这时一般在成长期阶段。<strong>如果是导入期，尽量采用单体架构</strong>。</li><li><strong>团队规模</strong>：一般是团队达到百人的时候，主要还是要结合业务复杂度</li><li><strong>技术储备</strong>：领域驱动设计、注册中心、配置中心、日志系统、持续交付、监控系统、分布式定时任务、CAP&nbsp;理论、分布式调用链、API&nbsp;网关等等。</li><li><strong>人才储备</strong>：精通微服务落地经验的架构师及相应开发人员。</li><li><strong>研发效率</strong>：研发效率大幅下降。</li></ul><h1>微服务拆分原则</h1><ul><li>单一服务内部功能<strong>高内聚低耦合</strong>：每个服务只完成自己职责内的任务，对于不是自己职责的功能交给其它服务来完成</li><li><strong>闭包原则</strong>（CCP）：微服务的闭包原则就是当我们需要改变一个微服务的时候，所有依赖都在这个微服务的组件内，不需要修改其他微服务</li><li><strong>服务自治、接口隔离</strong>原则：尽量消除对其他服务的强依赖，这样可以降低沟通成本，提升服务稳定性。服务通过标准的接口隔离，隐藏内部实现细节。这使得服务可以独立开发、测试、部署、运行，以服务为单位持续交付。</li><li><strong>持续演进</strong>原则：在服务拆分的初期，你其实很难确定服务究竟要拆成什么样。应逐步划分，持续演进，<strong>避免服务数量的爆炸性增长</strong>。</li><li>拆分的过程尽量<strong>避免影响产品的日常功能迭代</strong>：也就是说要一边做产品功能迭代，一边完成服务化拆分。比如<strong>优先剥离比较独立的边界服务</strong>（如短信服务等），从非核心的服务出发减少拆分对现有业务的影响，也给团队一个练习、试错的机会。同时<strong>当两个服务存在依赖关系时优先拆分被依赖的服务</strong>。</li><li><strong>服务接口的定义要具备可扩展性</strong>：比如微服务的接口因为升级把之前的三个参数改成了四个，上线后导致调用方大量报错，推荐做法服务接口的参数类型最好是封装类，这样如果增加参数就不必变更接口的签名</li><li><strong>避免环形依赖与双向依赖</strong>：尽量不要有服务之间的环形依赖或双向依赖，原因是存在这种情况说明我们的<strong>功能边界</strong>没有化分清楚或者有通用的功能没有下沉下来。</li><li><strong>阶段性合并</strong>：随着你对业务领域理解的逐渐深入或者业务本身逻辑发生了比较大的变化，亦或者之前的拆分没有考虑的很清楚，导致拆分后的服务边界变得越来越混乱，这时就要<strong>重新梳理领域边界，不断纠正拆分的合理性</strong>。</li><li><strong>自动化驱动</strong>：部署和运维的成本会随着服务的增多呈指数级增长，每个服务都需要部署、监控、日志分析等运维工作，成本会显著提升。因此，在服务划分之前，应该首先构建自动化的工具及环境。开发人员应该以自动化为驱动力，简化服务在创建、开发、测试、部署、运维上的重复性工作，通过工具实现更可靠的操作，避免微服务数量增多带来的开发、管理复杂度问题。</li></ul><h1>微服务拆分策略</h1><h2 id="功能维度拆分策略">功能维度拆分策略</h2><p>大的原则是基于业务复杂度拆分服务：<strong>业务复杂度足够高，应该基于领域驱动拆分服务。业务复杂度较低，选择基于数据驱动拆分服务</strong></p><ul><li><strong>基于数据驱动</strong>拆分服务：自下而上的架构设计方法，通过分析需求，确定整体数据结构，<strong>根据表之间的关系拆分服务</strong>。<ul><li>拆分步骤：需求分析，抽象数据结构，划分服务，确定调用关系和业务流程验证。</li></ul></li><li><strong>基于领域驱动</strong>拆分服务：自上而下的架构设计方法，通过和领域专家建立统一的语言，不断交流，<strong>确定关键业务场景，逐步确定边界上下文</strong>。领域驱动更强调业务实现效果，认为自下而上的设计可能会导致技术人员不能更好地理解业务方向，进而偏离业务目标。<ul><li>拆分步骤：通过模型和领域专家建立统一语言，业务分析，寻找聚合，确定服务调用关系，业务流程验证和持续优化。</li></ul></li><li><strong>从已有单体架构中逐步拆分服务</strong><ul><li>拆分步骤：前后端分离，提取公共基础服务（如授权服务，分布式ID服务），不断从老系统抽取服务，垂直划分优先，适当水平切分</li></ul></li></ul><p>以上几种拆分方式不是多选一，而是可以<strong>根据实际情况自由排列组合</strong>。<br><strong>拆分不仅仅是架构上的调整，也意味着要在组织结构上做出相应的适应性优化，以确保拆分后的服务由相对独立的团队负责维护。</strong></p><h2 id="非功能维度拆分策略">非功能维度拆分策略</h2><p>主要考虑六点：<strong>扩展性、复用性、高性能、高可用、安全性、异构性</strong></p><ul><li><strong>扩展性</strong>：<strong>区分系统中变与不变的部分</strong>，不变的部分一般是成熟的、通用的服务功能，变的部分一般是改动比较多、满足业务迭代扩展性需要的功能，我们可以将不变的部分拆分出来，作为共用的服务，将变的部分独立出来满足个性化扩展需要同时根据二八原则，系统中经常变动的部分大约只占&nbsp;20%，而剩下的&nbsp;80%&nbsp;基本不变或极少变化，这样的拆分也解决了发布频率过多而影响成熟服务稳定性的问题。</li><li><strong>复用性</strong>：不同的业务里或服务里经常会出现<strong>重复的功能</strong>，比如每个服务都有鉴权、限流、安全及日志监控等功能，可以将这些通过的功能拆分出来形成独立的服务。</li><li><strong>高性能</strong>：将性能要求高或者性能压力大的模块拆分出来，<strong>避免性能压力大的服务影响其它服务</strong>。<ul><li>我们也可以基于<strong>读写分离</strong>来拆分，比如电商的商品信息，在&nbsp;App&nbsp;端主要是商品详情有大量的读取操作，但是写入端商家中心访问量确很少。因此可以对流量较大或较为核心的服务做读写分离，拆分为两个服务发布，一个负责读，另外一个负责写。</li><li><strong>数据一致性</strong>是另一个基于性能维度拆分需要考虑的点，<strong>对于强一致的数据，属于强耦合，尽量放在同一个服务中</strong>（但是有时会因为各种原因需要进行拆分，那就需要有相应的机制进行保证），弱一致性通常可以拆分为不同的服务。</li></ul></li><li><strong>高可用</strong>：将可靠性要求高的核心服务和可靠性要求低的非核心服务拆分开来，然后<strong>重点保证核心服务的高可用</strong>。具体拆分的时候，核心服务可以是一个也可以是多个，只要最终的服务数量满足“三个火枪手”的原则就可以。</li><li><strong>安全性</strong>：不同的服务可能对信息安全有不同的要求，因此把需要<strong>高度安全的服务拆分出来</strong>，进行区别部署，比如设置特定的&nbsp;DMZ&nbsp;区域对服务进行分区部署，可以更有<strong>针对性地满足信息安全的要求</strong>，也可以降低对防火墙等安全设备吞吐量、并发性等方面的要求，降低成本，提高效率。</li><li><strong>异构性</strong>：对于对开发语言种类有要求的业务场景，可以用<strong>不同的语言</strong>将其功能独立出来实现一个独立服务。</li></ul><h1>拆分注意的风险</h1><ul><li>不打无准备之仗：开发团队是否具备足够的经验，<strong>能否驾驭微服务的技术栈</strong>，可能是第一个需要考虑的点。</li><li><strong>不断纠正</strong>：我们需要承认我们的认知是有限的，只能基于目前的业务状态和有限的对未来的预测来制定出一个相对合适的拆分方案，而不是所谓的最优方案，任何方案都只能保证在当下提供了相对合适的粒度和划分原则，要时刻做好在未来的末一个时刻会变得不和时宜、需要再次调整的准备。</li><li>要做行动派，而不是理论派：在具体怎么拆分上，也不要太纠结于是否合适，<strong>如果拆了之后发现真的不合适，在重新调整就好了</strong>。如果要灵活调整，可以针对服务化架构搭建起一套完整的能力体系，比如服务治理平台、数据迁移工具、数据双写等等</li><li>服务只拆不合：<ul><li><strong>拆相当于我们开发代码，合相当于重构代码</strong>。随着我们对应用程序领域的了解越来越深，它们需要随着时间的推移而变化。</li><li>人员和服务数量的不匹配，导致的<strong>维护成本</strong>增加，也是导致服务合并的一个重要原因。</li><li>如果<strong>微服务数量过多和资源不匹配</strong>，则可以考虑合并多个微服务到服务包，部署到一台服务器，这样可以节省服务运行时的基础资源消耗也降低了维护成本。需要注意的是，<strong>虽然服务包是运行在一个进程中，但是服务包内的服务依然要满足微服务定义</strong>，以便在未来某一天要重新拆开的时候可以很快就分离</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;微服务拆分时机&lt;/h1&gt;
&lt;p&gt;微服务不仅仅是技术的升级，更是开发方式、组织架构、开发观念的转变。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;业务规模&lt;/strong&gt;：业务模式得到市场的验证，需要进一步加快脚步快速占领市场，这时业务的规模变得越来越大，按产品生命周期来划</summary>
      
    
    
    
    <category term="电商系统" scheme="https://jxch.github.io/categories/%E7%94%B5%E5%95%86%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="微服务拆分" scheme="https://jxch.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%8B%86%E5%88%86/"/>
    
  </entry>
  
  <entry>
    <title>02.多数据源架构</title>
    <link href="https://jxch.github.io/2023/07/08/architect/dian-shang-xi-tong/02-duo-shu-ju-yuan-jia-gou/"/>
    <id>https://jxch.github.io/2023/07/08/architect/dian-shang-xi-tong/02-duo-shu-ju-yuan-jia-gou/</id>
    <published>2023-07-08T03:55:35.000Z</published>
    <updated>2023-07-08T04:20:16.595Z</updated>
    
    <content type="html"><![CDATA[<h1>01. AbstractRoutingDataSource（Spring）</h1><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token annotation punctuation">@Component</span><span class="token annotation punctuation">@Primary</span> <span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">DynamicDataSource</span> <span class="token keyword">extends</span> <span class="token class-name">AbstractRoutingDataSource</span> <span class="token punctuation">{</span>    <span class="token comment">// 更改数据源时切换 ThreadLocal 即可</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token class-name">ThreadLocal</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">&gt;</span></span> name <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ThreadLocal</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token annotation punctuation">@Autowired</span>    <span class="token class-name">DataSource</span> dataSource1<span class="token punctuation">;</span>    <span class="token annotation punctuation">@Autowired</span>    <span class="token class-name">DataSource</span> dataSource2<span class="token punctuation">;</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">protected</span> <span class="token class-name">Object</span> <span class="token function">determineCurrentLookupKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> name<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">afterPropertiesSet</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token comment">// 为targetDataSources初始化所有数据源</span>        <span class="token class-name">Map</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Object</span><span class="token punctuation">,</span> <span class="token class-name">Object</span><span class="token punctuation">&gt;</span></span> targetDataSources<span class="token operator">=</span><span class="token keyword">new</span> <span class="token class-name">HashMap</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">&gt;</span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        targetDataSources<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">"W"</span><span class="token punctuation">,</span>dataSource1<span class="token punctuation">)</span><span class="token punctuation">;</span>        targetDataSources<span class="token punctuation">.</span><span class="token function">put</span><span class="token punctuation">(</span><span class="token string">"R"</span><span class="token punctuation">,</span>dataSource2<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">super</span><span class="token punctuation">.</span><span class="token function">setTargetDataSources</span><span class="token punctuation">(</span>targetDataSources<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment">// 为defaultTargetDataSource 设置默认的数据源</span>        <span class="token keyword">super</span><span class="token punctuation">.</span><span class="token function">setDefaultTargetDataSource</span><span class="token punctuation">(</span>dataSource1<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">super</span><span class="token punctuation">.</span><span class="token function">afterPropertiesSet</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1>02. SqlSessionFactory（MyBatis）</h1><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token annotation punctuation">@Configuration</span><span class="token comment">// 使用不同数据源的接口放到不同的 mapper 包下即可</span><span class="token annotation punctuation">@MapperScan</span><span class="token punctuation">(</span>basePackages <span class="token operator">=</span> <span class="token string">"com.tuling.datasource.dynamic.mybatis.mapper.r"</span><span class="token punctuation">,</span> sqlSessionFactoryRef<span class="token operator">=</span><span class="token string">"rSqlSessionFactory"</span><span class="token punctuation">)</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">RMyBatisConfig</span> <span class="token punctuation">{</span>    <span class="token annotation punctuation">@Bean</span>    <span class="token annotation punctuation">@ConfigurationProperties</span><span class="token punctuation">(</span>prefix <span class="token operator">=</span> <span class="token string">"spring.datasource.datasource2"</span><span class="token punctuation">)</span>    <span class="token keyword">public</span> <span class="token class-name">DataSource</span> <span class="token function">dataSource2</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> <span class="token class-name">DruidDataSourceBuilder</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token annotation punctuation">@Bean</span>    <span class="token annotation punctuation">@Primary</span>    <span class="token keyword">public</span> <span class="token class-name">SqlSessionFactory</span> <span class="token function">rSqlSessionFactory</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">throws</span> <span class="token class-name">Exception</span> <span class="token punctuation">{</span>        <span class="token keyword">final</span> <span class="token class-name">SqlSessionFactoryBean</span> sessionFactory <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">SqlSessionFactoryBean</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        sessionFactory<span class="token punctuation">.</span><span class="token function">setDataSource</span><span class="token punctuation">(</span><span class="token function">dataSource2</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token comment">// 指定主库对应的mapper.xml文件</span>        sessionFactory<span class="token punctuation">.</span><span class="token function">setMapperLocations</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">PathMatchingResourcePatternResolver</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">getResources</span><span class="token punctuation">(</span><span class="token string">"classpath:mapper/r/*.xml"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> sessionFactory<span class="token punctuation">.</span><span class="token function">getObject</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token annotation punctuation">@Bean</span>    <span class="token keyword">public</span> <span class="token class-name">DataSourceTransactionManager</span> <span class="token function">rTransactionManager</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token class-name">DataSourceTransactionManager</span> dataSourceTransactionManager <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">DataSourceTransactionManager</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        dataSourceTransactionManager<span class="token punctuation">.</span><span class="token function">setDataSource</span><span class="token punctuation">(</span><span class="token function">dataSource2</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">return</span> dataSourceTransactionManager<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token annotation punctuation">@Bean</span>    <span class="token keyword">public</span> <span class="token class-name">TransactionTemplate</span> <span class="token function">rTransactionTemplate</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>        <span class="token keyword">return</span> <span class="token keyword">new</span> <span class="token class-name">TransactionTemplate</span><span class="token punctuation">(</span><span class="token function">rTransactionManager</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1>03. dynamic-datasource</h1><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>com.baomidou<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>dynamic-datasource-spring-boot-starter<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>3.5.0<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">spring</span><span class="token punctuation">:</span>  <span class="token key atrule">datasource</span><span class="token punctuation">:</span>    dynamic<span class="token punctuation">:</span>      <span class="token comment"># 设置默认的数据源或者数据源组,默认值即为master</span>      primary<span class="token punctuation">:</span> master      <span class="token comment"># 严格匹配数据源,默认false. true未匹配到指定数据源时抛异常,false使用默认数据源</span>      strict<span class="token punctuation">:</span> <span class="token boolean important">false</span>      datasource<span class="token punctuation">:</span>        master<span class="token punctuation">:</span>          url<span class="token punctuation">:</span> jdbc<span class="token punctuation">:</span>mysql<span class="token punctuation">:</span>//127.0.0.1<span class="token punctuation">:</span>3306/datasource1<span class="token punctuation">?</span>serverTimezone=UTC<span class="token important">&amp;useUnicode=true&amp;characterEncoding=UTF8&amp;useSSL=false</span>          username<span class="token punctuation">:</span> root          password<span class="token punctuation">:</span> <span class="token number">123456</span>          initial<span class="token punctuation">-</span><span class="token key atrule">size</span><span class="token punctuation">:</span> <span class="token number">1</span>          min<span class="token punctuation">-</span><span class="token key atrule">idle</span><span class="token punctuation">:</span> <span class="token number">1</span>          max<span class="token punctuation">-</span><span class="token key atrule">active</span><span class="token punctuation">:</span> <span class="token number">20</span>          test<span class="token punctuation">-</span><span class="token key atrule">on-borrow</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>          driver<span class="token punctuation">-</span><span class="token key atrule">class-name</span><span class="token punctuation">:</span> com.mysql.cj.jdbc.Driver        slave_1<span class="token punctuation">:</span>          url<span class="token punctuation">:</span> jdbc<span class="token punctuation">:</span>mysql<span class="token punctuation">:</span>//127.0.0.1<span class="token punctuation">:</span>3306/datasource2<span class="token punctuation">?</span>serverTimezone=UTC<span class="token important">&amp;useUnicode=true&amp;characterEncoding=UTF8&amp;useSSL=false</span>          username<span class="token punctuation">:</span> root          password<span class="token punctuation">:</span> <span class="token number">123456</span>          initial<span class="token punctuation">-</span><span class="token key atrule">size</span><span class="token punctuation">:</span> <span class="token number">1</span>          min<span class="token punctuation">-</span><span class="token key atrule">idle</span><span class="token punctuation">:</span> <span class="token number">1</span>          max<span class="token punctuation">-</span><span class="token key atrule">active</span><span class="token punctuation">:</span> <span class="token number">20</span>          test<span class="token punctuation">-</span><span class="token key atrule">on-borrow</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>          driver<span class="token punctuation">-</span><span class="token key atrule">class-name</span><span class="token punctuation">:</span> com.mysql.cj.jdbc.Driver<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token annotation punctuation">@DS</span><span class="token punctuation">(</span><span class="token string">"master"</span><span class="token punctuation">)</span><span class="token annotation punctuation">@DSTransactional</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;01. AbstractRoutingDataSource（Spring）&lt;/h1&gt;
&lt;pre class=&quot;line-numbers language-java&quot; data-language=&quot;java&quot;&gt;&lt;code class=&quot;language-java&quot;&gt;&lt;spa</summary>
      
    
    
    
    <category term="电商系统" scheme="https://jxch.github.io/categories/%E7%94%B5%E5%95%86%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="数据源" scheme="https://jxch.github.io/tags/%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
    
  </entry>
  
  <entry>
    <title>01.单体VS微服务</title>
    <link href="https://jxch.github.io/2023/07/08/architect/dian-shang-xi-tong/01-dan-ti-vs-wei-fu-wu/"/>
    <id>https://jxch.github.io/2023/07/08/architect/dian-shang-xi-tong/01-dan-ti-vs-wei-fu-wu/</id>
    <published>2023-07-08T03:22:57.000Z</published>
    <updated>2023-07-08T03:51:47.779Z</updated>
    
    <content type="html"><![CDATA[<h1>单体服务的优点</h1><ul><li><strong>应用的开发很简单</strong>，IDE 和其他开发工具只需要构建这一个单独的应用程序</li><li><strong>易于对应用程序进行大规模的更改</strong>:可以更改代码和数据库模式，然后构建和部署测试相对简单。</li><li><strong>开发测试简单</strong>:开发者只需要写几个端到端的测试,启动应用程序，调用 RESTAPI，然后使用 Selenium 这样的工具测试用户界面。</li><li><strong>部署简单</strong>明了:开发者唯一需要做的,就是把 WAR 文件复制到安装了 Tomcat 的服务器上。</li><li><strong>横向扩展简单</strong>:运行多个实例，由一个负载均衡器进行调度。</li></ul><h1>单体服务的缺点</h1><p>单体架构存在着巨大的局限性。<strong>随着业务的复杂和用户的增多</strong>，小巧的、简单的、由一个小团队开发维护的应用程序就会演变成一个由大团队开发的巨无霸单体应用程序。此时应用就变成了单体地狱。开发变得缓慢和痛苦。</p><h2 id="font-color-red-过度的复杂性-font"><font color="red">过度的复杂性</font></h2><p>大型单体应用程序的首要问题就是它的过度复杂性，系统本身过于庞大和复杂导致任何一个开发者都很难理解它的全部。因此，修复软件中的问题和正确地实现新功能就变得困难且耗时。各种交付截止时间都可能被错过。这种极度的复杂性正在形成一个恶性循环:由于代码库太难于理解，因此开发人员在更改时更容易出错，每一次更改都会让代码库变得更复杂、更难懂。就演变成为了我们常说的“<strong>代码屎山</strong>”。</p><h2 id="font-color-red-开发速度缓慢-font"><font color="red">开发速度缓慢</font></h2><p>巨大的项目<strong>从编辑到构建、运行再到测试这个周期花费的时间越来越长</strong>,这严重地影响了团队的工作效率。</p><h2 id="font-color-red-部署周期长-易出问题-font"><font color="red">部署周期长,易出问题</font></h2><p>巨大的项目从代码完成到运行在生产环境是一个漫长且费力的过程。一个问题是，<font color="red"><strong>众多开发人员都向同一个代码库提交代码更改，这常常使得这个代码库的构建结果处于无法交付的状态。采用功能分支来解决这个问题时，带来的是漫长且痛苦的合并过程。</strong></font>紧接着，一旦团队完成一个冲刺任务，随后迎接他们的将是一个漫长的测试和代码稳定周期。把更改推向生产环境的另一个挑战是运行测试需要很长时间。因为代码库如此复杂,以至于一个更改可能引起的影响是未知的，为了避免<font color="red"><strong>牵一发而动全身</strong></font>的后果，即使是一个微小的更改，开发人员也必须在持续集成服务器上运行所有的测试套件。系统的某些部分甚至还需要手工测试。如果测试失败，诊断和修复也需要更多的时间。因此，<font color="red"><strong>完成这样的测试往往需要数天甚至更长时间</strong></font>。</p><h2 id="font-color-red-难以扩展和可靠性不佳-font"><font color="red">难以扩展和可靠性不佳</font></h2><p>在很多的时候，<font color="red"><strong>应用的不同功能和模块对资源的需求是相互冲突的</strong></font>。例如，有些模块需要将数据保存在一个大型的内存数据库中，理想情况下运行这个应用的服务器应该有较大容量的内存；另外，有些模块存在大量的计算又需要比较快的CPU，这需要项目部署在具有多个高性能 CPU 和大内存的服务器之上。因为这些模块都是在一个应用程序内，因此在选用服务器时必须满足所有模块的需要。<font color="red"><strong>应用程序缺乏故障隔离</strong></font>，因为所有模块都在同一个进程中运行。每隔一段时间，在一个模块中的代码错误，例如内存泄漏，将会导致应用程序的所有实例都崩溃。</p><h1>单体应用的适用场景</h1><p><font color="green"><strong>小、少、短、快、雏</strong></font><br>公司规模较小，开发团队人数较少、产品上线周期短、产品在快速迭代期，核心功能尚未稳定时；或者用户规模和用户群体较少时。</p><h1>微服务架构的特点</h1><p>今天,针对大型复杂应用的开发,越来越多的共识趋向于考虑使用微服务架构。但微服务到底是什么？针对微服务架构有多种定义。有些仅仅是在字面意义上做了定义:服务应该是微小的不超过 100 行代码,等等。另外有些定义要求服务的开发周期必须被限制在两周之内。<br>曾在 Netflix 工作的著名架构师Adrian Cockcroft 把<strong>微服务架构定义为面向服务的架构，它们由松耦合和具有边界上下文的元素组成</strong>。由这个定义可以看到，微服务其实<strong>和 DDD 是天生一对</strong>。</p><h2 id="微服务是模块化的">微服务是模块化的</h2><p>模块化是开发大型、复杂应用程序的基础。现代互联网应用程序为了让不同的人开发和理解，大型应用需要拆分为模块。在单体应用中，模块通常由一组编程语言所提供的结构（例如Java 的包）,或者Java JAR 文件这样的构建制品来定义。但是，即使这样，随着时间的推移和反复的开发迭代,单体应用依然会变成我们前面所说的单体地狱。<br>微服务架构使用服务作为模块化的单元。服务的 API 为它自身构筑了一个不可逾越的边界,你无法越过 API 去访问服务内部的类，这与采用 Java 包的单体应用完全不同。因此模块化的服务更容易随着时间推移而不断演化。微服务架构也带来其他的好处,例如服务可以独立进行部署和扩展。</p><h2 id="每个服务都拥有自己的数据库">每个服务都拥有自己的数据库</h2><p>微服务架构的一个关键特性是每一个服务之间都是<strong>松耦合</strong>的，它们仅通过 API 进行通信。实现这种松耦合的方式之一，是每个服务都拥有自己的私有数据库。对于我们的项目来说，订单服务拥有一个包括 oms_order 等表的数据库，用户服务拥有一个包含 ums_member 等表的数据库。在开发阶段就可以修改自己服务的数据库模式，而不必同其他服务的开发者协调。在运行时，服务实现了相互之间的独立。服务不会因为其他的服务锁住了数据库而进入堵塞的状态。</p><h1>微服务架构的好处</h1><ul><li>使大型的复杂应用程序可以<strong>持续交付和持续部署</strong>。</li><li>每个服务都相对较小并<strong>容易维护</strong>。</li><li>服务可以<strong>独立部署</strong>。</li><li>服务可以<strong>独立扩展</strong>。</li><li>微服务架构可以实现团队的自主和松散耦合。</li><li>更容易实验和采纳新的技术。</li><li>更好的<strong>容错性</strong>，比如更好的故障隔离。</li></ul><h1>微服务架构的弊端</h1><h2 id="font-color-red-服务的拆分和定义是一项挑战-font"><font color="red">服务的拆分和定义是一项挑战</font></h2><p>采用微服务架构首当其冲的问题，就是根本没有一个具体的、良好定义的算法可以完成服务的拆分工作。与软件开发一样，服务的拆分和定义更像是一门艺术。更糟糕的是，如果对系统的服务拆分出现了偏差，你很有可能会构建出一个<font color="red"><strong>分布式的单体应用</strong></font>: <strong>一个包含了一大堆互相之间紧耦合的服务，却又必须部署在一起的所谓分布式系统。<strong>这将会把单体架构和微服务架构</strong>两者的弊端集于一身</strong>。</p><h2 id="font-color-red-分布式系统带来的各种复杂性-font-使开发、测试和部署变得更困难"><font color="red">分布式系统带来的各种复杂性</font>,使开发、测试和部署变得更困难</h2><p>使用微服务架构的另一个问题是开发人员必须处理创建分布式系统的额外复杂性。服务必须使用进程间通信机制。这比简单的方法调用更复杂。此外，<font color="red"><strong>必须设计服务来处理局部故障,并处理远程服务不可用或出现高延迟的各种情况。</strong></font>实现跨多个服务的用例需要使用不熟悉的技术。每个服务都有自己的数据库,这使得实现<font color="red"><strong>跨服务的事务和查询</strong></font>成为一项挑战。基于微服务的应用程序必须使用所谓的 Saga 来维护服务之间的<font color="red"><strong>数据一致性</strong></font>。基于微服务的应用程序**无法使用简单查询从多个服务中检索数据。**相反，它必须使用 API 组合或 CQRS 视图实现查询。</p><p>IDE 等开发工具都是为单体应用设计的，它们并不具备开发分布式应用所需要的特定功能支持。编写<strong>包含多项服务在内的自动化测试</strong>也是很令人头疼的工作。这些都是跟微服务架构直接相关的问题。因此,团队中的开发人员必须具备先进的软件开发和交付技能才能成功使用微服务。</p><p>微服务架构还引入了显著的<strong>运维复杂性</strong>。必须在生产环境中管理更多活动组件:不同类型服务的多个实例。要成功部署微服务，需要高度自动化的基础设施,比如自动化部署等等。</p><h2 id="font-color-red-当部署跨越多个服务的功能时需要谨慎地协调-font"><font color="red">当部署跨越多个服务的功能时需要谨慎地协调</font></h2><p>使用微服务架构的另外一项挑战在于当部署跨越多个服务的功能时需要谨慎地协调更多开发团队。必须制定一个发布计划，<font color="red"><strong>把服务按照依赖关系进行排序</strong></font>，这就是我们常说的服务编排。这跟单体架构下批量部署多个组件的方式截然不同。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;单体服务的优点&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;应用的开发很简单&lt;/strong&gt;，IDE 和其他开发工具只需要构建这一个单独的应用程序&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;易于对应用程序进行大规模的更改&lt;/strong&gt;:可以更改代码和数据库模式，然后构建和部</summary>
      
    
    
    
    <category term="电商系统" scheme="https://jxch.github.io/categories/%E7%94%B5%E5%95%86%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="微服务" scheme="https://jxch.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
  </entry>
  
  <entry>
    <title>26.03.缩梯</title>
    <link href="https://jxch.github.io/2023/07/08/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/26.lou-ti-xing-tai-kuan-fu-qu-shi-tong-dao/26-03-suo-ti/"/>
    <id>https://jxch.github.io/2023/07/08/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/26.lou-ti-xing-tai-kuan-fu-qu-shi-tong-dao/26-03-suo-ti/</id>
    <published>2023-07-07T17:18:39.000Z</published>
    <updated>2023-07-07T17:31:54.438Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc26%2FSlide3.JPG" alt="图 26.3 缩梯"></p><blockquote><p><font color="red"><strong>市场每一次突破幅度都小于上一次,说明动能在减弱,可能很快出现较深的回撤甚至反转。</strong></font>如图26.3所示,<strong>市场走出楼梯形态上升趋势,形成3个或以上趋势性上移的高点和低点,大致处于一个通道之中。</strong><font color="green"><strong>K线4、6、8形成缩梯,代表上升动能减弱,预示行情可能反转。</strong></font><font color="black"><strong>上升通道相当于一个大的熊旗,在K线9向下突破。</strong></font><br><font color="green"><strong>在K线9突破之后,市场出现突破回踩</strong></font>,<font color="black"><strong>并在K线10形成高点下降,进入一段楼梯形态的下降趋势。</strong></font><font color="green">K线10与截至K线9下跌走势中第一次反弹的高点大致构成一个双顶熊旗。K线11对下降通道下轨构成过靶,引发一轮两段式的小规模反转并向上超越通道上轨。</font><br><font color="green"><strong>当市场开始形成楼梯形下跌趋势,通常你可以在每一根强趋势K线向下突破时逆势刮头皮。在每一根空头趋势K线收盘跌破前一个摆动低点时刮头皮做多。</strong></font><font color="green">同样在楼梯形上涨行情中,你可以在每一根趋势K线超越上一个摆动高点时刮头皮做空。</font>不过一般来讲,<font color="green">**更安全的做法是在市场反转时挂止损单入场。**比如说,如果市场从通道底部向上反转,可以在前一根K线高点上方挂单入场。</font></p></blockquote><hr><p>详见：<br>《高级趋势技术分析 价格行为交易系统之趋势分析》<br>第四篇 常见趋势形态<br>第26章 楼梯形态：宽幅趋势通道<br>第3个案例（图 26.3）<br>第378-379页</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc26%2FSlide3.JPG&quot; alt=</summary>
      
    
    
    
    <category term="案例-趋势分析-常见趋势形态" scheme="https://jxch.github.io/categories/%E6%A1%88%E4%BE%8B-%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90-%E5%B8%B8%E8%A7%81%E8%B6%8B%E5%8A%BF%E5%BD%A2%E6%80%81/"/>
    
    
    <category term="案例-楼梯形态" scheme="https://jxch.github.io/tags/%E6%A1%88%E4%BE%8B-%E6%A5%BC%E6%A2%AF%E5%BD%A2%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>26.02.楼梯形态加速进入强劲趋势</title>
    <link href="https://jxch.github.io/2023/07/08/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/26.lou-ti-xing-tai-kuan-fu-qu-shi-tong-dao/26-02-lou-ti-xing-tai-jia-su-jin-ru-qiang-jing-qu-shi/"/>
    <id>https://jxch.github.io/2023/07/08/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/26.lou-ti-xing-tai-kuan-fu-qu-shi-tong-dao/26-02-lou-ti-xing-tai-jia-su-jin-ru-qiang-jing-qu-shi/</id>
    <published>2023-07-07T17:12:33.000Z</published>
    <updated>2023-07-07T17:17:52.480Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc26%2FSlide2.JPG" alt="图 26.2 楼梯形态加速进入强劲趋势"></p><blockquote><p>如图26.2所示,<font color="red"><strong>楼梯形态有时候会加速,进入更强劲的趋势。</strong></font>在欧元兑美元的日线图上,<strong>截至K线7,市场处于一个上升通道,形成3个高点抬升和低点抬升,因此属于楼梯形态上升趋势。</strong><br><font color="black"><strong>K线8是一根多头趋势K线,突破通道上轨。随后出现一根空头反转K线,但没有被触发。</strong></font><font color="red"><strong>突破走势应该可以持续到将通道等距上移的平行线位置(安德鲁叉运动)</strong></font>,结果的确如此。<font color="orange"><strong>在楔形顶部失败的情况下,这种加速上涨是很常见的。</strong></font><strong>截至K线6有3波向上推动</strong>,<font color="black"><strong>不过如果你从截至K线4的那波强劲拉升算起,也可以将K线8前面那波小幅向上推动看成第三波。</strong></font><font color="red"><strong>楔形失败之后往往会出现大约与楔形高度等距的行情</strong></font>(从K线6高点到K线3或K线1低点)。</p></blockquote><hr><p>详见：<br>《高级趋势技术分析 价格行为交易系统之趋势分析》<br>第四篇 常见趋势形态<br>第26章 楼梯形态：宽幅趋势通道<br>第2个案例（图 26.2）<br>第377-378页</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc26%2FSlide2.JPG&quot; alt=</summary>
      
    
    
    
    <category term="案例-趋势分析-常见趋势形态" scheme="https://jxch.github.io/categories/%E6%A1%88%E4%BE%8B-%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90-%E5%B8%B8%E8%A7%81%E8%B6%8B%E5%8A%BF%E5%BD%A2%E6%80%81/"/>
    
    
    <category term="案例-楼梯形态" scheme="https://jxch.github.io/tags/%E6%A1%88%E4%BE%8B-%E6%A5%BC%E6%A2%AF%E5%BD%A2%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>26.01.下降楼梯</title>
    <link href="https://jxch.github.io/2023/07/08/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/26.lou-ti-xing-tai-kuan-fu-qu-shi-tong-dao/26-01-xia-jiang-lou-ti/"/>
    <id>https://jxch.github.io/2023/07/08/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/26.lou-ti-xing-tai-kuan-fu-qu-shi-tong-dao/26-01-xia-jiang-lou-ti/</id>
    <published>2023-07-07T16:53:31.000Z</published>
    <updated>2023-07-07T17:11:31.669Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc26%2FSlide1.JPG" alt="图 26.1 下降楼梯"></p><blockquote><p><font color="red"><strong>下降楼梯形态是一个向下倾斜的通道,每次突破至新低之后都会回撤至突破点之上。</strong></font>比如说,在图26.1中,<font color="black"><strong>从K线8到K线9的下跌形成向下突破,随后的回撤站上了K线7的低点;截至K线11的下跌突破了K线9,随后的回撤站上K线9突破点,与上一个区间形成重叠。</strong></font><br><font color="green"><strong>部分交易者会在趋势通道线附近做多,在趋势线附近做空。其他交易者则关注回撤之前突破走势的幅度。</strong></font>比如说,<font color="black"><strong>K线5低点大约低于K线3低点4个点。激进多头将会在K线5低点下方3~4个点挂限价单买入。他们的买单将会在K线7获得成交。一旦市场跌破K线7,他们又在K线7低点下方3~4个点挂买单,将会在截至K线9的下跌中获得成交(K线9低点低于K线7低点4个点)由于前面的反弹也是大约4个点,他们将会在入场点上方大约3个点的位置锁定利润。对于截至K线11和16的下跌,他们也如法炮制。在截至K线13的下跌过程中他们也进行了同样的挂单,但由于跌幅太小,并未获得成交。空头的做法刚好相反。</strong></font><strong>他们看到前面的反弹为4~6个点,因此他们在最近摆动低点上方3~5个点的位置分批建立空单。</strong><font color="green"><strong>这种操作手法只适合经验丰富的交易者。初学者应该只采取挂止损单的方法入场,因为这样他们入场时市场已经朝他们的方向运行</strong></font>(将在第二本书讨论)。<br><font color="black"><strong>K线7是第三波向下推动,属于缩梯形态</strong></font>(K线7跌破K线5的幅度小于K线5跌破K线3的幅度)。图中的通道线是大致画的,目的让大家看到市场处于下跌通道之中。<strong>在通道内,双向交易迹象明显,交易者在发现恰当建仓形态时可低买高卖。</strong></p><p>本图的深入探讨</p><p>在图26.1中,<font color="black"><strong>当天市场开盘处于前一天开始的下降通道下轨附近。开盘的向下突破失败,市场出现双K线反转并引发连续4根K线的迅速拉升。市场测试下降趋势线之后形成双顶,然后急速下跌至K线13。</strong></font><font color="orange"><strong>急速上涨之后又急速下跌,说明多空双方在激烈争夺对通道方向的控制权。</strong></font><font color="black"><strong>多头启动了一个通道,但在趋势线处失败,反转进入下跌通道。</strong></font><font color="green"><strong>市场在K线16测试趋势通道线之后向上反转,但K线16低点未能触到通道线,意味着多头非常激进。K线16是一个双K线反转,同时也是一个末端旗形做多形态(K线15构成一个4根K线的末端旗形)。</strong></font><br><font color="red"><strong>三连推并不能保证趋势一定反转。</strong></font><font color="black"><strong>截至K线7的下跌过程中并未表现出多少买压,没有一根长多头趋势K线,也没有高潮性反转。从K线7开始的上涨也不是特别强劲。这并不是强势反转应有的特征,因此并未吸引足够的多头参与并带来反转。</strong></font>相反,<font color="green"><strong>市场形成了一个楔形熊旗(K线6以及随后从K线7开始的两波向上推动共同构成三连推)和高点下降(虽然反弹站上了K线6,表现出一定力度,但仍低于K线4),然后下跌趋势恢复。</strong></font></p></blockquote><hr><p>详见：<br>《高级趋势技术分析 价格行为交易系统之趋势分析》<br>第四篇 常见趋势形态<br>第26章 楼梯形态：宽幅趋势通道<br>第1个案例（图 26.1）<br>第376-377页</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc26%2FSlide1.JPG&quot; alt=</summary>
      
    
    
    
    <category term="案例-趋势分析-常见趋势形态" scheme="https://jxch.github.io/categories/%E6%A1%88%E4%BE%8B-%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90-%E5%B8%B8%E8%A7%81%E8%B6%8B%E5%8A%BF%E5%BD%A2%E6%80%81/"/>
    
    
    <category term="案例-楼梯形态" scheme="https://jxch.github.io/tags/%E6%A1%88%E4%BE%8B-%E6%A5%BC%E6%A2%AF%E5%BD%A2%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>26.00.02.楼梯形态交易日</title>
    <link href="https://jxch.github.io/2023/07/07/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/26.lou-ti-xing-tai-kuan-fu-qu-shi-tong-dao/26-00-02-lou-ti-xing-tai-jiao-yi-ri/"/>
    <id>https://jxch.github.io/2023/07/07/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/26.lou-ti-xing-tai-kuan-fu-qu-shi-tong-dao/26-00-02-lou-ti-xing-tai-jiao-yi-ri/</id>
    <published>2023-07-07T08:22:55.000Z</published>
    <updated>2023-07-07T16:52:03.620Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><font color="red"><strong>当市场走出3波或以上趋势性摆动,构成一个略微倾斜的交易区间或者通道,多头和空头都表现活跃,但一方略微掌握更大的控制权。每次回撤都会超越突破点,使得每一突破走势与随后的回撤之间存在重叠。</strong></font><strong>在宽幅通道内,双向交易正在发生,因此交易者可以寻找两个方向的入场机会。</strong><font color="red"><strong>如果突破幅度越来越小,就属于缩梯形态,意味着动能减弱。它通常会导致两段式反转走势和趋势线突破。</strong></font><font color="orange"><strong>许多三连推反转都属于失败并反转的楼梯形态或缩梯形态。</strong></font><font color="black"><strong>在较高时间级别上,楼梯形态往往只是回调或旗形。</strong></font><font color="red"><strong>楼梯形态经常发生在交易日最后1~2个小时,然后在第二天开盘突破这个旗形。</strong></font><font color="black"><strong>比如说,某天走出的宽幅上升通道可能只是一个大的熊旗,下跌趋势可能在第二天向下突破。</strong></font><br><font color="red"><strong>有时候,楼梯形态可能突然加速,以顺势方向突破趋势通道。如果随后立即反转,那么这一过靶与反转走势有可能造成至少两段式调整。如果没有反转,突破走势可能至少再持续两波行情,或发生大致相当于通道高度的等距运动(价格超越通道的距离应该大约相当于通道之内的距离)。</strong></font><br><font color="green"><strong>交易者将会密切关注突破走势超越最近摆动点的幅度,然后利用这一幅度来对后面的突破走势进行逆势交易,期待市场出现突破回踩。</strong></font>比如说,<font color="black"><strong>如果最近的摆动低点低于上一个摆动低点14个最小报价单位,交易者将会从最近摆动低点下方10单位左右开始分批买入(往往在趋势通道线附近)。如果最近一次突破之后的回撤幅度约为15单位,他们将会在低点上方大约10-15单位寻求锁定利润(往往在趋势线附近)。</strong></font></p></blockquote><hr><p>详见：<br>《高级趋势技术分析 价格行为交易系统之趋势分析》<br>第四篇 常见趋势形态<br>第26章 楼梯形态：宽幅趋势通道<br>第375-376页</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;font color=&quot;red&quot;&gt;&lt;strong&gt;当市场走出3波或以上趋势性摆动,构成一个略微倾斜的交易区间或者通道,多头和空头都表现活跃,但一方略微掌握更大的控制权。每次回撤都会超越突破点,使得每一突破走势与随后的回撤之间存在重叠。&lt;/stro</summary>
      
    
    
    
    <category term="趋势分析-常见趋势形态" scheme="https://jxch.github.io/categories/%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90-%E5%B8%B8%E8%A7%81%E8%B6%8B%E5%8A%BF%E5%BD%A2%E6%80%81/"/>
    
    
    <category term="楼梯形态" scheme="https://jxch.github.io/tags/%E6%A5%BC%E6%A2%AF%E5%BD%A2%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>26.00.01.楼梯形态交易日的主要特征</title>
    <link href="https://jxch.github.io/2023/07/07/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/26.lou-ti-xing-tai-kuan-fu-qu-shi-tong-dao/26-00-01-lou-ti-xing-tai-jiao-yi-ri-de-zhu-yao-te-zheng/"/>
    <id>https://jxch.github.io/2023/07/07/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/26.lou-ti-xing-tai-kuan-fu-qu-shi-tong-dao/26-00-01-lou-ti-xing-tai-jiao-yi-ri-de-zhu-yao-te-zheng/</id>
    <published>2023-07-07T08:15:30.000Z</published>
    <updated>2023-07-07T08:22:16.921Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>楼梯形态交易日的主要特征:</p><ul><li><font color="red"><strong>楼梯形态交易日是趋势性交易区间交易日的变体,至少包含3个交易区间。</strong></font></li><li><font color="red">交易日波动剧烈,但形成趋势性高点和低点。</font></li><li><font color="red">由于波动较大,交易者往往会朝两个方向交易,<strong>但他们应该试图让部分或全部顺势头寸参与摆动。</strong></font></li><li><font color="red"><strong>几乎在所有突破之后市场都会出现超越突破点的回撤(突破测试),造成相邻摆动之间存在一些重叠。</strong></font>举例而言,<font color="orange"><strong>在宽幅下降通道中,所有突破至新低的走势都跟随一波反弹,返回突破点之上但未能超越最近的摆动高点。不过,有时候会有一两波反弹略微超过上一个摆动高点。这将让部分交易者怀疑市场是否正在反转,但通常情况下趋势会很快恢复。</strong></font></li><li><font color="red"><strong>如果每一次突破都比上一次幅度小一点,就属于缩梯形态,说明动能减弱可能引发更大的调整。</strong></font></li></ul></blockquote><hr><p>详见：<br>《高级趋势技术分析 价格行为交易系统之趋势分析》<br>第四篇 常见趋势形态<br>第26章 楼梯形态：宽幅趋势通道<br>第375页</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;楼梯形态交易日的主要特征:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;font color=&quot;red&quot;&gt;&lt;strong&gt;楼梯形态交易日是趋势性交易区间交易日的变体,至少包含3个交易区间。&lt;/strong&gt;&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;&lt;font color=</summary>
      
    
    
    
    <category term="趋势分析-常见趋势形态" scheme="https://jxch.github.io/categories/%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90-%E5%B8%B8%E8%A7%81%E8%B6%8B%E5%8A%BF%E5%BD%A2%E6%80%81/"/>
    
    
    <category term="楼梯形态" scheme="https://jxch.github.io/tags/%E6%A5%BC%E6%A2%AF%E5%BD%A2%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>25.04.趋势在几天之后恢复</title>
    <link href="https://jxch.github.io/2023/07/07/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/25.qu-shi-hui-fu-jiao-yi-ri/25-04-qu-shi-zai-ji-tian-zhi-hou-hui-fu/"/>
    <id>https://jxch.github.io/2023/07/07/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/25.qu-shi-hui-fu-jiao-yi-ri/25-04-qu-shi-zai-ji-tian-zhi-hou-hui-fu/</id>
    <published>2023-07-07T08:07:37.000Z</published>
    <updated>2023-07-07T08:13:23.287Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc25%2FSlide4.JPG" alt="图 25.4 趋势在几天之后恢复"></p><blockquote><p><font color="red"><strong>趋势恢复形态可以持续几个交易日。</strong></font>在图25.4中,<strong>市场截至K线2走出一轮强劲下跌,然后进入交易区间,持续了两个半交易日。</strong><font color="orange"><strong>交易区间可以持续很长时间,通常情况下都会朝趋势方向突破。</strong></font><font color="black"><strong>几天后,下跌趋势恢复,市场进入第二轮下跌,从K线5持续到K线14。</strong></font><br><font color="black"><strong>从K线5到K线6的下跌也跟随一个交易区间,第二波下跌结束于第二天开盘后的K线9。</strong></font><br><font color="black"><strong>从K线7到K线9的下跌同样跟随一个交易区间,持续到K线13,下跌趋势恢复发生在截至K线14的下跌行情。</strong></font><strong>因此本例中有3个交易日都属于趋势恢复形态。</strong></p></blockquote><hr><p>详见：<br>《高级趋势技术分析 价格行为交易系统之趋势分析》<br>第四篇 常见趋势形态<br>第25章 趋势恢复交易日<br>第4个案例（图 25.4）<br>第373-374页</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc25%2FSlide4.JPG&quot; alt=</summary>
      
    
    
    
    <category term="案例-趋势分析-常见趋势形态" scheme="https://jxch.github.io/categories/%E6%A1%88%E4%BE%8B-%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90-%E5%B8%B8%E8%A7%81%E8%B6%8B%E5%8A%BF%E5%BD%A2%E6%80%81/"/>
    
    
    <category term="案例-趋势恢复交易日" scheme="https://jxch.github.io/tags/%E6%A1%88%E4%BE%8B-%E8%B6%8B%E5%8A%BF%E6%81%A2%E5%A4%8D%E4%BA%A4%E6%98%93%E6%97%A5/"/>
    
  </entry>
  
  <entry>
    <title>25.03.趋势恢复</title>
    <link href="https://jxch.github.io/2023/07/07/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/25.qu-shi-hui-fu-jiao-yi-ri/25-03-qu-shi-hui-fu/"/>
    <id>https://jxch.github.io/2023/07/07/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/25.qu-shi-hui-fu-jiao-yi-ri/25-03-qu-shi-hui-fu/</id>
    <published>2023-07-07T07:58:19.000Z</published>
    <updated>2023-07-07T08:05:43.361Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc25%2FSlide3.JPG" alt="图 25.3 趋势恢复"></p><blockquote><p><font color="red"><strong>有时候,虽然最初的上涨只有几根强趋势K线,看起来似乎会造成交易区间交易日,趋势恢复行情仍可能非常强劲。</strong></font>在图25.中,<font color="black"><strong>市场对前一天低点假突破之后,从一个扩散三角形底部上涨,形成始于开盘的上升趋势。上涨持续两根K线,然后在前一天交易区间中部止步不前。</strong></font><font color="orange"><strong>任何三连推形态所造成的上涨通常至少有两波</strong></font>(本例最终也是如此)。<font color="black"><strong>K线2是一次突破回踩,引发新轮小幅上涨,但市场开始失去动能。接下来市场继续在均线上方弱势运行到K线3。</strong></font>在这个时点,我们很明显感到有什么地方不太对劲。我们知道,<font color="black"><strong>始于开盘的上升趋势是最强劲的趋势类型之一,但例中的趋势显然算不上强劲。这意味着交易者很快将对行情的属性重新作出评估</strong></font>,然后决定离场还是等待。<strong>他们可能转而判断当天可能成为交易区间交易日,甚至可能再创盘中新低。</strong><font color="green"><strong>K线2低点有可能跟随一个双底牛旗,但由于强势多头的缺席,空头有可能将市场强力打压至当天新低,K线2低点有可能失败。</strong></font><font color="green"><strong>K线4是跌破K线2低点之后的第二波小幅向下推动</strong></font>,**市场从这里展开了一轮持续到收盘的上升趋势,构成一个趋势恢复交易日(尽管较弱)。**此外,<font color="green"><strong>K线4刚好是对始于开盘趋势的信号K线高点的突破回测。</strong></font></p></blockquote><hr><p>详见：<br>《高级趋势技术分析 价格行为交易系统之趋势分析》<br>第四篇 常见趋势形态<br>第25章 趋势恢复交易日<br>第3个案例（图 25.3）<br>第372-373页</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc25%2FSlide3.JPG&quot; alt=</summary>
      
    
    
    
    <category term="案例-趋势分析-常见趋势形态" scheme="https://jxch.github.io/categories/%E6%A1%88%E4%BE%8B-%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90-%E5%B8%B8%E8%A7%81%E8%B6%8B%E5%8A%BF%E5%BD%A2%E6%80%81/"/>
    
    
    <category term="案例-趋势恢复交易日" scheme="https://jxch.github.io/tags/%E6%A1%88%E4%BE%8B-%E8%B6%8B%E5%8A%BF%E6%81%A2%E5%A4%8D%E4%BA%A4%E6%98%93%E6%97%A5/"/>
    
  </entry>
  
  <entry>
    <title>25.02.窄幅交易区间之后反转</title>
    <link href="https://jxch.github.io/2023/07/07/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/25.qu-shi-hui-fu-jiao-yi-ri/25-02-zhai-fu-jiao-yi-qu-jian-zhi-hou-fan-zhuan/"/>
    <id>https://jxch.github.io/2023/07/07/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/25.qu-shi-hui-fu-jiao-yi-ri/25-02-zhai-fu-jiao-yi-qu-jian-zhi-hou-fan-zhuan/</id>
    <published>2023-07-07T07:41:27.000Z</published>
    <updated>2023-07-07T07:56:17.537Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc25%2FSlide2.JPG" alt="图 25.2 窄幅交易区间之后反转"></p><blockquote><p><font color="red"><strong>有时候强劲趋势之后形成的窄幅交易区间会造成趋势反转而非趋势恢复。</strong></font>在图25.2中,<font color="orange"><strong>市场当天从K线3出现一轮强劲抛盘,然后进入窄幅交易区间,持续数小时。这种走势往往会导致尾盘恢复下跌趋势,而且尾盘下跌行情规模与初始下跌相当。在最后一轮下跌开始之前,市场往往对区间上轨频繁做出假突破。</strong></font><font color="green"><strong>K线12是一笔摆动做空交易的完美建仓形态,因为它是一根空头反转K线,在交易日尾盘突破窄幅交易区间的上轨。然而它后面那根K线并未成为一轮大跌行情的入场K线,而是形成一根小内包阳线,因此属于突破回踩做多形态。K线12为突破走势,这根内包K线为走势暂停(属于回调的一种)。</strong></font></p><p>本图的深入探讨</p><p>在图25.2中,<font color="green"><strong>交易日大幅跳空低开,第一根K线为强势多头反转K线,构成一个突破失败做多形态,有可能开启始于开盘的上升趋势交易日。</strong></font><br><font color="green"><strong>K线13和K线14均为长多头趋势K线,构成两根K线的突破走势。通常情况下,所有突破都会跟随一段与突破幅度相当的等距行情。</strong></font><font color="orange"><strong>等距幅度的测量一般是从急速行情第一根K线的开盘或低点到最后一根K线的收盘或高点。</strong></font><strong>市场当天刚好收于从K线13开盘到K线14高点幅度的等距目标处。</strong><br><font color="red"><strong>大部分下跌趋势恢复交易日开盘后都不会出现大幅上涨,而本例中一开盘的大拉升说明多头当天有激进买入的意愿。</strong></font><font color="orange"><strong>因此,虽然午盘的震荡为尾盘出现新一轮大跌做了极好的铺垫,但你不可能百分之百确定,因为总是有40%的可能性出现完全相反的情况。</strong></font><font color="green"><strong>市场有可能上涨测试当天开盘的另一个线索是,当天低点几乎刚好处于从开盘到第一轮上涨行情高点幅度的等距下跌目标处。</strong></font><font color="orange"><strong>也就是说,当天开盘处于全天振幅的中位。如果市场返回这一位置,在日线上将收出一根十字星(这种情况十分常见)。</strong></font>另外,<font color="green"><strong>我们注意到市场反复测试K线7低点的支撑位,但不断获得买盘支撑,无法跌破哪怕1个最小报价单位。K线8后面那根内包K线、K线9后面那根内包K线以及K线10和K线11的低点抬升均构成双底回踩做多建仓形态。</strong></font><br><font color="orange"><strong>这根支撑线低于最初的K线5入场K线(卖出高潮之后预期的回撤)1个最小报价单位。</strong></font><font color="green"><strong>K线5是当天低点双K线反转形态的入场K线。</strong></font><font color="orange"><strong>市场扫掉了这根入场K线下方的止损,但始终无法进一步下跌1个单位。这说明强势多头在行动。</strong></font>在整个窄幅交易区间中,买入和卖出程序(程序化交易)都在活动,但最终买入程序战胜了卖出程序。<strong>所有这些空头都必然要回补,从而增强了市场买压。除此之外,许多卖出程序还会转为买人程序,进一步强化买盘力度。</strong><font color="black"><strong>截至K线14的急速拉升之后跟随一个上升通道,持续到收盘。</strong></font></p></blockquote><hr><p>详见：<br>《高级趋势技术分析 价格行为交易系统之趋势分析》<br>第四篇 常见趋势形态<br>第25章 趋势恢复交易日<br>第2个案例（图 25.2）<br>第371-372页</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc25%2FSlide2.JPG&quot; alt=</summary>
      
    
    
    
    <category term="案例-趋势分析-常见趋势形态" scheme="https://jxch.github.io/categories/%E6%A1%88%E4%BE%8B-%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90-%E5%B8%B8%E8%A7%81%E8%B6%8B%E5%8A%BF%E5%BD%A2%E6%80%81/"/>
    
    
    <category term="案例-趋势恢复交易日" scheme="https://jxch.github.io/tags/%E6%A1%88%E4%BE%8B-%E8%B6%8B%E5%8A%BF%E6%81%A2%E5%A4%8D%E4%BA%A4%E6%98%93%E6%97%A5/"/>
    
  </entry>
  
  <entry>
    <title>25.01.跳空高开之后测试缺口</title>
    <link href="https://jxch.github.io/2023/07/07/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/25.qu-shi-hui-fu-jiao-yi-ri/25-01-tiao-kong-gao-kai-zhi-hou-ce-shi-que-kou/"/>
    <id>https://jxch.github.io/2023/07/07/pa/qu-shi-fen-xi/04.chang-jian-qu-shi-xing-tai/25.qu-shi-hui-fu-jiao-yi-ri/25-01-tiao-kong-gao-kai-zhi-hou-ce-shi-que-kou/</id>
    <published>2023-07-07T07:32:28.000Z</published>
    <updated>2023-07-07T07:40:27.395Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc25%2FSlide1.JPG" alt="图 25.1 跳空高开之后测试缺口"></p><blockquote><p><font color="red"><strong>在大幅跳空交易日,市场往往会在趋势开始之前测试开盘缺口。</strong></font>如图25.1所示,<font color="black"><strong>市场大幅跳空高开,以双底形式测试开盘低点,然后大幅上涨至K线3。从这里开始,市场进入窄幅交易区间,持续时间超过3个小时,让交易者误以为好的交易时机巳经过去了。</strong></font><font color="green"><strong>K线6小幅探破下降趋势通道线和K线4信号K线高点之后向上反转。这一走势导致一些空头错误地入场做空,许多多头被震仓出局。截至收盘的这轮上涨行情的信号K线是当天第一根均线缺口K线。</strong></font><br>当天还有不少其他做多入场点,<font color="green"><strong>比如K线7、9、10对微型趋势线假突破之后的向上反转。</strong></font><br>在图25.1中,<font color="green"><strong>K线7是一个高2买点,也是跌破微型上升趋势线1个最小报价单位之后向上反转。</strong></font><br><font color="green"><strong>K线8是高2买点的变体</strong></font>(阴线一阳线一阴线:K线7后面那根K线是阴线,因此是第一次向下推动,下一根K线为阳线,属于向上推动,接着又是一根阴线,为第二次向下推动)。</p></blockquote><hr><p>详见：<br>《高级趋势技术分析 价格行为交易系统之趋势分析》<br>第四篇 常见趋势形态<br>第25章 趋势恢复交易日<br>第1个案例（图 25.1）<br>第370页</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://objectstorage.us-phoenix-1.oraclecloud.com/n/axdikqaqm3dc/b/bucket1/o/pa-price-charts%2Ftrends%2Fc25%2FSlide1.JPG&quot; alt=</summary>
      
    
    
    
    <category term="案例-趋势分析-常见趋势形态" scheme="https://jxch.github.io/categories/%E6%A1%88%E4%BE%8B-%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90-%E5%B8%B8%E8%A7%81%E8%B6%8B%E5%8A%BF%E5%BD%A2%E6%80%81/"/>
    
    
    <category term="案例-趋势恢复交易日" scheme="https://jxch.github.io/tags/%E6%A1%88%E4%BE%8B-%E8%B6%8B%E5%8A%BF%E6%81%A2%E5%A4%8D%E4%BA%A4%E6%98%93%E6%97%A5/"/>
    
  </entry>
  
</feed>

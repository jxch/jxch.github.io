<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PA &amp; CODING</title>
  
  <subtitle>求仁得仁</subtitle>
  <link href="https://jxch.github.io/atom.xml" rel="self"/>
  
  <link href="https://jxch.github.io/"/>
  <updated>2024-06-13T03:51:28.858Z</updated>
  <id>https://jxch.github.io/</id>
  
  <author>
    <name>钱不寒</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Java并发-synchronized</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-synchronized/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-synchronized/</id>
    <published>2024-06-13T03:46:49.000Z</published>
    <updated>2024-06-13T03:51:28.858Z</updated>
    
    <content type="html"><![CDATA[<ul><li>synchronized是JVM内置锁，基于Monitor机制实现，依赖底层操作系统的互斥原语Mutex（互斥量），被阻塞的线程会被挂起、等待重新调度，会导致“用户态和内核态”两个态之间来回切换，对性能有较大影响</li><li>Monitor，直译为“监视器”，而操作系统领域一般翻译为“管程”。管程是指管理共享变量以及对共享变量操作的过程，让它们支持并发。</li><li>Java&nbsp;参考了&nbsp;MESA&nbsp;模型（管程中引入了条件变量的概念，而且每个条件变量都对应有一个等待队列），语言内置的管程（synchronized）对&nbsp;MESA&nbsp;模型进行了精简。MESA&nbsp;模型中，条件变量可以有多个，Java&nbsp;语言内置的管程里只有一个条件变量</li><li>Object&nbsp;类定义了&nbsp;wait()，notify()，notifyAll()&nbsp;方法，这些方法的具体实现，依赖于&nbsp;ObjectMonitor&nbsp;实现<ul><li>在获取锁时，是将当前线程插入到cxq的头部，而释放锁时，默认策略（QMode=0）是：如果EntryList为空，则将 cxq中的元素按原有顺序插入到EntryList，并唤醒第一个线程，也就是当EntryList为空时，是后来的线程先获取 锁。EntryList不为空，直接从EntryList中唤醒线程</li><li>锁状态被记录在每个对象的对象头的Mark&nbsp;Word中</li></ul></li></ul><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">ObjectMonitor()&nbsp;{_header&nbsp;=&nbsp;NULL;&nbsp;//对象头&nbsp;markOop_recursions&nbsp;=&nbsp;0;&nbsp;//&nbsp;锁的重入次数_object&nbsp;=&nbsp;NULL;&nbsp;//存储锁对象_owner&nbsp;=&nbsp;NULL;&nbsp;//&nbsp;标识拥有该monitor的线程（当前获取锁的线程）_WaitSet&nbsp;=&nbsp;NULL;&nbsp;//&nbsp;等待线程（调用wait）组成的双向循环链表，_WaitSet是第一个节点_cxq&nbsp;=&nbsp;NULL&nbsp;;&nbsp;//多线程竞争锁会先存到这个单向链表中&nbsp;（FILO栈结构）_EntryList&nbsp;=&nbsp;NULL&nbsp;;&nbsp;//存放在进入或重新进入时被阻塞(blocked)的线程&nbsp;(也是竞争锁失败的线程)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-synchronized-1.png" alt=""><br><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-synchronized-2.png" alt=""></p><hr><h2 id="对象的内存布局">对象的内存布局</h2><p>Hotspot虚拟机中，对象在内存中存储的布局可以分为三块区域</p><ul><li>对象头（Header）：比如&nbsp;hash码，对象所属的年代，对象锁，锁状态标志，偏向锁（线程）ID，偏向时间，数组长度（数组对象才有）</li><li>实例数据 （Instance&nbsp;Data）：存放类的属性数据信息，包括父类的属性信息</li><li>对齐填充（Padding）：对象起始地址必须是8字节的整数倍</li></ul><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-synchronized-3.png" alt=""></p><h3 id="对象头">对象头</h3><p>HotSpot虚拟机的对象头包括</p><ul><li>Mark&nbsp;Word：用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据的长度在32位和64位的虚拟机中分别为32bit和64bit</li><li>Klass&nbsp;Pointer：对象头的另外一部分是klass类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。&nbsp;32位4字节，64位开启指针压缩或最大堆内存&lt;32g时4字节，否则8字节（-XX:-UseCompressedOops 关闭指针压缩）</li><li>数组长度（只有数组对象有）：如果对象是一个数组,&nbsp;那在对象头中还必须有一块数据用于记录数组长度。&nbsp;4字节</li></ul><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-synchronized-4.png" alt=""></p><p>使用JOL工具查看内存布局</p><ul><li>OFFSET：偏移地址，单位字节</li><li>SIZE：占用的内存大小，单位为字节</li><li>TYPE&nbsp;DESCRIPTION：类型描述，其中object&nbsp;header为对象头</li><li>VALUE：对应内存中当前存储的值，二进制32位</li></ul><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.openjdk.jol<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span> <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>jol‐core<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token class-name">System</span><span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token class-name">ClassLayout</span><span class="token punctuation">.</span><span class="token function">parseInstance</span><span class="token punctuation">(</span>obj<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toPrintable</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="Mark-Word-记录锁状态（markOop-hpp）">Mark&nbsp;Word 记录锁状态（markOop.hpp）</h4><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-synchronized-5.png" alt="32位"><br><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-synchronized-6.png" alt="64位"></p><ul><li>hash：&nbsp;保存对象的哈希码。运行期间调用System.identityHashCode()来计算，延迟计算，并把结果赋值到这里</li><li>age：&nbsp;保存对象的分代年龄。表示对象被GC的次数，当该次数到达阈值的时候，对象就会转移到老年代</li><li>biased_lock：&nbsp;偏向锁标识位。由于无锁和偏向锁的锁标识都是&nbsp;01，没办法区分，这里引入一位的偏向锁标识位</li><li>lock：&nbsp;锁状态标识位。区分锁状态，但是11时表示对象待GC回收状态,&nbsp;只有最后2位锁标识(11)有效</li><li>JavaThread*：&nbsp;保存持有偏向锁的线程ID。偏向模式的时候，当某个线程持有对象的时候，对象这里就会被置为该线程的ID。&nbsp;在后面的操作中，就无需再进行尝试获取锁的动作。这个线程ID并不是JVM分配的线程ID号，和Java&nbsp;Thread中的ID是两个概念</li><li>epoch：&nbsp;保存偏向时间戳。偏向锁在CAS锁操作过程中，偏向性标识，表示对象更偏向哪个锁</li><li>ptr_to_lock_record：轻量级锁状态下，指向栈中锁记录的指针。当锁获取是无竞争时，JVM使用原子操作而不是OS互斥，这种技术称为轻量级锁定。在轻量级锁定的情况下，JVM通过CAS操作在对象的Mark&nbsp;Word中设置指向锁记录的指针</li><li>ptr_to_heavyweight_monitor：重量级锁状态下，指向对象监视器Monitor的指针。如果两个不同的线程同时在同一个对象上竞争，则必须将轻量级锁定升级到Monitor以管理等待的线程。在重量级锁定的情况下，JVM在对象的ptr_to_heavyweight_monitor设置指向Monitor的指针</li></ul><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-synchronized-7.png" alt=""></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">enum&nbsp;{&nbsp;locked_value&nbsp;=&nbsp;0,&nbsp;//00&nbsp;轻量级锁unlocked_value&nbsp;=&nbsp;1,&nbsp;//001&nbsp;无锁monitor_value&nbsp;=&nbsp;2,&nbsp;//10&nbsp;监视器锁，也叫膨胀锁，也叫重量级锁 marked_value&nbsp;=&nbsp;3,&nbsp;//11&nbsp;GC标记biased_lock_pattern&nbsp;=&nbsp;5&nbsp;//101&nbsp;偏向锁};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><ul><li>偏向锁：偏向锁是一种针对加锁操作的优化手段，经过研究发现，在大多数情况下，锁不仅不存在多 线程竞争，而且总是由同一线程多次获得，因此为了消除数据在无竞争情况下锁重入（CAS操作）的开销而引入偏向锁。对于没有锁竞争的场合，偏向锁有很好的优化效果<ul><li>匿名偏向状态：新创建对象的Mark&nbsp;Word中的Thread&nbsp;Id为0，说明此时处于可偏向但未偏向任何线程，也叫做匿名偏向状态(anonymously&nbsp;biased)</li><li>偏向锁延迟偏向：HotSpot&nbsp;虚拟机在启动后有个&nbsp;4s&nbsp;的延迟才会对每个新建的对象开启偏向锁模式。JVM启动时会进行一系列的复杂活动，比如装载配置，系统类初始化等等。在这个过程中会使用大量synchronized关键字对对象加锁，且这些锁大多数都不是偏向锁。为了减少初始化时间，JVM默认延时加载偏向锁</li><li>偏向锁撤销<ul><li>调用对象HashCode：调用锁对象的obj.hashCode()或System.identityHashCode(obj)方法会导致该对象的偏向锁被撤销。因为对于一个对象，其HashCode只会生成一次并保存，偏向锁是没有地方保存hashcode的<ul><li>轻量级锁会在锁记录中记录&nbsp;hashCode</li><li>重量级锁会在&nbsp;Monitor&nbsp;中记录&nbsp;hashCode</li><li>当对象处于可偏向（也就是线程ID为0）和已偏向的状态下，调用HashCode计算将会使对象再也无法偏向<ul><li>当对象可偏向时，MarkWord将变成未锁定状态，并只能升级成轻量锁</li><li>当对象正处于偏向锁时，调用HashCode将使偏向锁强制升级成重量锁</li></ul></li></ul></li><li>调用wait/notify<ul><li>偏向锁状态执行obj.notify()&nbsp;会升级为轻量级锁</li><li>调用obj.wait(timeout)&nbsp;会升级为重量级锁</li></ul></li></ul></li><li>偏向锁批量重偏向&amp;批量撤销：当只有一个线程反复进入同步块时，偏向锁带来的性能开销基本可以忽略，但是当有其他线程尝试获得锁时，就需要等到safe&nbsp;point时，再将偏向锁撤销为无锁状态或升级为轻量级，会消耗一定的性能，所以在多线程竞争频繁的情况下，偏向锁不仅不能提高性能，还会导致性能下降。于是，就有了批量重偏向与批量撤销的机制<ul><li>批量重偏向：默认class偏向撤销阈值20<ul><li>以class为单位，为每个class维护一个偏向锁撤销计数器，每一次该class的对象发生偏向撤销操作时，该计数器+1，当这个值达到重偏向阈值（默认20）时，JVM就认为该class的偏向锁有问题，因此会进行批量重偏向</li></ul></li><li>批量撤销：默认class偏向撤销阈值40<ul><li>每个class对象会有一个对应的epoch字段，每个处于偏向锁状态对象的Mark&nbsp;Word中也有该字段，其初始值为创建该对象时class中的epoch的值。每次发生批量重偏向时，就将该值+1，同时遍历JVM中所有线程的栈，找到该class所有正处于加锁状态的偏向锁，将其epoch字段改为新值。下次获得锁时，发现当前对象的epoch值和class的epoch不相等，那就算当前已经偏向了其他线程，也不会执行撤销操作，而是直接通过CAS操作将其Mark&nbsp;Word的Thread&nbsp;Id&nbsp;改成当前线程Id</li><li>当达到重偏向阈值（默认20）后，假设该class计数器继续增长，当其达到批量撤销的阈值后（默认40），JVM就认为该class的使用场景存在多线程竞争，会标记该class为不可偏向，之后，对于该class的锁，直接走轻量级锁的逻辑</li></ul></li><li>应用场景<ul><li>批量重偏向（bulk&nbsp;rebias）机制是为了解决：一个线程创建了大量对象并执行了初始的同步操作，后来另一个线程也来将这些对象作为锁对象进行操作，这样会导致大量的偏向锁撤销操作</li><li>批量撤销（bulk&nbsp;revoke）机制是为了解决：在明显多线程竞争剧烈的场景下使用偏向锁是不合适的</li></ul></li><li>JVM参数<ul><li><code>-XX:BiasedLockingBulkRebiasThreshold</code>：偏向锁批量重偏向阈值</li><li><code>-XX:BiasedLockingBulkRevokeThreshold</code>：偏向锁批量撤销阈值</li></ul></li><li>批量重偏向和批量撤销是针对类的优化，和对象无关</li><li>偏向锁重偏向一次之后不可再次重偏向</li><li>当某个类已经触发批量撤销机制后，JVM会默认当前类产生了严重的问题，剥夺了该类的新实例对象使用偏向锁的权利</li></ul></li></ul></li><li>轻量级锁：倘若偏向锁失败，虚拟机并不会立即升级为重量级锁，它还会尝试使用一种称为轻量级锁的优化手段，此时Mark&nbsp;Word&nbsp;的结构也变为轻量级锁的结构。轻量级锁所适应的场景是线程交替执行同步块的场合，如果存在同一时间多个线程访问同一把锁的场合，就会导致轻量级锁膨胀为重量级锁<ul><li>轻量级锁所适应的场景是：线程交替执行同步块</li><li>偏向锁升级轻量级锁</li><li>CAS自旋修改Mark Word中锁记录的地址，失败超过一定次数轻量级锁会膨胀为重量级锁</li></ul></li><li>重量级锁<ul><li>自旋优化：重量级锁竞争的时候，还可以使用自旋来进行优化，如果当前线程自旋成功（即这时候持锁线程已经退出了同步块，释放了锁），这时当前线程就可以避免阻塞<ul><li>自旋会占用&nbsp;CPU&nbsp;时间，单核&nbsp;CPU&nbsp;自旋就是浪费，多核&nbsp;CPU&nbsp;自旋才能发挥优势</li><li>自旋是自适应的，比如对象刚刚的一次自旋操作成功过，那么认为这次自旋成功的可能性会高，就多自旋几次；反之，就少自旋甚至不自旋</li><li>不能控制是否开启自旋功能</li><li>自旋的目的是为了减少线程挂起的次数，尽量避免直接挂起线程（挂起操作涉及系统调用，存在用户态和内核态切换，这才是重量级锁最大的开销）</li></ul></li></ul></li><li>锁粗化：假设一系列的连续操作都会对同一个对象反复加锁及解锁，甚至加锁操作是出现在循环体中的，即使没有出现线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗。如果JVM检测到有一连串零碎的操作都是对同一对象的加锁，将会扩大加锁同步的范围（即锁粗化）到整个操作序列的外部</li><li>锁消除：即删除不必要的加锁操作。锁消除是Java虚拟机在JIT编译期间，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过锁消除，可以节省毫无意义的请求锁时间<ul><li><code>‐XX:+EliminateLocks</code> 开启锁消除</li></ul></li></ul><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-synchronized-8.png" alt=""></p><hr><ul><li>逃逸分析（Escape&nbsp;Analysis）：是一种可以有效减少Java&nbsp;程序中同步负载和内存堆分配压力的跨函数全局数据流分析算法。通过逃逸分析，Java&nbsp;Hotspot编译器能够分析出一个新的对象的引用的使用范围从而决定是否要将这个对象分配到堆上<ul><li>逃逸分析的基本行为就是分析对象动态作用域</li><li>方法逃逸：对象逃出当前方法</li><li>线程逃逸：对象逃出当前线程</li><li>使用逃逸分析，编译器可以对代码做如下优化<ul><li>同步省略或锁消除（Synchronization&nbsp;Elimination）。如果一个对象被发现只能从一个线程被访问到，那么对于这个对象的操作可以不考虑同步</li><li>将堆分配转化为栈分配（Stack&nbsp;Allocation）。如果一个对象在子程序中被分配，要使指向该对象的指针永远不会逃逸，对象可能是栈分配的候选，而不是堆分配</li><li>分离对象或标量替换（Scalar&nbsp;Replacement）。有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分（或全部）可以不存储在内存，而是存储在CPU寄存器中</li></ul></li><li>JVM参数<ul><li><code>‐XX:+DoEscapeAnalysis</code> 开启逃逸分析</li><li><code>‐XX:+EliminateAllocations</code> 开启标量替换</li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;synchronized是JVM内置锁，基于Monitor机制实现，依赖底层操作系统的互斥原语Mutex（互斥量），被阻塞的线程会被挂起、等待重新调度，会导致“用户态和内核态”两个态之间来回切换，对性能有较大影响&lt;/li&gt;
&lt;li&gt;Monitor，直译为“监视器</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-Semaphore &amp; CountDownLatch &amp; CyclicBarrier</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-semaphore-countdownlatch-cyclicbarrier/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-semaphore-countdownlatch-cyclicbarrier/</id>
    <published>2024-06-13T03:45:49.000Z</published>
    <updated>2024-06-13T03:45:30.753Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Semaphore，俗称信号量，它是操作系统中PV操作（P表示通过；V表示释放）的原语在java的实现，它也是基于AbstractQueuedSynchronizer实现的，可以用于做流量控制，特别是公用资源有限的应用场景<ul><li>acquire()&nbsp;表示阻塞并获取许可</li><li>tryAcquire()&nbsp;方法在没有许可的情况下会立即返回&nbsp;false，要获取许可的线程不会阻塞</li><li>release()&nbsp;表示释放许可</li><li>int&nbsp;availablePermits()：返回此信号量中当前可用的许可证数</li><li>int&nbsp;getQueueLength()：返回正在等待获取许可证的线程数</li><li>boolean&nbsp;hasQueuedThreads()：是否有线程正在等待获取许可证</li><li>void&nbsp;reducePermit(int&nbsp;reduction)：减少&nbsp;reduction&nbsp;个许可证</li><li>Collection&nbsp;getQueuedThreads()：返回所有等待获取许可证的线程集合</li></ul></li><li>CountDownLatch（闭锁）是一个同步协助类，允许一个或多个线程等待，直到其他线程完成操作集<ul><li>CountDownLatch使用给定的计数值（count）初始化。await方法会阻塞直到当前的计数值（count）由于countDown方法的调用达到0，count为0之后所有等待的线程都会被释放，并且随后对await方法的调用都会立即返回。这是一个一次性现象&nbsp;——&nbsp;count不会被重置。如果你需要一个重置count的版本，那么请考虑使用CyclicBarrier</li><li>底层基于&nbsp;AbstractQueuedSynchronizer&nbsp;实现，CountDownLatch&nbsp;构造函数中指定的count直接赋给AQS的state；每次countDown()则都是release(1)减1，最后减到0时unpark阻塞线程；这一步是由最后一个执行countdown方法的线程执行的</li><li>调用await()方法时，当前线程就会判断state属性是否为0，如果为0，则继续往下执行，如果不为0，则使当前线程进入等待状态，直到某个线程将state属性置为0，其就会唤醒在await()方法中等待的线程</li></ul></li><li>Thread.&nbsp;join()：&nbsp;的实现原理是不停检查join线程是否存活，如果&nbsp;join&nbsp;线程存活则让当前线程永远等待</li><li>CyclicBarrier：字面意思回环栅栏（循环屏障），通过它可以实现让一组线程等待至某个状态（屏障点）之后再全部同时执行。叫做回环是因为当所有等待线程都被释放以后，CyclicBarrier可以被重用<ul><li>CyclicBarrier&nbsp;可以用于多线程计算数据，最后合并计算结果的场景</li></ul></li><li>CountDownLatch与CyclicBarrier的区别<ul><li>CountDownLatch的计数器只能使用一次，而CyclicBarrier的计数器可以使用reset()&nbsp;方法重置</li><li>CyclicBarrier还提供getNumberWaiting（可以获得CyclicBarrier阻塞的线程数量）、isBroken（用来知道阻塞的线程是否被中断）等方法</li><li>CountDownLatch会阻塞主线程，CyclicBarrier不会阻塞主线程，只会阻塞子线程</li><li>CountDownLatch和CyclicBarrier都能够实现线程之间的等待，只不过它们侧重点不同。CountDownLatch一般用于一个或多个线程，等待其他线程执行完任务后，再执行。CyclicBarrier一般用于一组线程互相等待至某个状态，然后这一组线程再同时执行</li><li>CyclicBarrier&nbsp;还可以提供一个&nbsp;barrierAction，合并多线程计算结果</li><li>CyclicBarrier是通过ReentrantLock的"独占锁"和Conditon来实现一组线程的阻塞唤醒的，而CountDownLatch则是通过AQS的“共享锁”实现</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Semaphore，俗称信号量，它是操作系统中PV操作（P表示通过；V表示释放）的原语在java的实现，它也是基于AbstractQueuedSynchronizer实现的，可以用于做流量控制，特别是公用资源有限的应用场景
&lt;ul&gt;
&lt;li&gt;acquire()&amp;</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-ReentrantReadWriteLock</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-reentrantreadwritelock/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-reentrantreadwritelock/</id>
    <published>2024-06-13T03:42:49.000Z</published>
    <updated>2024-06-13T03:44:46.326Z</updated>
    
    <content type="html"><![CDATA[<ul><li>写锁是独占的，读锁是共享的：读读可以并发；读写，写读，写写互斥。在读多写少的场景中，读写锁能够提供比排它锁更好的并发性和吞吐量<ul><li>读锁不支持条件变量</li><li>重入时升级不支持：持有读锁的情况下去获取写锁，会导致获取永久等待</li><li>重入时支持降级：&nbsp;持有写锁的情况下可以去获取读锁</li></ul></li><li>线程进入读锁的前提条件<ul><li>没有其他线程的写锁</li><li>没有写请求或者有写请求，但调用线程和持有锁的线程是同一个</li></ul></li><li>线程进入写锁的前提条件<ul><li>没有其他线程的读锁</li><li>没有其他线程的写锁</li></ul></li><li>读写锁有以下三个重要的特性<ul><li>公平选择性：支持非公平（默认）和公平的锁获取方式，吞吐量还是非公平优于公平</li><li>可重入：读锁和写锁都支持线程重入。以读写线程为例：读线程获取读锁后，能够再次获取读锁。写线程在获取写锁之后能够再次获取写锁，同时也可以获取读锁</li><li>锁降级：遵循获取写锁、再获取读锁最后释放写锁的次序，写锁能够降级成为读锁</li></ul></li><li>锁降级：锁降级指的是写锁降级成为读锁<ul><li>锁降级是指把持住（当前拥有的）写锁，再获取到读锁，随后释放（先前拥有的）写锁的过程</li><li>如果当前线程拥有写锁，然后将其释放，最后再获取读锁，这种分段完成的过程不能称之为锁降级</li><li>锁降级可以帮助我们拿到当前线程修改后的结果而不被其他线程所破坏，防止更新丢失</li></ul></li><li>锁降级中为什么要获取读锁：主要是为了保证数据的可见性<ul><li>如果当前线程不获取读锁而是直接释放写锁，假设此刻另一个线程（记作线程T）获取了写锁并修改了数据，那么当前线程无法感知线程T的数据更新</li><li>如果当前线程获取读锁，即遵循锁降级的步骤，则线程T将会被阻塞，直到当前线程使用数据并释放读锁之后，线程T才能获取写锁进行数据更新</li></ul></li><li>RentrantReadWriteLock不支持锁升级（把持读锁、获取写锁，最后释放读锁的过程）：目的也是保证数据可见性<ul><li>如果读锁已被多个线程获取，其中任意线程成功获取了写锁并更新了数据，则其更新对其他获取到读锁的线程是不可见的</li></ul></li><li>用一个变量如何维护多种状态：高16为表示读，低16为表示写<ul><li>通过位运算确定读锁和写锁的状态<ul><li>写状态，等于&nbsp;S&nbsp;&amp;&nbsp;0x0000FFFF（将高&nbsp;16&nbsp;位全部抹去）。&nbsp;当写状态加1，等于S+1</li><li>读状态，等于&nbsp;S&nbsp;&gt;&gt;&gt;&nbsp;16&nbsp;(无符号补&nbsp;0&nbsp;右移&nbsp;16&nbsp;位)。当读状态加1，等于 S +（1&lt;&lt;16）,也就是 S+0x00010000</li></ul></li><li>S不等于0时，当写状态（S&amp;0x0000FFFF）等于0时，读状态（S&gt;&gt;&gt;16）大于0，即读锁已被获取</li></ul></li><li>exclusiveCount(int&nbsp;c)&nbsp;静态方法，获得持有写状态的锁的次数</li><li>sharedCount(int&nbsp;c)&nbsp;静态方法，获得持有读状态的锁的线程数量。</li><li>HoldCounter&nbsp;计数器：不同于写锁，读锁可以同时被多个线程持有。而每个线程持有的读锁支持重入的特性，所以需要对每个线程持有的读锁的数量单独计数，这就需要用到&nbsp;HoldCounter&nbsp;计数器<ul><li>读锁的内在机制其实就是一个共享锁。一次共享锁的操作就相当于对HoldCounter&nbsp;计数器的操作。获取共享锁，则该计数器&nbsp;+&nbsp;1，释放共享锁，该计数器&nbsp;-&nbsp;1。只有当线程获取共享锁后才能对共享锁进行释放、重入操作</li><li>通过&nbsp;ThreadLocalHoldCounter&nbsp;类，HoldCounter&nbsp;与线程进行绑定。HoldCounter&nbsp;是绑定线程的一个计数器，而&nbsp;ThreadLocalHoldCounter&nbsp;则是线程绑定的&nbsp;ThreadLocal<ul><li>HoldCounter是用来记录读锁重入数的对象</li><li>ThreadLocalHoldCounter是ThreadLocal变量，用来存放不是第一个获取读锁的线程的其他线程的读锁重入数对象</li></ul></li></ul></li><li>写锁的获取：写锁是一个支持重进入的排它锁。如果当前线程已经获取了写锁，则增加写状态。如果当前线程在获取写锁时，读锁已经被获取（读状态不为0）或者该线程不是已经获取写锁的线程，&nbsp;则当前线程进入等待状态</li></ul><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-ReentrantReadWriteLock-1.png" alt="写锁获取"><br><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-ReentrantReadWriteLock-2.png" alt="写锁释放"><br><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-ReentrantReadWriteLock-3.png" alt="读锁获取"><br><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-ReentrantReadWriteLock-4.png" alt="读锁释放"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;写锁是独占的，读锁是共享的：读读可以并发；读写，写读，写写互斥。在读多写少的场景中，读写锁能够提供比排它锁更好的并发性和吞吐量
&lt;ul&gt;
&lt;li&gt;读锁不支持条件变量&lt;/li&gt;
&lt;li&gt;重入时升级不支持：持有读锁的情况下去获取写锁，会导致获取永久等待&lt;/li&gt;
&lt;</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-ReentrantLock</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-reentrantlock/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-reentrantlock/</id>
    <published>2024-06-13T03:41:49.000Z</published>
    <updated>2024-06-13T03:42:33.908Z</updated>
    
    <content type="html"><![CDATA[<p>ReentrantLock是一种基于AQS框架的应用实现，是JDK中的一种线程并发访问的同步手段，它的功能类似于synchronized是一种互斥锁，可以保证线程安全。相对于&nbsp;synchronized，&nbsp;ReentrantLock具备如下特点：</p><ul><li>可中断</li><li>可以设置超时时间</li><li>可以设置为公平锁（ReentrantLock&nbsp;默认是不公平的）</li><li>支持多个条件变量</li><li>与&nbsp;synchronized&nbsp;一样，都支持可重入</li></ul><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-ReentrantLock-1.png" alt="加锁入队"><br><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-ReentrantLock-2.png" alt="解锁出队"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;ReentrantLock是一种基于AQS框架的应用实现，是JDK中的一种线程并发访问的同步手段，它的功能类似于synchronized是一种互斥锁，可以保证线程安全。相对于&amp;nbsp;synchronized，&amp;nbsp;ReentrantLock具备如下特点：&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-JMM</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-jmm/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-jmm/</id>
    <published>2024-06-13T03:39:49.000Z</published>
    <updated>2024-06-13T03:41:04.446Z</updated>
    
    <content type="html"><![CDATA[<p>并发三大特性</p><ul><li>可见性：volatile；内存屏障；synchronized；Lock；final</li><li>有序性（指令重排序）：volatile；内存屏障；synchronized；Lock</li><li>原子性：synchronized；Lock；CAS</li></ul><hr><ul><li>Java内存模型（Java Memory Model，JMM）：用于屏蔽掉各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的并发效果<ul><li>规定了一个线程如何和何时可以看到由其他线程修改过后的共享变量的值，以及在必须时如何同步的访问共享变量</li><li>JMM是围绕原子性、有序性、可见性展开的</li></ul></li><li>内存交互操作：关于主内存与工作内存之间的具体交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步到主内存之间的实现细节，Java内存模型定义了以下八种操作来完成<ul><li>lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。</li><li>unlock（解锁）：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。</li><li>read（读取）：作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用</li><li>load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。</li><li>use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。</li><li>assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。</li><li>store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。</li><li>write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。</li><li><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-JMM-1.png" alt=""></li></ul></li><li>Java内存模型还规定了在执行上述八种基本操作时，必须满足如下规则<ul><li>如果要把一个变量从主内存中复制到工作内存，就需要按顺寻地执行read和load操作， 如果把变量从工作内存中同步回主内存中，就要按顺序地执行store和write操作。但Java内存模型只要求上述操作必须按顺序执行，而没有保证必须是连续执行。</li><li>不允许read和load、store和write操作之一单独出现</li><li>不允许一个线程丢弃它的最近assign的操作，即变量在工作内存中改变了之后必须同步到主内存中。</li><li>不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步回主内存中。</li><li>一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量。即就是对一个变量实施use和store操作之前，必须先执行过了assign和load操作。</li><li>一个变量在同一时刻只允许一条线程对其进行lock操作，但lock操作可以被同一条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。lock和unlock必须成对出现</li><li>如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值</li><li>如果一个变量事先没有被lock操作锁定，则不允许对它执行unlock操作；也不允许去unlock一个被其他线程锁定的变量。</li><li>对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作）。</li></ul></li><li>JMM的内存可见性保证<ul><li>单线程程序。单线程程序不会出现内存可见性问题。编译器、runtime和处理器会共同确保单线程程序的执行结果与该程序在顺序一致性模型中的执行结果相同。</li><li>正确同步的多线程程序。正确同步的多线程程序的执行将具有顺序一致性（程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同）。这是JMM关注的重点，JMM通过限制编译器和处理器的重排序来为程序员提供内存可见性保证。</li><li>未同步/未正确同步的多线程程序。JMM为它们提供了最小安全性保障：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值，未同步程序在JMM中的执行时，整体上是无序的，其执行结果无法预知。 JMM不保证未同步程序的执行结果与该程序在顺序一致性模型中的执行结果一致。<ul><li>未同步程序在JMM中的执行时，整体上是无序的，其执行结果无法预知。未同步程序在两个模型中的执行特性有如下几个差异<ul><li>顺序一致性模型保证单线程内的操作会按程序的顺序执行，而JMM不保证单线程内的操作会按程序的顺序执行，比如正确同步的多线程程序在临界区内的重排序</li><li>顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而JMM不保证所有线程能看到一致的操作执行顺序</li><li>顺序一致性模型保证对所有的内存读/写操作都具有原子性，而JMM不保证对64位的long型和double型变量的写操作具有原子性（32位处理器）<ul><li>JVM在32位处理器上运行时，可能会把一个64位long/double型变量的写操作拆分为两个32位的写操作来执行。这两个32位的写操作可能会被分配到不同的总线事务中执行，此时对这个64位变量的写操作将不具有原子性。从JSR-133内存模型开始（即从JDK5开始），仅仅只允许把一个64位long/double型变量的写操作拆分为两个32位的写操作来执行，任意的读操作在JSR-133中都必须具有原子性</li></ul></li></ul></li></ul></li></ul></li></ul><hr><ul><li>volatile<ul><li>可见性：对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入<ul><li>内存交互层面：volatile修饰的变量的read、load、use操作和assign、store、write必须是连续的，即修改后必须立即同步回主内存，使用时必须从主内存刷新，由此保证volatile变量操作对多线程的可见性</li><li>硬件层面：通过lock前缀指令，会锁定变量缓存行区域并写回主内存，这个操作称为“缓存锁定”，缓存一致性机制会阻止同时修改被两个以上处理器缓存的内存区域数据。一个处理器的缓存回写到内存会导致其他处理器的缓存无效</li></ul></li><li>原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不具有原子性（基于这点，我们通过会认为volatile不具备原子性）。volatile仅仅保证对单个volatile变量的读/写具有原子性，而锁的互斥执行的特性可以确保对整个临界区代码的执行具有原子性<ul><li>64位的long型和double型变量，只要它是volatile变量，对该变量的读/写就具有原子性</li></ul></li><li>有序性：对volatile修饰的变量的读写操作前后加上各种特定的内存屏障来禁止指令重排序来保障有序性<ul><li>在JSR-133之前的旧Java内存模型中，虽然不允许volatile变量之间重排序，但旧的Java内存模型允许volatile变量与普通变量重排序。为了提供一种比锁更轻量级的线程之间通信的机制，JSR-133专家组决定增强volatile的内存语义：严格限制编译器和处理器对volatile变量与普通变量的重排序，确保volatile的写-读和锁的释放-获取具有相同的内存语义</li></ul></li><li>内存语义<ul><li>当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量值刷新到主内存</li><li>当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效，线程接下来将从主内存中读取共享变量</li></ul></li><li>hotspot<ul><li>字节码解释器：<code>if (cache -&gt; is_volatile()) { OrderAccess::storeload(); }</code></li><li>模板解释器：对每个指令都写了一段对应的汇编代码，启动时将每个指令与对应汇编代码入口绑定。x86处理器中利用lock实现类似内存屏障的效果：<code>lock addl $0, $0(%rsp)</code>。</li></ul></li></ul></li></ul><hr><ul><li>LOCK 前缀指令：<ul><li>确保后续指令执行的原子性。在Pentium及之前的处理器中，带有lock前缀的指令在执行期间会锁住总线，使得其它处理器暂时无法通过总线访问内存，很显然，这个开销很大。在新的处理器中，Intel使用缓存锁定来保证指令执行的原子性，缓存锁定将大大降低lock前缀指令的执行开销。</li><li>LOCK前缀指令具有类似于内存屏障的功能，禁止该指令与前面和后面的读写指令重排序。</li><li>LOCK前缀指令会等待它之前所有的指令完成、并且所有缓冲的写操作写回内存(也就是将store buffer中的内容写入内存)之后才开始执行，并且根据缓存一致性协议，刷新store buffer的操作会导致其他cache中的副本失效。</li><li>汇编层面（日志）：<code>-XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly -Xcomp</code></li><li>硬件层面：32位的IA-32处理器支持对系统内存中的位置进行锁定的原子操作。这些操作通常用于管理共享的数据结构（如信号量、段描述符、系统段或页表），在这些结构中，两个或多个处理器可能同时试图修改相同的字段或标志。处理器使用三种相互依赖的机制来执行锁定的原子操作<ul><li>有保证的原子操作；总线锁定，使用LOCK#信号和LOCK指令前缀；缓存一致性协议，确保原子操作可以在缓存的数据结构上执行（缓存锁，这种机制出现在Pentium4、Intel Xeon和P6系列处理器中）</li></ul></li></ul></li></ul><hr><ul><li>指令重排序：JVM线程内部维持顺序化语义。即只要程序的最终结果与它顺序化情况的结果相等，那么指令的执行顺序可以与代码顺序不一致，此过程叫指令的重排序。</li><li>指令重排序的意义：JVM能根据处理器特性（CPU多级缓存系统、多核处理器等）适当的对机器指令进行重排序，使机器指令能更符合CPU的执行特性，最大限度的发挥机器性能。在编译器与CPU处理器中都能执行指令重排优化操作</li><li><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-JMM-2.png" alt=""></li><li>volatile重排序规则</li><li><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-JMM-3.png" alt=""></li><li>volatile禁止重排序场景<ul><li>第二个操作是volatile写，不管第一个操作是什么都不会重排序</li><li>第一个操作是volatile读，不管第二个操作是什么都不会重排序</li><li>第一个操作是volatile写，第二个操作是volatile读，也不会发生重排序</li></ul></li><li>JMM内存屏障插入策略<ul><li>在每个volatile写操作的前面插入一个StoreStore屏障</li><li>在每个volatile写操作的后面插入一个StoreLoad屏障</li><li>在每个volatile读操作的后面插入一个LoadLoad屏障</li><li>在每个volatile读操作的后面插入一个LoadStore屏障</li><li>x86处理器不会对读-读、读-写和写-写操作做重排序, 会省略掉这3种操作类型对应的内存屏障。仅会对写-读操作做重排序，所以volatile写-读操作只需要在volatile写后插入StoreLoad屏障</li></ul></li><li><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-JMM-4.png" alt=""></li><li>JVM层面的内存屏障：JSR规范中定义了4种内存屏障<ul><li>LoadLoad屏障：（指令Load1; LoadLoad; Load2），在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。</li><li>LoadStore屏障：（指令Load1; LoadStore; Store2），在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。</li><li>StoreStore屏障：（指令Store1; StoreStore; Store2），在Store2及后续写入操作执行前，保证Store1的写入操作对其它处理器可见。</li><li>StoreLoad屏障：（指令Store1; StoreLoad; Load2），在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能</li><li>由于x86只有store load可能会重排序，所以只有JSR的StoreLoad屏障对应它的mfence或lock前缀指令，其他屏障对应空操作</li></ul></li><li>硬件层内存屏障：硬件层提供了一系列的内存屏障 memory barrier / memory fence(Intel的提法)来提供一致性的能力。拿X86平台来说，有几种主要的内存屏障<ul><li>lfence，是一种 Load Barrier 读屏障</li><li>sfence, 是一种 Store Barrier 写屏障</li><li>mfence, 是一种全能型的屏障，具备lfence和sfence的能力</li><li>Lock前缀，Lock不是一种内存屏障，但是它能完成类似内存屏障的功能。Lock会对CPU总线和高速缓存加锁，可以理解为CPU指令级的一种锁。它后面可以跟ADD, ADC, AND, BTC, BTR, BTS, CMPXCHG, CMPXCH8B, DEC, INC, NEG, NOT, OR, SBB, SUB, XOR, XADD, and XCHG等指令</li></ul></li><li>内存屏障有两个能力：阻止屏障两边的指令重排序；刷新处理器缓存/冲刷处理器缓存<ul><li>对Load Barrier来说，在读指令前插入读屏障，可以让高速缓存中的数据失效，重新从主内存加载数据；</li><li>对Store Barrier来说，在写指令之后插入写屏障，能让写入缓存的最新数据写回到主内存。</li><li>Lock前缀实现了类似的能力，它先对总线和缓存加锁，然后执行后面的指令，最后释放锁后会把高速缓存中的数据刷新回主内存。在Lock锁住总线的时候，其他CPU的读写请求都会被阻塞，直到锁释放。</li></ul></li><li>不同硬件实现内存屏障的方式不同，Java内存模型屏蔽了这种底层硬件平台的差异，由JVM来为不同的平台生成相应的机器码。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;并发三大特性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可见性：volatile；内存屏障；synchronized；Lock；final&lt;/li&gt;
&lt;li&gt;有序性（指令重排序）：volatile；内存屏障；synchronized；Lock&lt;/li&gt;
&lt;li&gt;原子性：synchroniz</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-happens-before</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-happens-before/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-happens-before/</id>
    <published>2024-06-13T03:38:49.000Z</published>
    <updated>2024-06-13T03:38:16.930Z</updated>
    
    <content type="html"><![CDATA[<ul><li>as-if-serial：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义<ul><li>为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序</li></ul></li><li>happens-before：从JDK 5 开始，JMM使用happens-before的概念来阐述多线程之间的内存可见性。在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须存在happens-before关系<ul><li>它是判断数据是否存在竞争、线程是否安全的主要依据，依靠这个原则，我们解决在并发环境下两操作之间是否可能存在冲突的所有问题</li></ul></li><li>happens-before原则定义<ul><li>如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。</li><li>两个操作之间存在happens-before关系，并不意味着一定要按照happens-before原则制定的顺序来执行。如果重排序之后的执行结果与按照happens-before关系来执行的结果一致，那么这种重排序并不非法。</li></ul></li><li>happens-before原则规则<ul><li>程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作；</li><li>锁定规则：一个unLock操作先行发生于后面对同一个锁的lock操作；</li><li>volatile变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作；</li><li>传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C；</li><li>线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作；</li><li>线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生；</li><li>线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行；</li><li>对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始；</li></ul></li><li>如果两个操作不存在上述任一一个happens-before规则，那么这两个操作就没有顺序的保障，JVM可以对这两个操作进行重排序。如果操作A happens-before操作 B，那么操作A在内存上所做的操作对操作B都是可见的</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;as-if-serial：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义
&lt;ul&gt;
&lt;li&gt;为了遵守as-if-serial语义，编译器和处理器不会对存在数</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-Future</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-future/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-future/</id>
    <published>2024-06-13T03:37:49.000Z</published>
    <updated>2024-06-13T03:37:43.801Z</updated>
    
    <content type="html"><![CDATA[<ul><li>FutureTask: 存储任务的处理结果，更新任务的状态<ul><li>既可以被当做Runnable来执行，也可以被当做Future来获取Callable的返回结果</li></ul></li><li>Future: <code>cancel isCancelled isDone get</code></li><li>CompletionService 一边生成任务，一边获取任务的返回值，任务之间不会互相阻塞，可以实现先执行完的先取结果，不再依赖任务顺序了<ul><li>内部通过阻塞队列+FutureTask，实现了任务先完成可优先获取到，即结果按照完成先后顺序排序</li><li>内部有一个先进先出的阻塞队列，用于保存已经执行完成的Future，通过调用它的take方法或poll方法可以获取到一个已经执行完成的Future，进而通过调用Future接口实现类的get方法获取最终的结果</li></ul></li><li>CompletionStage 执行某一个阶段，可向下执行后续阶段。异步执行，默认线程池是 <code>ForkJoinPool.commonPool()</code></li><li>CompletableFuture 实现了对任务的编排能力（业务逻辑处理存在串行[依赖]、并行、聚合的关系）<ul><li>获取结果<ul><li>join 抛出的是 uncheck 异常（即未经检查的异常),不会强制开发者抛出</li><li>get 抛出的是经过检查的异常，ExecutionException,&nbsp;InterruptedException&nbsp;需要用户手动处理</li></ul></li><li>结果处理：<code>whenComplete whenCompleteAsync exceptionally</code><ul><li>当 CompletableFuture 的计算结果完成，或者抛出异常的时候，我们可以执行特定的&nbsp;Action</li><li>Action的类型是 <code>BiConsumer&lt;?&nbsp;super&nbsp;T,?&nbsp;super&nbsp;Throwable&gt;</code>，它可以处理正常的计算结果，或者异常情况</li><li>方法不以Async结尾，意味着Action使用相同的线程执行，而Async可能会使用其它的线程去执行（如果使用相同的线程池，也可能会被同一个线程选中执行）</li><li>这几个方法都会返回CompletableFuture，当Action执行完毕后它的结果返回原始的CompletableFuture的计算结果或者返回异常</li></ul></li><li>结果消费：结果消费系列函数只对结果执行Action，而不返回新的计算值<ul><li>thenAccept系列（<code>thenAccept thenAcceptAsync</code>）：对单个结果进行消费</li><li>thenAcceptBoth系列（<code>thenAcceptBoth thenAcceptBothAsync</code>）：对两个结果进行消费<ul><li>当两个&nbsp;CompletionStage&nbsp;都正常完成计算的时候，就会执行提供的 action 消费两个异步的结果</li></ul></li><li>thenRun系列（<code>thenRun thenRunAsync</code>）：不关心结果，只对结果执行Action<ul><li>也是对线程任务结果的一种消费函数，与 thenAccept 不同的是，thenRun&nbsp;会在上一阶段&nbsp;CompletableFuture&nbsp;计算完成的时候执行一个Runnable，Runnable 并不使用该&nbsp;CompletableFuture&nbsp;计算的结果</li></ul></li></ul></li><li>依赖关系（结果转换）<ul><li>thenApply()&nbsp;把前面异步任务的结果，交给后面的Function<ul><li>thenApply&nbsp;转换的是泛型中的类型，返回的是同一个CompletableFuture</li></ul></li><li>thenCompose() 用来连接两个有依赖关系的任务，结果由第二个任务返回<ul><li>thenCompose&nbsp;将内部的&nbsp;CompletableFuture&nbsp;调用展开来并使用上一个CompletableFutre&nbsp;调用的结果在下一步的&nbsp;CompletableFuture&nbsp;调用中进行运算，是生成一个新的CompletableFuture</li></ul></li></ul></li><li>and聚合关系（结果组合）：合并线程任务的结果，并进一步处理<ul><li>thenCombine 任务合并，有返回值</li><li>thenAccepetBoth 两个任务执行完成后，将结果交给thenAccepetBoth消耗，无返回值</li><li>runAfterBoth 两个任务都执行完成后，执行下一步操作（Runnable）</li></ul></li><li>or聚合关系（任务交互：将两个线程任务获取结果的速度相比较，按一定的规则进行下一步处理）<ul><li>applyToEither 两个任务谁执行的快，就使用那一个结果，有返回值</li><li>acceptEither 两个任务谁执行的快，就消费那一个结果，无返回值</li><li>runAfterEither 两个线程任意一个任务执行完成，进行下一步操作 (Runnable)</li><li>runAfterBoth 两个线程任务相比较，两个全部执行完成，才进行下一步操作，不关心运行结果</li></ul></li><li>并行执行（多个 CompletableFuture 并行执行）<ul><li><code>CompletableFuture#anyOf</code> anyOf&nbsp;方法的参数是多个给定的&nbsp;CompletableFuture，当其中的任何一个完成时，方法返回这个&nbsp;CompletableFuture</li><li><code>CompletableFuture#allOf</code> allOf方法用来实现多&nbsp;CompletableFuture&nbsp;的同时返回</li></ul></li><li>异步操作<ul><li>runAsync 无返回值</li><li>supplyAsync 有返回值</li></ul></li></ul></li></ul><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-Future.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;FutureTask: 存储任务的处理结果，更新任务的状态
&lt;ul&gt;
&lt;li&gt;既可以被当做Runnable来执行，也可以被当做Future来获取Callable的返回结果&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Future: &lt;code&gt;cancel isC</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-ForkJoin</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-forkjoin/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-forkjoin/</id>
    <published>2024-06-13T03:35:49.000Z</published>
    <updated>2024-06-13T03:36:36.001Z</updated>
    
    <content type="html"><![CDATA[<ul><li>线程数计算方法：<code>线程数&nbsp;=&nbsp;CPU&nbsp;核心数&nbsp;*（1+平均等待时间/平均工作时间）</code><ul><li>CPU密集型任务：线程数为 CPU&nbsp;核心数的&nbsp;1~2&nbsp;倍</li><li>IO密集型任务：线程数一般会大于&nbsp;CPU&nbsp;核心数很多倍</li></ul></li><li>分治算法：将一个规模为N的问题分解为K个规模较小的子问题，这些子问题相互独立且与原问题性质相同。求出子问题的解，就可得到原问题的解<ul><li>步骤：分解；求解；合并</li><li>在分治法中，子问题一般是相互独立的，因此，经常通过递归调用算法来求解子问题</li></ul></li><li>Fork/Join：ForkJoinPool允许其他线程向它提交任务，并根据设定将这些任务拆分为粒度更细的子任务，这些子任务将由ForkJoinPool内部的工作线程来并行执行，并且工作线程之间可以窃取彼此之间的任务<ul><li>最适合计算密集型任务，而且最好是非阻塞任务</li></ul></li><li>ForkJoinPool 是用于执行&nbsp;ForkJoinTask&nbsp;任务的执行池<ul><li>维护了一个队列数组&nbsp;WorkQueue（WorkQueue[]），这样在提交任务和线程任务的时候大幅度减少碰撞</li><li>四个核心参数：并行数、工作线程的创建、异常处理和模式指定</li><li>任务提交<ul><li>提交异步执行：execute</li><li>等待并获取结果：invoke</li><li>提交执行获取Future结果：submit</li></ul></li></ul></li><li>ForkJoinTask 定义了任务执行时的具体逻辑和拆分逻辑，继承了Future接口<ul><li>fork() 提交任务：用于向当前任务所运行的线程池中提交任务。如果当前线程是ForkJoinWorkerThread类型，将会放入该线程的工作队列，否则放入common线程池的工作队列中</li><li>join() 获取任务执行结果：调用join()时，将阻塞当前线程直到对应的子任务完成运行并返回结果</li><li>通常情况下我们不需要直接继承ForkJoinTask类，而只需要继承它的子类<ul><li>RecursiveAction 用于递归执行但不需要返回结果的任务</li><li>RecursiveTask 用于递归执行需要返回结果的任务</li><li><code>CountedCompleter&lt;T&gt;</code> 在任务完成执行后会触发执行一个自定义的钩子函数</li></ul></li></ul></li><li>ForkJoinPool&nbsp;的工作原理<ul><li>ForkJoinPool&nbsp;内部有多个工作队列，当我们通过&nbsp;ForkJoinPool&nbsp;的&nbsp;invoke()&nbsp;或者&nbsp;submit()&nbsp;方法提交任务时，ForkJoinPool&nbsp;根据一定的路由规则把任务提交到一个工作队列中，如果任务在执行过程中会创建出子任务，那么子任务会提交到工作线程对应的工作队列中</li><li>ForkJoinPool&nbsp;的每个工作线程都维护着一个工作队列（WorkQueue），这是一个双端队列（Deque），里面存放的对象是任务（ForkJoinTask）</li><li>每个工作线程在运行中产生新的任务（通常是因为调用了&nbsp;fork()）时，会放入工作队列的top，并且工作线程在处理自己的工作队列时，使用的是&nbsp;LIFO&nbsp;方式，也就是说每次从top取出任务来执行</li><li>每个工作线程在处理自己的工作队列同时，会尝试窃取一个任务，窃取的任务位于其他线程的工作队列的base，也就是说工作线程在窃取其他工作线程的任务时，使用的是FIFO&nbsp;方式</li><li>在遇到&nbsp;join()&nbsp;时，如果需要&nbsp;join&nbsp;的任务尚未完成，则会先处理其他任务，并等待其完成</li><li>在既没有自己的任务，也没有可以窃取的任务时，进入休眠</li></ul></li><li>工作窃取：就是允许空闲线程从繁忙线程的双端队列中窃取任务<ul><li>减少线程竞争任务的可能性：工作线程从它自己的双端队列的头部获取任务。但是，当自己的任务为空时，线程会从其他繁忙线程双端队列的尾部中获取任务</li><li>工作窃取队列（work-stealing&nbsp;queues&nbsp;）：由内部类WorkQueue实现，它是Deques的特殊形式，但仅支持三种操作方式：push、pop和poll（也称为窃取）</li><li>队列的读取有着严格的约束，push和pop仅能从其所属线程调用，而poll则可以从其他线程调用</li></ul></li><li>为什么工作线程总是从头部获取任务，窃取线程从尾部获取任务<ul><li>通过始终选择最近提交的任务，可以增加资源仍分配在CPU缓存中的机会</li><li>窃取者之所以从尾部获取任务，则是为了降低线程之间的竞争可能</li><li>由于任务是可分割的，那队列中较旧的任务最有可能粒度较大，因为它们可能还没有被分割，而空闲的线程则相对更有“精力”来完成这些粒度较大的任务</li></ul></li><li>工作队列 WorkQueue<ul><li>WorkQueue&nbsp;是双向列表，用于任务的有序执行，如果&nbsp;WorkQueue&nbsp;用于自己的执行线程&nbsp;Thread，线程默认将会从尾端选取任务用来执行&nbsp;LIFO</li><li>每个&nbsp;ForkJoinWorkThread&nbsp;都有属于自己的&nbsp;WorkQueue，但不是每个&nbsp;WorkQueue&nbsp;都有对应的&nbsp;ForkJoinWorkThread</li><li>没有&nbsp;ForkJoinWorkThread&nbsp;的&nbsp;WorkQueue&nbsp;保存的是&nbsp;submission，来自外部提交，在 <code>WorkQueues[]</code>&nbsp;的下标是偶数位</li></ul></li><li>ForkJoinWorkThread 是用于执行任务的线程，用于区别使用非&nbsp;ForkJoinWorkThread&nbsp;线程提交task。启动一个该&nbsp;Thread，会自动注册一个&nbsp;WorkQueue&nbsp;到&nbsp;Pool，拥有&nbsp;Thread&nbsp;的&nbsp;WorkQueue&nbsp;只能出现在&nbsp;<code>WorkQueues[]</code>&nbsp;的奇数位</li></ul><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-ForkJoin-1.png" alt=""></p><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-ForkJoin-2.png" alt=""></p><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-ForkJoin-3.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;线程数计算方法：&lt;code&gt;线程数&amp;nbsp;=&amp;nbsp;CPU&amp;nbsp;核心数&amp;nbsp;*（1+平均等待时间/平均工作时间）&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;CPU密集型任务：线程数为 CPU&amp;nbsp;核心数的&amp;nbsp;1~2&amp;nbsp;倍&lt;/li&gt;</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-Disruptor</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-disruptor/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-disruptor/</id>
    <published>2024-06-13T03:33:49.000Z</published>
    <updated>2024-06-13T03:34:41.084Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Disruptor 通过以下设计来解决队列速度慢的问题<ul><li>环形数组结构：为了避免垃圾回收，采用数组而非链表。同时，数组对处理器的缓存机制更加友好（空间局部性原理）</li><li>元素位置定位：数组长度2^n，通过位运算，加快定位的速度。下标采取递增的形式。不用担心index溢出的问题。index是long类型，即使100万QPS的处理速度，也需要30万年才能用完</li><li>无锁设计：每个生产者或者消费者线程，会先申请可以操作的元素在数组中的位置，申请到之后，直接在该位置写入或者读取数据</li><li>利用缓存行填充解决了伪共享的问题</li><li>实现了基于事件驱动的生产者消费者模型（观察者模式）</li></ul></li><li>RingBuffer 数据结构：一个可自定义大小的环形数组，还有一个序列号(sequence)，用以指向下一个可用的元素<ul><li>Disruptor要求设置数组长度为2的n次幂。在知道索引(index)下标的情况下，存与取数组上的元素时间复杂度只有O(1)，而这个index我们可以通过序列号与数组的长度取模来计算得出，index=sequence&nbsp;%&nbsp;entries.length。也可以用位运算来计算效率更高，此时array.length必须是2的幂次方，index=sequece&amp;(entries.length-1)</li><li>当需要覆盖数据时，会执行一个策略，Disruptor给提供多种策略<ul><li>BlockingWaitStrategy策略，常见且默认的等待策略。使用ReentrantLock+Condition实现阻塞，最节省cpu，但高并发场景下性能最差。适合CPU资源紧缺，吞吐量和延迟并不重要的场景</li><li>SleepingWaitStrategy策略，会在循环中不断等待数据。先进行自旋等待如果不成功，则使用Thread.yield()让出CPU,并最终使用LockSupport.parkNanos(1L)进行线程休眠，以确保不占用太多的CPU资源。因此这个策略会产生比较高的平均延时。典型的应用场景就是异步日志</li><li>YieldingWaitStrategy策略，这个策略用于低延时的场合。消费者线程会不断循环监控缓冲区变化，在循环内部使用Thread.yield()让出CPU给别的线程执行时间。如果需要一个高性能的系统，并且对延时比较有严格的要求，可以考虑这种策略</li><li>BusySpinWaitStrategy策略:&nbsp;采用死循环，消费者线程会尽最大努力监控缓冲区的变化。对延时非常苛刻的场景使用，cpu核数必须大于消费者线程数量。推荐在线程绑定到固定的CPU的场景下使用</li></ul></li></ul></li><li>一个生产者单线程写数据的流程<ol><li>申请写入m个元素</li><li>若是有m个元素可以写入，则返回最大的序列号。这里主要判断是否会覆盖未读的元素</li><li>若是返回的正确，则生产者开始写入元素</li></ol></li><li>多个生产者写数据的流程<ol><li>申请写入m个元素</li><li>若是有m个元素可以写入，则返回最大的序列号。每个生产者会被分配一段独享的空间</li><li>生产者写入元素，写入元素的同时设置available&nbsp;Buffer里面相应的位置，以标记自己哪些位置是已经写入成功的</li></ol><ul><li>问题<ul><li>如何防止多个线程重复写同一个元素：每个线程通过CAS获取不同的一段数组空间进行操作</li><li>如何防止读取的时候，读到还未写的元素：引入了一个与Ring&nbsp;Buffer大小相同的buffer：available&nbsp;Buffer。当某个位置写入成功的时候，便把availble&nbsp;Buffer相应的位置置位，标记为写入成功。读取的时候，会遍历available&nbsp;Buffer，来判断元素是否已经就绪</li></ul></li></ul></li><li>生产者多线程写入的情况下读数据<ol><li>申请读取到序号n</li><li>若writer&nbsp;cursor&nbsp;&gt;=&nbsp;n，这时仍然无法确定连续可读的最大下标。从reader&nbsp;cursor开始读取available&nbsp;Buffer，一直查到第一个不可用的元素，然后返回最大连续可读元素的位置</li><li>消费者读取元素</li></ol></li></ul><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-Disruptor-1.png" alt=""></p><hr><ul><li>Disruptor核心概念<ul><li>RingBuffer（环形缓冲区）：基于数组的内存级别缓存，是创建sequencer(序号)与定义WaitStrategy(拒绝策略)的入口</li><li>Disruptor（总体执行入口）：对RingBuffer的封装，持有RingBuffer、消费者线程池Executor、消费之集合ConsumerRepository等引用</li><li>Sequence（序号分配器）：对RingBuffer中的元素进行序号标记，通过顺序递增的方式来管理进行交换的数据(事件/Event)，一个Sequence可以跟踪标识某个事件的处理进度，同时还能消除伪共享</li><li>Sequencer（数据传输器）：Sequencer里面包含了Sequence，是Disruptor的核心，Sequencer有两个实现类：SingleProducerSequencer(单生产者实现)、MultiProducerSequencer(多生产者实现)，Sequencer主要作用是实现生产者和消费者之间快速、正确传递数据的并发算法</li><li>SequenceBarrier（消费者屏障）：用于控制RingBuffer的Producer和Consumer之间的平衡关系，并且决定了Consumer是否还有可处理的事件的逻辑</li><li>WaitStrategy（消费者等待策略）：决定了消费者如何等待生产者将Event生产进Disruptor，WaitStrategy有多种实现策略</li><li>Event：从生产者到消费者过程中所处理的数据单元，Event由使用者自定义</li><li>EventHandler：由用户自定义实现，就是我们写消费者逻辑的地方，代表了Disruptor中的一个消费者的接口</li><li>EventProcessor：这是个事件处理器接口，实现了Runnable，处理主要事件循环，处理Event，拥有消费者的Sequence</li></ul></li></ul><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-Disruptor-2.png" alt=""></p><hr><p>Disruptor的使用</p><pre class="line-numbers language-markup" data-language="markup"><code class="language-markup"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>com.lmax<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>disruptor<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ol><li>创建 Event（消息载体/事件）和 EventFactory（事件工厂） <code>implements&nbsp;EventFactory</code></li><li>创建消息（事件）生产者 <code>RingBuffer#publish</code></li><li>创建消费者 <code>implements&nbsp;EventHandler</code><ul><li><code>disruptor#handleEventsWithWorkerPool</code> 多消费者下一个消息只会被一个消费者消费，要实现 <code>WorkHandler</code></li><li>顺序消费：<code>disruptor.handleEventsWith().then().then()</code></li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Disruptor 通过以下设计来解决队列速度慢的问题
&lt;ul&gt;
&lt;li&gt;环形数组结构：为了避免垃圾回收，采用数组而非链表。同时，数组对处理器的缓存机制更加友好（空间局部性原理）&lt;/li&gt;
&lt;li&gt;元素位置定位：数组长度2^n，通过位运算，加快定位的速度。下标采</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-CAS &amp; Atomic</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-cas-atomic/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-cas-atomic/</id>
    <published>2024-06-13T03:30:49.000Z</published>
    <updated>2024-06-13T03:33:00.749Z</updated>
    
    <content type="html"><![CDATA[<ul><li>CAS</li><li>Atomic</li><li>LongAdder</li></ul><hr><ul><li>CAS（Compare&nbsp;And&nbsp;Swap，比较并交换）：针对一个变量，首先比较它的内存值与某个期望值是否相同，如果相同，就给它赋一个新值<ul><li>一个不可分割的原子操作，并且其原子性是直接在硬件层面得到保障的</li></ul></li><li>CAS缺陷<ul><li>自旋&nbsp;CAS&nbsp;长时间地不成功，则会给&nbsp;CPU&nbsp;带来非常大的开销</li><li>只能保证一个共享变量原子操作</li><li>ABA&nbsp;问题<ul><li>加时间戳的原子操作类（AtomicStampedReference&lt;V&gt;）</li><li>AtomicMarkableReference（只关心是否修改过，而不关心修改次数）</li></ul></li></ul></li></ul><hr><ul><li>java.util.concurrent.atomic<ul><li>基本类型：AtomicInteger、AtomicLong、AtomicBoolean</li><li>引用类型：AtomicReference、AtomicStampedRerence、AtomicMarkableReference</li><li>数组类型：AtomicIntegerArray、AtomicLongArray、AtomicReferenceArray</li><li>对象属性原子修改器：AtomicIntegerFieldUpdater、AtomicLongFieldUpdater、AtomicReferenceFieldUpdater<ul><li>字段必须是volatile类型的</li><li>调用者能够直接操作对象字段（修饰符public/protected/default/private），但是子类是不能操作父类的字段，尽管子类可以访问父类的字段</li><li>只能是实例变量，不能是类变量，也就是说不能加static关键字</li><li>只能是可修改变量，不能使final变量，因为final的语义就是不可修改（实际上volatile和final的语义本来就是冲突的）</li><li>不能修改其包装类型，如果要修改包装类型就需要使用AtomicReferenceFieldUpdater</li></ul></li><li>原子类型累加器（jdk1.8增加的类）：DoubleAccumulator、DoubleAdder、LongAccumulator、LongAdder、Striped64</li></ul></li></ul><hr><ul><li>LongAdder解决高并发环境下AtomicLong的自旋瓶颈问题：尽量减少热点冲突，不到最后万不得已，尽量将CAS操作延迟<ul><li>基本思路就是分散热点，将value值分散到一个数组中，不同线程会命中到数组的不同槽中，各个线程只对自己槽中的那个值进行CAS操作</li><li>如果要获取真正的long值，只要将各个槽中的变量值累加返回</li></ul></li><li>LongAdder的内部结构：一个base变量，一个Cell[]数组<ul><li>base变量：非竞态条件下，直接累加到该变量上</li><li>Cell[]数组：竞态条件下，累加到各个线程自己的槽Cell[i]中</li></ul></li><li>LongAdder#add<ul><li>CPU核数，用来决定槽数组的大小，大小为2的次幂</li><li>没有遇到并发竞争时，直接使用base累加数值<ul><li>只有从未出现过并发冲突的时候，base基数才会使用到，一旦出现了并发冲突，之后所有的操作都只针对Cell[]数组中的单元Cell</li></ul></li><li>初始化cells数组时，必须要保证cells数组只能被初始化一次（即只有一个线程能对cells初始化），他竞争失败的线程会讲数值累加到base上<ul><li>如果Cell[]数组未初始化，会调用父类的longAccumelate去初始化Cell[]，如果Cell[]已经初始化但是冲突发生在Cell单元内，则也调用父类的longAccumelate，此时可能就需要对Cell[]扩容了</li></ul></li><li>累加到各个线程自己的槽Cell[i]中</li><li><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-CAS-Atomic-1.png" alt=""></li></ul></li><li>Striped64#longAccumulate<ul><li><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-CAS-Atomic-2.png" alt=""></li></ul></li><li>LongAdder#sum：返回累加的和，也就是"当前时刻"的计数值（计算总和时没有对Cell数组进行加锁）<ul><li>高并发时，除非全局加锁，否则得不到程序运行中某个时刻绝对准确的值</li><li>此返回值可能不是绝对准确的，因为调用这个方法时还有其他线程可能正在进行计数累加，方法的返回时刻和调用时刻不是同一个点，在有并发的情况下，这个值只是近似准确的计数值</li></ul></li><li>LongAccumulator是LongAdder的增强版：提供了自定义的函数操作（LongBinaryOperator）<ul><li>内部原理和LongAdder几乎完全一样，都是利用了父类Striped64的longAccumulate方法</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;CAS&lt;/li&gt;
&lt;li&gt;Atomic&lt;/li&gt;
&lt;li&gt;LongAdder&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;CAS（Compare&amp;nbsp;And&amp;nbsp;Swap，比较并交换）：针对一个变量，首先比较它的内存值与某个期望值是否相同，如果</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-BlockingQueue</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-blockingqueue/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-blockingqueue/</id>
    <published>2024-06-13T03:29:49.000Z</published>
    <updated>2024-06-13T03:29:24.351Z</updated>
    
    <content type="html"><![CDATA[<ul><li>BlockingQueue和JDK集合包中的Queue接口兼容，同时在其基础上增加了阻塞功能<ul><li>offer(E&nbsp;e)：如果队列没满，返回true，如果队列已满，返回false（不阻塞）</li><li>offer(E&nbsp;e,&nbsp;long&nbsp;timeout,&nbsp;TimeUnit&nbsp;unit)：可以设置阻塞时间，如果队列已满，则进行阻塞。超过阻塞时间，则返回false</li><li>put(E&nbsp;e)：队列没满的时候是正常的插入，如果队列已满，则阻塞，直至队列空出位置</li><li>poll()：如果有数据，出队，如果没有数据，返回null&nbsp;&nbsp;&nbsp;（不阻塞）</li><li>poll(long&nbsp;timeout,&nbsp;TimeUnit&nbsp;unit)：可以设置阻塞时间，如果没有数据，则阻塞，超过阻塞时间，则返回null</li><li>take()：队列里有数据会正常取出数据并删除；但是如果队列里无数据，则阻塞，直到队列里有数据</li></ul></li><li>ArrayBlockingQueue 基于数组的有界阻塞队列<ul><li>利用&nbsp;ReentrantLock&nbsp;实现线程安全（只能有一个线程可以进行入队或者出队操作）</li><li>利用了Lock锁的Condition通知机制进行阻塞控制（notEmpty、notFull）</li></ul></li><li>LinkedBlockingQueue  基于单链表的无界阻塞队列<ul><li>读写分离（只能从head取元素，从tail添加元素）：采用两把锁（putLock、takeLock）的锁分离技术实现入队出队互不阻塞</li></ul></li><li>SynchronousQueue 没有数据缓冲（容量为0）的 BlockingQueue ：它所做的就是直接传递<ul><li>每次取数据都要先阻塞，直到有数据被放入</li><li>每次放数据的时候也会阻塞，直到有消费者来取</li></ul></li><li>PriorityBlockingQueue 基于数组（二叉堆）的无界优先级阻塞队列（默认长度是11，虽然指定了数组的长度，但是可以无限的扩充）<ul><li>每次出队都返回优先级别最高的或者最低的元素（不能保证同优先级元素的顺序）</li><li>默认情况下元素采用自然顺序升序排序（也可以通过构造函数来指定Comparator来对元素进行排序）</li></ul></li><li>DelayQueue 基于优先队列&nbsp;PriorityQueue&nbsp;的支持延时获取元素的无界队阻塞队列<ul><li>元素必须实现&nbsp;Delayed&nbsp;接口（&nbsp;Delayed&nbsp;接口又继承了&nbsp;Comparable&nbsp;接口）</li><li>在创建元素时可以指定多久才可以从队列中获取当前元素，只有在延迟期满时才能从队列中提取元素</li><li>按照延迟时间的长短来排序，下一个即将执行的任务会排到队列的最前面</li><li>获取元素，先获取到锁对象，然后获取最早过期的元素（但是并不从队列中弹出元素）<ul><li>判断最早过期元素是否为空（如果为空则直接让当前线程无限期等待状态，并且让出当前锁对象）</li><li>如果不为空：获取最早过期元素的剩余过期时间（如果已经过期则直接返回当前元素）</li><li>如果没有过期（剩余时间还存在），则先获取Leader对象，如果Leader已经有线程在处理，则当前线程进行无限期等待，如果Leader为空，则首先将Leader设置为当前线程，并且让当前线程等待剩余时间</li><li>最后将Leader线程设置为空，并且队列有内容则唤醒一个等待的队列</li></ul></li></ul></li><li>线程池对于阻塞队列的选择<ul><li>FixedThreadPool（SingleThreadExecutor&nbsp;同理）选取的是&nbsp;LinkedBlockingQueue</li><li>CachedThreadPool&nbsp;选取的是&nbsp;SynchronousQueue</li><li>ScheduledThreadPool（SingleThreadScheduledExecutor同理）选取的是延迟队列</li></ul></li><li>选择策略：<ul><li>功能&nbsp;PriorityBlockingQueue 优先级；DelayQueue 延时</li><li>容量：SynchronousQueue 容量是0；DelayQueue 容量是Integer.MAX_VALUE</li><li>能否扩容：ArrayBlockingQueue 不能扩容；PriorityBlockingQueue 可以动态扩容</li><li>内存结构：ArrayBlockingQueue 数组的空间利用率更高</li><li>性能：LinkedBlockingQueue 读写分离；SynchronousQueue 直接传递</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;BlockingQueue和JDK集合包中的Queue接口兼容，同时在其基础上增加了阻塞功能
&lt;ul&gt;
&lt;li&gt;offer(E&amp;nbsp;e)：如果队列没满，返回true，如果队列已满，返回false（不阻塞）&lt;/li&gt;
&lt;li&gt;offer(E&amp;nbsp;e,&amp;</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-AQS</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-aqs/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-aqs/</id>
    <published>2024-06-13T03:28:49.000Z</published>
    <updated>2024-06-13T03:28:49.092Z</updated>
    
    <content type="html"><![CDATA[<ul><li>AQS具备的特性：阻塞等待队列；共享/独占；公平/非公平；可重入；允许中断</li><li>AQS内部维护属性<code>volatile&nbsp;int&nbsp;state</code>：state 表示资源的可用状态<ul><li>State三种访问方式：<code>getState()&nbsp;setState()&nbsp;compareAndSetState()</code></li></ul></li><li>AQS定义两种资源共享方式<ul><li>Exclusive-独占，只有一个线程能执行，如ReentrantLock</li><li>Share-共享，多个线程可以同时执行，如Semaphore/CountDownLatch</li></ul></li><li>AQS定义两种队列<ul><li>同步等待队列：&nbsp;主要用于维护获取锁失败时入队的线程</li><li>条件等待队列：&nbsp;调用await()的时候会释放锁，然后线程会加入到条件队列，调用signal()唤醒的时候会把条件队列中的线程节点移动到同步队列中，等待再次获得锁</li></ul></li><li>AQS&nbsp;定义了5个队列中节点状态<ul><li>值为0，初始化状态，表示当前节点在sync队列中，等待着获取锁</li><li>CANCELLED，值为1，表示当前的线程被取消</li><li>SIGNAL，值为-1，表示当前节点的后继节点包含的线程需要运行，也就是unpark</li><li>CONDITION，值为-2，表示当前节点在等待condition，也就是在condition队列中</li><li>PROPAGATE，值为-3，表示当前场景下后续的acquireShared能够得以执行</li></ul></li><li>自定义同步器实现时主要实现以下几种方法：只需要实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了<ul><li>isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它</li><li>tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false</li><li>tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false</li><li>tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源</li><li>tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false</li></ul></li><li>同步等待队列：AQS当中的同步等待队列也称CLH队列，一种基于双向链表数据结构的队列，是FIFO先进先出线程等待队列，Java中的CLH队列是原CLH队列的一个变种，线程由原自旋机制改为阻塞机制。AQS&nbsp;依赖CLH同步队列来完成同步状态的管理<ul><li>当前线程如果获取同步状态失败时，AQS则会将当前线程已经等待状态等信息构造成一个节点（Node）并将其加入到CLH同步队列，同时会阻塞当前线程</li><li>当同步状态释放时，会把首节点唤醒（公平锁），使其再次尝试获取同步状态</li><li>通过signal或signalAll将条件队列中的节点转移到同步队列。（由条件队列转化为同步队列）</li></ul></li><li>条件等待队列：AQS中条件队列是使用单向列表保存的，用nextWaiter来连接<ul><li>调用await方法阻塞线程</li><li>当前线程存在于同步队列的头结点，调用await方法进行阻塞（从同步队列转化到条件队列）</li></ul></li><li>Condition接口<ul><li>调用<code>Condition#await</code>方法会释放当前持有的锁，然后阻塞当前线程，同时向Condition队列尾部添加一个节点，所以调用<code>Condition#await</code>方法的时候必须持有锁</li><li>调用<code>Condition#signal</code>方法会将Condition队列的首节点移动到阻塞队列尾部，然后唤醒因调用<code>Condition#await</code>方法而阻塞的线程(唤醒之后这个线程就可以去竞争锁了)，所以调用<code>Condition#signal</code>方法的时候必须持有锁，持有锁的线程唤醒被因调用<code>Condition#await</code>方法而阻塞的线程</li></ul></li></ul><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-AQS.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;AQS具备的特性：阻塞等待队列；共享/独占；公平/非公平；可重入；允许中断&lt;/li&gt;
&lt;li&gt;AQS内部维护属性&lt;code&gt;volatile&amp;nbsp;int&amp;nbsp;state&lt;/code&gt;：state 表示资源的可用状态
&lt;ul&gt;
&lt;li&gt;State三种访</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-线程休眠与唤醒</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-xian-cheng-xiu-mian-yu-huan-xing/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-xian-cheng-xiu-mian-yu-huan-xing/</id>
    <published>2024-06-13T03:27:49.000Z</published>
    <updated>2024-06-13T03:27:44.646Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Thread#sleep<ul><li>必须指定休眠时间</li><li>休眠时线程状态为TIMED_WAITING</li><li>需要捕获InterruptedException异常</li><li>不会释放持有的锁</li></ul></li><li>Object#wait<ul><li>可以通过notify唤醒，notify必须在wait之后执行，否则会丢失唤醒信号</li><li>休眠时线程状态为WAITING</li><li>需要捕获InterruptedException异常</li><li>会释放持有的锁</li><li>必须在synchronized内使用</li><li>无法唤醒指定的线程</li></ul></li><li>LockSupport#park<ul><li>通过二元信号量实现的阻塞</li><li>休眠时线程状态为WAITING</li><li>无须捕获InterruptedException异常，但是也会响应中断</li><li>不会释放持有的锁</li><li>可以通过unpark唤醒，unpark方法可以比park先执行，不会丢失唤醒信号</li><li>可以指定任何线程进行唤醒</li></ul></li></ul><hr><ul><li>LockSupport就是通过控制变量_counter来对线程阻塞唤醒进行控制的，原理有点类似于信号量机制<ul><li>当调用park()方法时，会将_counter置为0，同时判断前值，小于1说明前面被unpark过，则直接退出，否则将使该线程阻塞</li><li>当调用unpark()方法时，会将_counter置为1，同时判断前值，小于1会进行线程唤醒，否则直接退出<ul><li>为什么唤醒两次后阻塞两次会阻塞线程：连续调用两次unpark和调用一次unpark效果一样（_counter最大为1）</li></ul></li></ul></li><li>pthread_cond_wait()&nbsp;&nbsp;用于阻塞当前线程，等待别的线程使用&nbsp;pthread_cond_signal()&nbsp;或pthread_cond_broadcast来唤醒它<ul><li>pthread_cond_signal()可以唤醒至少一个线程；发送一个信号给另外一个正在处于阻塞等待状态的线程，使其脱离阻塞状态，继续执行</li><li>pthread_cond_broadcast()则是唤醒等待该条件满足的所有线程</li></ul></li></ul><p><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-%E7%BA%BF%E7%A8%8B%E4%BC%91%E7%9C%A0%E4%B8%8E%E5%94%A4%E9%86%92.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Thread#sleep
&lt;ul&gt;
&lt;li&gt;必须指定休眠时间&lt;/li&gt;
&lt;li&gt;休眠时线程状态为TIMED_WAITING&lt;/li&gt;
&lt;li&gt;需要捕获InterruptedException异常&lt;/li&gt;
&lt;li&gt;不会释放持有的锁&lt;/li&gt;
&lt;/ul&gt;
&lt;/li</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-线程</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-xian-cheng/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-xian-cheng/</id>
    <published>2024-06-13T03:26:49.000Z</published>
    <updated>2024-06-13T03:26:45.217Z</updated>
    
    <content type="html"><![CDATA[<ul><li>线程共有六种状态：NEW（初始化状态）；RUNNABLE（可运行状态+运行状态）；BLOCKED（阻塞状态）；WAITING（无时限等待）；TIMED_WAITING（有时限等待）；TERMINATED（终止状态）<ul><li><img src="/static/architect/Java%E5%B9%B6%E5%8F%91-%E7%BA%BF%E7%A8%8B.png" alt=""></li></ul></li><li>中断机制：中断机制是一种协作机制，也就是说通过中断并不能直接终止另一个线程，而需要被中断的线程自己处理<ul><li>interrupt()：&nbsp;将线程的中断标志位设置为true，不会停止线程</li><li>isInterrupted()：判断当前线程的中断标志位是否为true，不会清除中断标志位</li><li>Thread.interrupted()：判断当前线程的中断标志位是否为true，并清除中断标志位，重置为false</li><li>InterruptedException&nbsp;异常，同时清除中断信号，将中断标记位设置成&nbsp;false</li></ul></li><li>管道输入输出流：PipedOutputStream、PipedInputStream、PipedReader和PipedWriter</li></ul><pre class="line-numbers language-java" data-language="java"><code class="language-java"><span class="token class-name">PipedWriter</span> out <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">PipedWriter</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token class-name">PipedReader</span> in <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">PipedReader</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">// 将输出流和输入流进行连接，否则在使用时会抛出IOException</span>out<span class="token punctuation">.</span><span class="token function">connect</span><span class="token punctuation">(</span>in<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">// 在不同的线程里使用out和in进行通讯</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;线程共有六种状态：NEW（初始化状态）；RUNNABLE（可运行状态+运行状态）；BLOCKED（阻塞状态）；WAITING（无时限等待）；TIMED_WAITING（有时限等待）；TERMINATED（终止状态）
&lt;ul&gt;
&lt;li&gt;&lt;img src=&quot;/sta</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Java并发-设计模式</title>
    <link href="https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-she-ji-mo-shi/"/>
    <id>https://jxch.github.io/2024/06/13/architect/java-bing-fa/java-bing-fa-she-ji-mo-shi/</id>
    <published>2024-06-13T03:25:49.000Z</published>
    <updated>2024-06-13T03:25:40.064Z</updated>
    
    <content type="html"><![CDATA[<ul><li>终止线程的设计模式<ul><li>Two-phase&nbsp;Termination（两阶段终止）模式：终止标志位</li></ul></li><li>避免共享的设计模式<ul><li>Immutability模式：只读</li><li>Copy-on-Write模式：写时复制</li><li>Thread-Specific&nbsp;Storage&nbsp;模式：线程本地存储 ThreadLocal</li></ul></li><li>多线程版本的 if 模式<ul><li>Guarded&nbsp;Suspension 模式（Guarded&nbsp;Wait&nbsp;模式、Spin&nbsp;Lock&nbsp;模式）：一个线程需要等待另外的线程完成后继续下一步操作</li><li>Balking 模式：一个线程发现另一个线程已经做了某一件相同的事，那么本线程就无需再做了，直接结束返回</li></ul></li><li>多线程分工模式<ul><li>Thread-Per-Message&nbsp;模式：为每个任务分配一个独立的线程</li><li>Worker&nbsp;Thread 模式：线程池</li><li>生产者-消费者模式：核心是一个任务队列<ul><li>过饱问题：生产者生产的速度大于消费者消费的速度<ul><li>消费者每天能处理的量比生产者生产的少：消费者加机器</li><li>消费者每天能处理的量比生产者生产的多：适当的加大队列</li><li>系统高峰期生产者速度太快：生产者限流</li></ul></li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;终止线程的设计模式
&lt;ul&gt;
&lt;li&gt;Two-phase&amp;nbsp;Termination（两阶段终止）模式：终止标志位&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;避免共享的设计模式
&lt;ul&gt;
&lt;li&gt;Immutability模式：只读&lt;/li&gt;
&lt;li&gt;Cop</summary>
      
    
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/categories/Java-%E5%B9%B6%E5%8F%91/"/>
    
    
    <category term="Java-并发" scheme="https://jxch.github.io/tags/Java-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch-Ingest Pipeline &amp; Painless Script</title>
    <link href="https://jxch.github.io/2024/06/13/architect/elasticsearch/elasticsearch-ingest-pipeline-painless-script/"/>
    <id>https://jxch.github.io/2024/06/13/architect/elasticsearch/elasticsearch-ingest-pipeline-painless-script/</id>
    <published>2024-06-13T03:21:49.000Z</published>
    <updated>2024-06-13T03:23:10.035Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Ingest&nbsp;Node &amp; Pipeline&nbsp;&amp;&nbsp;Processor<ul><li>Ingest&nbsp;Node</li><li>Pipeline&nbsp;&amp;&nbsp;Processor</li><li>内置的 Processors</li><li>创建 pipeline</li><li>使用 pipeline 更新数据</li><li>借助 update_by_query 更新已存在的文档</li><li>Ingest&nbsp;Node&nbsp;VS&nbsp;Logstash</li></ul></li><li>Painless</li></ul><hr><h2 id="Ingest-Node-Pipeline-Processor">Ingest&nbsp;Node &amp; Pipeline&nbsp;&amp;&nbsp;Processor</h2><ul><li>应用场景：&nbsp;修复与增强写入数据</li><li>Ingest&nbsp;Node<ul><li>默认配置下，每个节点都是 Ingest&nbsp;Node（Elasticsearch&nbsp;5.0后）</li><li>具有预处理数据的能力，可拦截lndex或&nbsp;Bulk&nbsp;API的请求</li><li>对数据进行转换，并重新返回给Index或&nbsp;Bulk&nbsp;APl</li><li>无需Logstash，就可以进行数据的预处理<ul><li>为某个字段设置默认值</li><li>重命名某个字段的字段名</li><li>对字段值进行Split&nbsp;操作</li><li>支持设置Painless脚本，对数据进行更加复杂的加工</li></ul></li></ul></li><li>Pipeline&nbsp;&amp;&nbsp;Processor<ul><li>Pipeline：管道会对通过的数据(文档)，按照顺序进行加工</li><li>Processor：Elasticsearch&nbsp;对一些加工的行为进行了抽象包装<ul><li>Elasticsearch&nbsp;有很多内置的Processors，也支持通过插件的方式，实现自己的Processor</li></ul></li><li><img src="/static/architect/ElasticSearch-IngestPipeline-PainlessScript.png" alt=""></li></ul></li><li>内置的 Processors<ul><li>Split&nbsp;Processor&nbsp;：&nbsp;将给定字段值分成一个数组</li><li>Remove&nbsp;/&nbsp;Rename&nbsp;Processor&nbsp;：移除一个重命名字段</li><li>Append&nbsp;：&nbsp;为商品增加一个新的标签</li><li>Convert：将商品价格，从字符串转换成float&nbsp;类型</li><li>Date&nbsp;/&nbsp;JSON：日期格式转换，字符串转JSON对象</li><li>Date&nbsp;lndex&nbsp;Name&nbsp;Processor︰将通过该处理器的文档,分配到指定时间格式的索引中</li><li>Fail&nbsp;Processor︰一旦出现异常，该Pipeline&nbsp;指定的错误信息能返回给用户</li><li>Foreach&nbsp;Process︰数组字段，数组的每个元素都会使用到一个相同的处理器</li><li>Grok&nbsp;Processor︰日志的日期格式切割</li><li>Gsub&nbsp;/&nbsp;Join&nbsp;/&nbsp;Split︰字符串替换│数组转字符串/字符串转数组</li><li>Lowercase&nbsp;/&nbsp;upcase︰大小写转换</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest"># 测试 split tagsPOST _ingest/pipeline/_simulate{"pipeline":{"description":"to split blog tags","processors": [ {"split":{"field":"tags","separator":","}}]},"docs": [  {"_index": "index","_id": "id","_source": {   "title": "Introducing big data......",    "tags": "hadoop,elasticsearch,spark",    "content": "You konw, for big data"}},  {"_index":"index","_id":"idxx","_source":{   "title":"Introducing cloud computering",    "tags":"openstack,k8s",   "content":"You konw, for cloud"}}]}# 同时为文档增加一个字段POST _ingest/pipeline/_simulate{"pipeline":{"description":"to split blog tags","processors":[   {"split":{"field":"tags","separator":","}},   {"set":{"field":"views","value":0}}]}, "docs": [  {"_index":"index","_id":"id","_source":{   "title":"Introducing big data......",   "tags":"hadoop,elasticsearch,spark",   "content":"You konw, for big data"}},  {"_index":"index","_id":"idxx","_source":{   "title":"Introducing cloud computering",    "tags":"openstack,k8s",   "content":"You konw, for cloud"}}]}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>创建 pipeline</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest"># 为 ES 添加一个 PipelinePUT _ingest/pipeline/blog_pipeline {"description":"a blog pipeline","processors":[  {"split":{"field":"tags","separator":","}},  {"set":{"field":"views","value":0}}]}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>使用 pipeline 更新数据</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest"># 使用 pipeline 更新数据PUT tech_blogs/_doc/2?pipeline=blog_pipeline {"title":"Introducing cloud computering", "tags":"openstack,k8s", "content":"You konw, for cloud"}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>借助 update_by_query 更新已存在的文档</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">#增加update_by_query的条件POST tech_blogs/_update_by_query?pipeline=blog_pipeline {"query":{"bool":{"must_not":{"exists":{"field":"views"}}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><ul><li>Ingest&nbsp;Node&nbsp;VS&nbsp;Logstash</li></ul><table><thead><tr><th></th><th>Logstash</th><th>Ingest&nbsp;Node</th></tr></thead><tbody><tr><td>数据输入与输出</td><td>支持从不同的数据源读取，并写<br>入不同的数据源</td><td>支持从 ES&nbsp;REST&nbsp;API 获取数据， <br>并且写入 Elasticsearch</td></tr><tr><td>数据缓冲</td><td>实现了简单的数据队列，支持重写</td><td>不支持缓冲</td></tr><tr><td>数据处理</td><td>支持大量的插件，也支持定制开发</td><td>内置的插件，可以开发Plugin进 <br>行扩展 (Plugin更新需要重启)</td></tr><tr><td>配置和使用</td><td>增加了一定的架构复杂度</td><td>无需额外部署</td></tr></tbody></table><hr><h2 id="Painless">Painless</h2><ul><li>Painless 支持所有 Java&nbsp;的数据类型及 Java&nbsp;API 子集</li><li>Painless&nbsp;Script 具备以下特性<ul><li>高性能/安全</li><li>支持显示类型或者动态定义类型</li></ul></li><li>Painless 的用途<ul><li>可以对文档字段进行加工处理<ul><li>更新或删除字段，处理数据聚合操作</li><li>Script&nbsp;Field: 对返回的字段提前进行计算</li><li>Function&nbsp;Score: 对文档的算分进行处理</li></ul></li><li>在lngest&nbsp;Pipeline中执行脚本</li><li>在Reindex&nbsp;APl，Update&nbsp;By&nbsp;Query时，对数据进行处理</li></ul></li><li>通过Painless脚本访问字段<ul><li>Ingestion:  <code>ctx.field_name</code></li><li>Update: <code>ctx._source.field_name</code></li><li>Search&nbsp;&amp;&nbsp;Aggregation: <code>doc["field_name"]</code></li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest"># 增加一个 Script PrcessorPOST _ingest/pipeline/_simulate{"pipeline":{"description":"to split blog tags","processors":[  {"split":{"field":"tags","separator":","}},  {"script":{"source":"""    if(ctx.containsKey("content")){      ctx.content_length = ctx.content.length();     }else{      ctx.content_length=0;    }"""}},  {"set":{"field":"views","value":0}}]}, "docs": [{"_index":"index","_id":"id","_source":{   "title":"Introducing big data......",    "tags":"hadoop,elasticsearch,spark",    "content":"You konw, for big data"}}, {"_index":"index","_id":"idxx","_source":{   "title":"Introducing cloud computering",    "tags":"openstack,k8s",   "content":"You konw, for cloud"}}]}PUT tech_blogs/_doc/1{"title":"Introducing big data......",  "tags":"hadoop,elasticsearch,spark",  "content":"You konw, for big data", "views":0}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">POST tech_blogs/_update/1{"script":{"params":{"new_views":100}, "source":"ctx._source.views += params.new_views"}}# 查看 views 计数POST tech_blogs/_search<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest"># 保存脚本在 Cluster StatePOST _scripts/update_views{"script":{"lang":"painless","source":"ctx._source.views += params.new_views"}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET tech_blogs/_search{"script_fields":{"rnd_views":{"script":{"lang":"painless","source":"""   java.util.Random rnd = new Random();    doc['views'].value+rnd.nextInt(1000);"""}}}, "query":{"match_all":{}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>脚本缓存：脚本编译的开销较大，Elasticsearch会将脚本编译后缓存在Cache&nbsp;中</p><ul><li>Inline&nbsp;scripts和&nbsp;Stored&nbsp;Scripts都会被缓存</li><li>默认缓存100个脚本</li></ul></li><li><p><code>script.cache.max_size</code> 设置最大缓存数</p></li><li><p><code>script.cache.expire</code> 设置缓存超时</p></li><li><p><code>script.max_compilations_rate</code> 默认5分钟最多75次编译 (75/5m)</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Ingest&amp;nbsp;Node &amp;amp; Pipeline&amp;nbsp;&amp;amp;&amp;nbsp;Processor
&lt;ul&gt;
&lt;li&gt;Ingest&amp;nbsp;Node&lt;/li&gt;
&lt;li&gt;Pipeline&amp;nbsp;&amp;amp;&amp;nbsp;Processor&lt;/li</summary>
      
    
    
    
    <category term="ElasticSearch" scheme="https://jxch.github.io/categories/ElasticSearch/"/>
    
    
    <category term="ElasticSearch" scheme="https://jxch.github.io/tags/ElasticSearch/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch-ELK</title>
    <link href="https://jxch.github.io/2024/06/13/architect/elasticsearch/elasticsearch-elk/"/>
    <id>https://jxch.github.io/2024/06/13/architect/elasticsearch/elasticsearch-elk/</id>
    <published>2024-06-13T03:17:49.000Z</published>
    <updated>2024-06-13T03:20:55.352Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Logstash<ul><li>Logstash 配置文件结构</li><li>Logstash 导入数据到 ES</li><li>同步数据库数据到 ES</li></ul></li><li>FileBeat</li><li>ELK（采集 Tomcat 服务器日志）<ul><li>使用FileBeats将日志发送到Logstash</li><li>Logstash输出数据到Elasticsearch（logstash开头的索引）<ul><li>利用Logstash过滤器解析日志<ul><li>使用Grok插件通过模式匹配的方式来识别日志中的数据</li><li>使用mutate插件过滤掉不需要的字段</li><li>使用Date插件将日期格式进行转换</li></ul></li></ul></li><li>输出到Elasticsearch指定索引</li><li>完整的Logstash配置文件</li></ul></li></ul><hr><h2 id="Logstash">Logstash</h2><ul><li>Logstash&nbsp;是免费且开放的服务器端数据处理管道，能够从多个来源采集数据，转换数据，然后将数据发送到存储库</li><li>Pipeline<ul><li>包含了 input-filter-output 三个阶段的处理流程</li><li>插件生命周期管理</li><li>队列管理</li></ul></li><li>Logstash&nbsp;Event<ul><li>数据在内部流转时的具体表现形式：数据在 input&nbsp;阶段被转换为 Event，在&nbsp;output 被转化成目标格式数据</li><li>Event&nbsp;其实是一个 Java&nbsp;Object，在配置文件中，对 Event&nbsp;的属性进行增删改查</li></ul></li><li>Codec&nbsp;(Code&nbsp;/&nbsp;Decode)：将原始数据decode成Event;将Event&nbsp;encode成目标数据<ul><li><img src="/static/architect/ElasticSearch-ELK-1.png" alt=""></li></ul></li><li>Logstash数据传输原理<ul><li>数据采集与输入：Logstash支持各种输入选择，能够以连续的流式传输方式，轻松地从日志、指标、Web应用以及数据存储中采集数据</li><li>实时解析和数据转换：通过Logstash过滤器解析各个事件，识别已命名的字段来构建结构，并将它们转换成通用格式，最终将数据从源端传输到存储库中</li><li>存储与数据导出：Logstash提供多种输出选择，可以将数据发送到指定的地方</li></ul></li><li>Logstash通过管道完成数据的采集与处理，管道配置中包含input、output和filter（可选）插件<ul><li>input和output用来配置输入和输出数据源、filter用来对数据进行过滤或预处理</li><li><img src="/static/architect/ElasticSearch-ELK-2.png" alt=""></li></ul></li><li>Logstash 配置文件结构<ul><li>Logstash的管道配置文件对每种类型的插件都提供了一个单独的配置部分，用于处理管道事件</li><li>每个配置部分可以包含一个或多个插件<ul><li>指定多个filter插件，Logstash会按照它们在配置文件中出现的顺序进行处理</li></ul></li></ul></li><li><code>bin/logstash&nbsp;‐f&nbsp;logstash‐demo.conf</code> 运行</li></ul><pre class="line-numbers language-c" data-language="c"><code class="language-c">input <span class="token punctuation">{</span>  <span class="token constant">stdin</span> <span class="token punctuation">{</span> <span class="token punctuation">}</span> <span class="token punctuation">}</span>filter <span class="token punctuation">{</span>  grok <span class="token punctuation">{</span>    match <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{</span> <span class="token string">"message"</span> <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"%{COMBINEDAPACHELOG}"</span> <span class="token punctuation">}</span>  <span class="token punctuation">}</span>  date <span class="token punctuation">{</span>    match <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span> <span class="token string">"timestamp"</span> <span class="token punctuation">,</span> <span class="token string">"dd/MMM/yyyy:HH:mm:ss Z"</span> <span class="token punctuation">]</span>   <span class="token punctuation">}</span><span class="token punctuation">}</span>output <span class="token punctuation">{</span>  elasticsearch <span class="token punctuation">{</span> hosts <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"localhost:9200"</span><span class="token punctuation">]</span><span class="token punctuation">}</span>   <span class="token constant">stdout</span> <span class="token punctuation">{</span> codec <span class="token operator">=</span><span class="token operator">&gt;</span> rubydebug <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><p>Logstash&nbsp;Queue</p><ul><li><img src="/static/architect/ElasticSearch-ELK-3.png" alt=""><ul><li>In&nbsp;Memory&nbsp;Queue: 进程Crash，机器宕机，都会引起数据的丢失</li><li>Persistent&nbsp;Queue: 机器宕机，数据也不会丢失;&nbsp;数据保证会被消费;&nbsp;可以替代&nbsp;Kafka等消息队列缓冲区的作用</li></ul></li><li><code>queue.type:&nbsp;persisted</code> 默认是 memory</li><li><code>queue.max_bytes:&nbsp;4gb</code></li></ul></li><li><p>Codec&nbsp;Plugin&nbsp;-&nbsp;Multiline</p><ul><li>pattern:&nbsp;设置行匹配的正则表达式</li><li>what&nbsp;:&nbsp;如果匹配成功，那么匹配行属于上一个事件还是下一个事件<ul><li>previous&nbsp;/&nbsp;next</li></ul></li><li>negate&nbsp;:&nbsp;是否对pattern结果取反<ul><li>true&nbsp;/&nbsp;false</li></ul></li></ul></li></ul><pre class="line-numbers language-c" data-language="c"><code class="language-c">input <span class="token punctuation">{</span>  <span class="token constant">stdin</span> <span class="token punctuation">{</span>    codec <span class="token operator">=</span><span class="token operator">&gt;</span> multiline <span class="token punctuation">{</span>      pattern <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"^\s"</span>      what <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"previous"</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span>filter <span class="token punctuation">{</span> <span class="token punctuation">}</span>output <span class="token punctuation">{</span>  <span class="token constant">stdout</span> <span class="token punctuation">{</span> codec <span class="token operator">=</span><span class="token operator">&gt;</span> rubydebug <span class="token punctuation">}</span> <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>Input&nbsp;Plugin&nbsp;-&nbsp;File<ul><li>支持从文件中读取数据，如日志文件</li><li>文件读取需要解决的问题：只被读取一次。重启后需要从上次读取的位置继续 (通过 sincedb&nbsp;实现)</li><li>读取到文件新内容，发现新文件</li><li>文件发生归档操作 （文档位置发生变化，日志 rotation），不能影响当前的内容读取</li></ul></li><li>Filter&nbsp;Plugin：Filter&nbsp;Plugin可以对Logstash&nbsp;Event进行各种处理，例如解析，删除字段，类型转换<ul><li>Date:&nbsp;日期解析</li><li>Dissect:&nbsp;分割符解析</li><li>Grok:&nbsp;正则匹配解析</li><li>Mutate:&nbsp;处理字段。重命名，删除，替换</li><li>Ruby:&nbsp;利用Ruby&nbsp;代码来动态修改Event</li></ul></li><li>Filter&nbsp;Plugin&nbsp;-&nbsp;Mutate<ul><li>Convert&nbsp;:&nbsp;类型转换</li><li>Gsub&nbsp;:&nbsp;字符串替换</li><li>Split&nbsp;/&nbsp;Join&nbsp;/Merge:&nbsp;&nbsp;字符串切割，数组合并字符串，数组合并数组</li><li>Rename:&nbsp;字段重命名</li><li>Update&nbsp;/&nbsp;Replace:&nbsp;字段内容更新替换</li><li>Remove_field:&nbsp;字段删除</li></ul></li><li>Logstash 导入数据到 ES</li></ul><pre class="line-numbers language-c" data-language="c"><code class="language-c">input <span class="token punctuation">{</span>   file <span class="token punctuation">{</span>     path <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"/home/es/logstash‐7.17.3/dataset/movies.csv"</span>    start_position <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"beginning"</span>    sincedb_path <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"/dev/null"</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span>filter <span class="token punctuation">{</span>  csv <span class="token punctuation">{</span>    separator <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">","</span>    columns <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"id"</span><span class="token punctuation">,</span><span class="token string">"content"</span><span class="token punctuation">,</span><span class="token string">"genre"</span><span class="token punctuation">]</span>   <span class="token punctuation">}</span>  mutate <span class="token punctuation">{</span>    split <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{</span> <span class="token string">"genre"</span> <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"|"</span> <span class="token punctuation">}</span>    remove_field <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"path"</span><span class="token punctuation">,</span> <span class="token string">"host"</span><span class="token punctuation">,</span><span class="token string">"@timestamp"</span><span class="token punctuation">,</span><span class="token string">"message"</span><span class="token punctuation">]</span>   <span class="token punctuation">}</span>  mutate <span class="token punctuation">{</span>    split <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">,</span> <span class="token string">"("</span><span class="token punctuation">]</span>    add_field <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{</span> <span class="token string">"title"</span> <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"%{[content][0]}"</span><span class="token punctuation">}</span>     add_field <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{</span> <span class="token string">"year"</span> <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"%{[content][1]}"</span><span class="token punctuation">}</span>   <span class="token punctuation">}</span>  mutate <span class="token punctuation">{</span>    convert <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{</span>      <span class="token string">"year"</span> <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"integer"</span>    <span class="token punctuation">}</span>    strip <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"title"</span><span class="token punctuation">]</span>    remove_field <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"path"</span><span class="token punctuation">,</span> <span class="token string">"host"</span><span class="token punctuation">,</span><span class="token string">"@timestamp"</span><span class="token punctuation">,</span><span class="token string">"message"</span><span class="token punctuation">,</span><span class="token string">"content"</span><span class="token punctuation">]</span>   <span class="token punctuation">}</span><span class="token punctuation">}</span>output <span class="token punctuation">{</span>  elasticsearch <span class="token punctuation">{</span>    hosts <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"http://localhost:9200"</span>     index <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"movies"</span>    document_id <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"%{id}"</span>    user <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"elastic"</span>    password <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"123456"</span>  <span class="token punctuation">}</span>  <span class="token constant">stdout</span> <span class="token punctuation">{</span> <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>同步数据库数据到 ES: 借助 JDBC&nbsp;Input&nbsp;Plugin 将数据从数据库读到 Logstash<ul><li>需要自己提供所需的&nbsp;JDBC&nbsp;Driver</li><li>JDBC&nbsp;Input&nbsp;Plugin&nbsp;支持定时任务&nbsp;Scheduling，其语法来自&nbsp;Rufus-scheduler<ul><li>其扩展了&nbsp;Cron，使用&nbsp;Cron&nbsp;的语法可以完成任务的触发</li></ul></li><li>JDBC&nbsp;Input&nbsp;Plugin&nbsp;支持通过&nbsp;Tracking_column&nbsp;/&nbsp;sql_last_value&nbsp;的方式记录&nbsp;State，最终实现增量的更新</li><li>需要拷贝 jdbc 依赖到 logstash/drivers 目录下</li><li>ES 创建&nbsp;alias，只显示没有被标记&nbsp;deleted 的用户<ul><li>需要数据库表中有 deleted 字段</li></ul></li></ul></li></ul><pre class="line-numbers language-c" data-language="c"><code class="language-c">input <span class="token punctuation">{</span>   jdbc <span class="token punctuation">{</span>     jdbc_driver_library <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"/home/es/logstash‐7.17.3/drivers/mysql‐connector-java‐5.1.49.jar"</span>    jdbc_driver_class <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"com.mysql.jdbc.Driver"</span>    jdbc_connection_string <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"jdbc:mysql://localhost:3306/test?useSSL=false"</span>    jdbc_user <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"root"</span>    jdbc_password <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"123456"</span>    # 启用追踪，如果为true，则需要指定tracking_column    use_column_value <span class="token operator">=</span><span class="token operator">&gt;</span> true    # 指定追踪的字段，    tracking_column <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"last_updated"</span>    # 追踪字段的类型，目前只有数字<span class="token punctuation">(</span>numeric<span class="token punctuation">)</span>和时间类型<span class="token punctuation">(</span>timestamp<span class="token punctuation">)</span>，默认是数字类型     tracking_column_type <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"numeric"</span>    # 记录最后一次运行的结果    record_last_run <span class="token operator">=</span><span class="token operator">&gt;</span> true    # 上面运行结果的保存位置    last_run_metadata_path <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"jdbc‐position.txt"</span>    statement <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"SELECT * FROM user where last_updated &gt;:sql_last_value;"</span>     schedule <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">" * * * * * *"</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span>output <span class="token punctuation">{</span>    elasticsearch <span class="token punctuation">{</span>    document_id <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"%{id}"</span>    document_type <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"_doc"</span>    index <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"users"</span>    hosts <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"http://localhost:9200"</span><span class="token punctuation">]</span>     user <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"elastic"</span>    password <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"123456"</span>  <span class="token punctuation">}</span>  <span class="token constant">stdout</span><span class="token punctuation">{</span>    codec <span class="token operator">=</span><span class="token operator">&gt;</span> rubydebug  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest"># 创建 alias，只显示没有被标记 deleted 的用户POST /_aliases{"actions":[{"add":{  "index":"users",  "alias":"view_users",  "filter":{"term":{"is_deleted":0}}}}]}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="FileBeat">FileBeat</h2><ul><li>Beats 轻量型数据采集器是一个免费且开放的平台，集合了多种单一用途的数据采集器</li><li>FileBeat 专门用于转发和收集日志数据的轻量级采集工具<ul><li>它可以作为代理安装在服务器上，FileBeat监视指定路径的日志文件，收集日志数据</li><li>并将收集到的日志转发到Elasticsearch或者Logstash</li></ul></li><li>FileBeat 的工作原理<ul><li>![[…/…/assets/Attachment/Pasted image 20240527042649.png | 400]]</li><li>启动FileBeat时，会启动一个或者多个输入（Input），这些Input监控指定的日志数据位置</li><li>FileBeat会针对每一个文件启动一个Harvester（收割机）</li><li>Harvester读取每一个文件的日志，将新的日志发送到libbeat</li><li>libbeat将数据收集到一起，并将数据发送给输出（Output）</li></ul></li><li>logstash&nbsp;vs&nbsp;FileBeat<ul><li>Logstash是在jvm上运行的，资源消耗比较大。而FileBeat是基于golang编写的，功能较少但资源消耗也比较小，更轻量级</li><li>Logstash&nbsp;和Filebeat都具有日志收集功能，Filebeat更轻量，占用资源更少</li><li>Logstash&nbsp;具有Filter功能，能过滤分析日志</li><li>一般结构都是Filebeat采集日志，然后发送到消息队列、Redis、MQ中，然后Logstash去获取，利用Filter功能过滤分析，然后存储到Elasticsearch中</li><li>FileBeat和Logstash配合，实现背压机制<ul><li>当将数据发送到Logstash或&nbsp;Elasticsearch时，Filebeat使用背压敏感协议，以应对更多的数据量</li><li>如果Logstash正在忙于处理数据，则会告诉Filebeat&nbsp;减慢读取速度</li><li>一旦拥堵得到解决，Filebeat就会恢复到原来的步伐并继续传输数据</li></ul></li></ul></li><li><code>filebeat.yml</code></li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">output.elasticsearch</span><span class="token punctuation">:</span>  <span class="token key atrule">hosts</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"192.168.65.174:9200"</span><span class="token punctuation">,</span><span class="token string">"192.168.65.192:9200"</span><span class="token punctuation">,</span><span class="token string">"192.168.65.204:9200"</span><span class="token punctuation">]</span>  <span class="token key atrule">username</span><span class="token punctuation">:</span> <span class="token string">"elastic"</span>  <span class="token key atrule">password</span><span class="token punctuation">:</span> <span class="token string">"123456"</span>  <span class="token key atrule">setup.kibana</span><span class="token punctuation">:</span>    <span class="token key atrule">host</span><span class="token punctuation">:</span> <span class="token string">"192.168.65.174:5601"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>启用和配置数据收集模块</li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 查看可以模块列表</span>./filebeat modules list<span class="token comment"># 启用 nginx 模块</span>./filebeat modules <span class="token builtin class-name">enable</span> nginx<span class="token comment"># 启用 Logstash 模块</span>./filebeat modules <span class="token builtin class-name">enable</span> logstash<span class="token comment"># setup 命令加载 Kibana 仪表板。 如果仪表板已经设置，则忽略此命令。 </span>./filebeat setup<span class="token comment"># 启动 Filebeat</span>./filebeat ‐e<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token comment"># 如果需要更改 nginx 日志路径,修改 modules.d/nginx.yml </span>‐ module<span class="token punctuation">:</span> nginx    access<span class="token punctuation">:</span>    var.paths<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"/var/log/nginx/access.log*"</span><span class="token punctuation">]</span><span class="token comment"># 在 modules.d/logstash.yml 文件中修改设置</span>‐ module<span class="token punctuation">:</span> logstash    <span class="token key atrule">log</span><span class="token punctuation">:</span>    <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>    <span class="token key atrule">var.paths</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"/home/es/logstash‐7.17.3/logs/*.log"</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="ELK">ELK</h2><ul><li>集中化日志管理思路：日志收集 -&gt; 格式化分析 -&gt; 检索和可视化 -&gt; 风险告警</li><li>ELK架构分为两种，一种是经典的ELK，另外一种是加上消息队列（Redis或Kafka或RabbitMQ）和Nginx结构<ul><li>经典的ELK主要是由Filebeat&nbsp;+&nbsp;Logstash&nbsp;+&nbsp;Elasticsearch&nbsp;+&nbsp;Kibana组成<ul><li>适用于数据量小的开发环境，存在数据丢失的危险</li><li><img src="/static/architect/ElasticSearch-ELK-4.png" alt=""></li></ul></li><li>整合消息队列+Nginx架构：主要加上了Redis或Kafka或RabbitMQ做消息队列，保证了消息的不丢失<ul><li>主要用在生产环境，可以处理大数据量，并且不会丢失数据</li><li><img src="/static/architect/ElasticSearch-ELK-5.png" alt=""></li></ul></li></ul></li></ul><h3 id="采集-Tomcat-服务器日志">采集 Tomcat 服务器日志</h3><ul><li>使用FileBeats将日志发送到Logstash<ul><li>因为Tomcat的web&nbsp;log日志都是以IP地址开头的，所以我们需要把不以ip地址开头的行追加到上一行</li><li>multiline 多行日志<ul><li>pattern：正则表达式</li><li>negate：true&nbsp;或&nbsp;false<ul><li>默认是false，匹配pattern的行合并到上一行</li><li>true，不匹配pattern的行合并到上一行</li></ul></li><li>match：after&nbsp;或&nbsp;before，合并到上一行的末尾或开头</li></ul></li></ul></li></ul><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">vim</span> filebeat‐logstash.yml<span class="token function">chmod</span> <span class="token number">644</span> filebeat‐logstash.yml./filebeat ‐e ‐c filebeat‐logstash.yml<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">filebeat.inputs</span><span class="token punctuation">:</span>  ‐ type<span class="token punctuation">:</span> log    enabled<span class="token punctuation">:</span> <span class="token boolean important">true</span>    paths<span class="token punctuation">:</span>      ‐ /home/es/apache‐tomcat‐8.5.33/logs/<span class="token important">*access*.*</span>        <span class="token key atrule">multiline.pattern</span><span class="token punctuation">:</span> <span class="token string">'^\\d+\\.\\d+\\.\\d+\\.\\d+ '</span>        <span class="token key atrule">multiline.negate</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>        <span class="token key atrule">multiline.match</span><span class="token punctuation">:</span> after        <span class="token key atrule">output.logstash</span><span class="token punctuation">:</span>        <span class="token key atrule">enabled</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>        <span class="token key atrule">hosts</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"192.168.65.204:5044"</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>Logstash输出数据到Elasticsearch<ul><li><code>filebeat‐elasticSearch.conf</code></li><li><code>bin/logstash&nbsp;‐f&nbsp;config/filebeat‐elasticSearch.conf&nbsp;‐‐config.reload.automatic</code></li></ul></li><li>ES中会生成一个以logstash开头的索引</li></ul><pre class="line-numbers language-c" data-language="c"><code class="language-c">input <span class="token punctuation">{</span>  beats <span class="token punctuation">{</span>    port <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token number">5044</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span>output <span class="token punctuation">{</span>  elasticsearch <span class="token punctuation">{</span>    hosts <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"http://localhost:9200"</span><span class="token punctuation">]</span>     user <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"elastic"</span>    password <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"123456"</span>  <span class="token punctuation">}</span>  <span class="token constant">stdout</span><span class="token punctuation">{</span>    codec <span class="token operator">=</span><span class="token operator">&gt;</span> rubydebug  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>利用Logstash过滤器解析日志<ul><li><code>bin/logstash‐plugin&nbsp;list</code></li></ul></li><li>Grok插件：Grok是一种将非结构化日志解析为结构化的插件<ul><li>适合用来解析系统日志、Web服务器日志、MySQL或者是任意其他的日志格式</li><li>Grok是通过模式匹配的方式来识别日志中的数据，可以把Grok插件简单理解为升级版本的正则表达式</li><li>它拥有更多的模式，默认Logstash拥有120个模式。如果这些模式不满足我们解析日志的需求，我们可以直接使用正则表达式来进行匹配</li><li>可以使用Kibana来进行Grok开发</li></ul></li><li>Grok语法 <code> %{SYNTAX:SEMANTIC}</code><ul><li>SYNTAX（语法）指的是Grok模式名称</li><li>SEMANTIC（语义）是给模式匹配到的文本字段名</li><li><code>%{NUMBER:duration}&nbsp;%{IP:client}</code><ul><li>duration表示：匹配一个数字，client表示匹配一个IP地址</li></ul></li><li>默认在Grok中，所有匹配到的的数据类型都是字符串<ul><li>转换成int类型（目前只支持int和float）： <code>%{NUMBER:duration:int}&nbsp;%{IP:client}</code></li></ul></li></ul></li></ul><pre class="line-numbers language-c" data-language="c"><code class="language-c">filter <span class="token punctuation">{</span>  grok <span class="token punctuation">{</span># <span class="token number">192.168</span><span class="token number">.65</span><span class="token number">.103</span> ‐ ‐ <span class="token punctuation">[</span><span class="token number">23</span><span class="token operator">/</span>Jun<span class="token operator">/</span><span class="token number">2022</span><span class="token operator">:</span><span class="token number">22</span><span class="token operator">:</span><span class="token number">37</span><span class="token operator">:</span><span class="token number">23</span> <span class="token operator">+</span><span class="token number">0800</span><span class="token punctuation">]</span> <span class="token string">"GET /docs/images/docs‐stylesheet.css HTTP/1.1"</span> <span class="token number">200</span> <span class="token number">5780</span>    match <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{</span> <span class="token string">"message"</span> <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"%{IP:ip} ‐ ‐ \[%{HTTPDATE:date}\] \"%{WORD:method} %{PATH:uri} %{DATA:protocol}\" %{INT:status} %{INT:length}"</span> <span class="token punctuation">}</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>使用mutate插件过滤掉不需要的字段</li></ul><pre class="line-numbers language-c" data-language="c"><code class="language-c">mutate <span class="token punctuation">{</span>  enable_metric <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"false"</span>  remove_field <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"message"</span><span class="token punctuation">,</span> <span class="token string">"log"</span><span class="token punctuation">,</span> <span class="token string">"tags"</span><span class="token punctuation">,</span> <span class="token string">"input"</span><span class="token punctuation">,</span> <span class="token string">"agent"</span><span class="token punctuation">,</span> <span class="token string">"host"</span><span class="token punctuation">,</span> <span class="token string">"ecs"</span><span class="token punctuation">,</span> <span class="token string">"@version"</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li>要将日期格式进行转换，可以使用Date插件来实现</li></ul><pre class="line-numbers language-c" data-language="c"><code class="language-c">filter <span class="token punctuation">{</span>  date <span class="token punctuation">{</span>    match <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"date"</span><span class="token punctuation">,</span><span class="token string">"dd/MMM/yyyy:HH:mm:ss Z"</span><span class="token punctuation">,</span><span class="token string">"yyyy‐MM‐dd HH:mm:ss"</span><span class="token punctuation">]</span>    target <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"date"</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>输出到Elasticsearch指定索引<ul><li>index来指定索引名称，默认输出的index名称为：<code>logstash-%{+yyyy.MM.dd}</code></li><li>要在index中使用时间格式化，filter的输出必须包含&nbsp;<code>@timestamp</code> 字段，否则将无法解析日期</li></ul></li><li>注意：index名称中，不能出现大写字符</li></ul><pre class="line-numbers language-c" data-language="c"><code class="language-c">output <span class="token punctuation">{</span>  elasticsearch <span class="token punctuation">{</span>    index <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"tomcat_web_log_%{+YYYY‐MM}"</span>     hosts <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"http://localhost:9200"</span><span class="token punctuation">]</span>    user <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"elastic"</span>    password <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"123456"</span>  <span class="token punctuation">}</span>  <span class="token constant">stdout</span><span class="token punctuation">{</span>    codec <span class="token operator">=</span><span class="token operator">&gt;</span> rubydebug   <span class="token punctuation">}</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>完整的Logstash配置文件<ul><li><code>bin/logstash&nbsp;‐f&nbsp;config/filebeat‐filter‐es.conf&nbsp;‐‐config.reload.automatic</code></li></ul></li></ul><pre class="line-numbers language-c" data-language="c"><code class="language-c">input <span class="token punctuation">{</span>  beats <span class="token punctuation">{</span>    port <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token number">5044</span>   <span class="token punctuation">}</span><span class="token punctuation">}</span>filter <span class="token punctuation">{</span>  grok <span class="token punctuation">{</span>    match <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">{</span>      <span class="token string">"message"</span> <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"%{IP:ip} ‐ ‐ \[%{HTTPDATE:date}\] \"%{WORD:method} %{PATH:uri} %{DATA:protocol}\" %{INT:status:int} %{INT:length:int}"</span>    <span class="token punctuation">}</span>  <span class="token punctuation">}</span>  mutate <span class="token punctuation">{</span>    enable_metric <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"false"</span>    remove_field <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"message"</span><span class="token punctuation">,</span> <span class="token string">"log"</span><span class="token punctuation">,</span> <span class="token string">"tags"</span><span class="token punctuation">,</span> <span class="token string">"input"</span><span class="token punctuation">,</span> <span class="token string">"agent"</span><span class="token punctuation">,</span> <span class="token string">"host"</span><span class="token punctuation">,</span> <span class="token string">"ecs"</span><span class="token punctuation">,</span> <span class="token string">"@version"</span><span class="token punctuation">]</span>  <span class="token punctuation">}</span>  date <span class="token punctuation">{</span>    match <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"date"</span><span class="token punctuation">,</span><span class="token string">"dd/MMM/yyyy:HH:mm:ss Z"</span><span class="token punctuation">,</span><span class="token string">"yyyy‐MM‐dd HH:mm:ss"</span><span class="token punctuation">]</span>    target <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"date"</span>  <span class="token punctuation">}</span><span class="token punctuation">}</span>output <span class="token punctuation">{</span>  <span class="token constant">stdout</span> <span class="token punctuation">{</span>    codec <span class="token operator">=</span><span class="token operator">&gt;</span> rubydebug  <span class="token punctuation">}</span>  elasticsearch <span class="token punctuation">{</span>    index <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"tomcat_web_log_%{+YYYY‐MM}"</span>     hosts <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token punctuation">[</span><span class="token string">"http://localhost:9200"</span><span class="token punctuation">]</span>    user <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"elastic"</span>    password <span class="token operator">=</span><span class="token operator">&gt;</span> <span class="token string">"123456"</span>  <span class="token punctuation">}</span> <span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Logstash
&lt;ul&gt;
&lt;li&gt;Logstash 配置文件结构&lt;/li&gt;
&lt;li&gt;Logstash 导入数据到 ES&lt;/li&gt;
&lt;li&gt;同步数据库数据到 ES&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;FileBeat&lt;/li&gt;
&lt;li&gt;ELK（采集 Tom</summary>
      
    
    
    
    <category term="ElasticSearch" scheme="https://jxch.github.io/categories/ElasticSearch/"/>
    
    
    <category term="ElasticSearch" scheme="https://jxch.github.io/tags/ElasticSearch/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch-DSL</title>
    <link href="https://jxch.github.io/2024/06/13/architect/elasticsearch/elasticsearch-dsl/"/>
    <id>https://jxch.github.io/2024/06/13/architect/elasticsearch/elasticsearch-dsl/</id>
    <published>2024-06-13T03:16:49.000Z</published>
    <updated>2024-06-13T03:16:59.021Z</updated>
    
    <content type="html"><![CDATA[<ul><li>查询所有 match_all<ul><li>分页查询 from&nbsp;+&nbsp;size</li><li>深分页查询 Scroll</li><li>指定字段排序 sort</li><li>返回指定字段<code>_source</code></li><li>match</li></ul></li><li>短语查询 match_phrase</li><li>多字段查询 multi_match</li><li>query_string<ul><li>simple_query_string</li></ul></li><li>关键词查询 Term<ul><li>结构化搜索</li></ul></li><li>前缀查询 prefix</li><li>通配符查询 wildcard</li><li>范围查询 range</li><li>多 id 查询 ids</li><li>模糊查询 fuzzy</li><li>高亮 highlight</li></ul><hr><ul><li>Query&nbsp;DSL（Domain&nbsp;Specified&nbsp;Language）是利用Rest&nbsp;API传递JSON格式的请求体(RequestBody)与ES进行交互</li></ul><h3 id="查询所有-match-all">查询所有 match_all</h3><ul><li>使用match_all，默认只会返回10条数据<ul><li><code>_search</code>查询默认采用的是分页查询，每页记录数size的默认值为10</li><li>如果想显示更多数据，指定size</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search# 等同于GET /es_db/_search {"query":{"match_all":{}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="分页查询-from-size">分页查询 from&nbsp;+&nbsp;size</h3><ul><li>返回指定条数size<ul><li>size&nbsp;关键字:&nbsp;指定查询结果中返回指定条数。&nbsp;默认返回值10条</li><li>from&nbsp;+&nbsp;size的结果必须小于或等于10000</li><li>可以采用scroll&nbsp;api更高效的请求大量数据集</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search {"query":{"match_all":{}},"size": 100}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>查询结果的窗口的限制可以通过参数index.max_result_window进行设置<ul><li>index.max_result_window主要用来限制单次查询满足查询条件的结果窗口的<ul><li>不能简单理解成查询返回给调用方的数据量</li><li>窗口大小由from&nbsp;+&nbsp;size共同决定</li></ul></li><li>主要是为了限制内存的消耗<ul><li>尽管最后我们只取了10条数据返回给客户端，但ES进程执行查询操作的过程中确需要将（1000000&nbsp;+&nbsp;10）的记录都加载到内存中</li><li>这也是ES中不推荐采用（from&nbsp;+&nbsp;size）方式进行深度分页的原因</li></ul></li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">PUT /es_db/_settings{"index.max_result_window":"20000"}# 修改所有的索引PUT /_all/_settings{"index.max_result_window":"20000"}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>分页查询 form<ul><li>from&nbsp;关键字:&nbsp;用来指定起始返回位置，和size关键字连用可实现分页效果</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search {"query":{"match_all":{}},"size":5,"from":0}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="深分页查询-Scroll">深分页查询 Scroll</h3><ul><li>查询命令中新增 scroll=1m，说明采用游标查询，保持游标查询窗口一分钟</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search?scroll=1m{"query":{"match_all":{}},"size":2}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>采用游标id查询<ul><li>多次根据scroll_id游标查询，直到没有数据返回则结束查询</li><li>采用游标查询索引全量数据，更安全高效，限制了单次对内存的消耗</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /_search/scroll{"scroll":"1m","scroll_id":"FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFmNwcVdjblRxUzVhZXlicG9HeU02bWcAAAAAAABmzRY2YlV3Z0o5VVNTdWJobkE5Z3MtXzJB"}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="指定字段排序-sort">指定字段排序 sort</h3><ul><li>会让得分失效</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search{"query":{"match_all":{}},"sort":[{"age":"desc"}]}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="返回指定字段-source">返回指定字段<code>_source</code></h2><ul><li><code>_source</code>&nbsp;关键字:&nbsp;是一个数组，在数组中用来指定展示那些字段</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search{"query":{"match_all":{}},"_source":["name","address"]}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="match">match</h3><ul><li>match在匹配时会对所查找的关键词进行分词，然后按分词匹配查找<ul><li>query&nbsp;:&nbsp;指定匹配的值</li><li>operator&nbsp;:&nbsp;匹配条件类型<ul><li>and&nbsp;:&nbsp;条件分词后都要匹配</li><li>or&nbsp;:&nbsp;条件分词后有一个匹配即可 (默认)</li></ul></li><li>minmum_should_match&nbsp;:&nbsp;最低匹配度，即条件在倒排索引中最低的匹配度</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest"># 模糊匹配 match 分词后 or 的效果GET /es_db/_search{"query":{"match":{"address":"广州白云山公园"}}}# 分词后 and 的效果GET /es_db/_search {"query":{"match":{"address":{"query":"广州白云山公园","operator":"AND"}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>当operator参数设置为or时，minnum_should_match参数用来控制匹配的分词的最少数量</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest"># 最少匹配 广州，公园 两个词GET /es_db/_search{"query":{"match":{"address":{"query":"广州公园","minimum_should_match":2}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><hr><h2 id="短语查询-match-phrase">短语查询 match_phrase</h2><ul><li>match_phrase 查询分析文本并根据分析的文本创建一个短语查询</li><li>match_phrase&nbsp;会将检索关键词分词</li><li>match_phrase 的分词结果必须在被检索字段的分词中都包含，而且顺序必须相同，而且默认必须都是连续的</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search{"query":{"match_phrase":{"address":"广州白云山"}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>address 改成”广州白云“可能查不出数据<ul><li>广州和白云不是相邻的词条，中间会隔一个白云山</li><li>而match_phrase匹配的是相邻的词条</li></ul></li><li>可以借助slop参数解决词条间隔的问题<ul><li>slop参数告诉match_phrase查询词条能够相隔多远时仍然将文档视为匹配</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest"># 广州云山分词后相隔为 2，可以匹配到结果 GET /es_db/_search{"query":{"match_phrase":{"address":{"query":"广州云山","slop":2}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="多字段查询-multi-match">多字段查询 multi_match</h2><ul><li>可以根据字段类型，决定是否使用分词查询，得分最高的在前面</li><li>字段类型分词，将查询条件分词之后进行查询，如果该字段不分词就会将查询条件作为整体进行查询</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search{"query":{"multi_match":{"query":"长沙张龙","fields":["address","name"]}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="query-string">query_string</h2><ul><li>允许我们在单个查询字符串中指定 AND&nbsp;|&nbsp;OR&nbsp;|&nbsp;NOT 条件，同时也支持多字段搜索</li><li>在所有字段中搜索</li><li>查询字段分词就将查询条件分词查询，查询字段不分词将查询条件不分词查询</li><li>未指定字段查询</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search{"query":{"query_string":{"query":"张三 OR 橘子洲"}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>指定单个字段查询</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search{"query":{"query_string":{"default_field":"address","query":"白云山 OR 橘子洲"}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>指定多个字段查询</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search{"query":{"query_string":{"fields":["name","address"],"query":"张三 OR (广州 AND 王五)"}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="simple-query-string">simple_query_string</h3><ul><li>类似 Query&nbsp;String，但是会忽略错误的语法</li><li>同时只支持部分查询语法，不支持 AND&nbsp;OR&nbsp;NOT，会当作字符串处理</li><li>支持部分逻辑<ul><li>+&nbsp;替代 AND</li><li>|&nbsp;替代 OR<br>-&nbsp;替代 NOT</li></ul></li><li>默认的 operator 是 OR</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search{"query":{"simple_query_string":{  "fields":["name","address"],  "query":"广州公园",  "default_operator":"AND"}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="关键词查询-Term">关键词查询 Term</h2><ul><li>Term 用来使用关键词查询 (精确匹配)，还可以用来查询没有被进行分词的数据类型</li><li>Term 是表达语意的最小单位</li><li>match 在匹配时会对所查找的关键词进行分词，然后按分词匹配查找</li><li>而 term 会直接对关键词进行查找</li><li>一般模糊查找的时候，多用 match，而精确查找时可以使用 term</li><li>只有 text 类型分词</li><li>Term 查询，对输入不做分词<ul><li>会将输入作为一个整体，在倒排索引中查找准确的词项</li><li>并且使用相关度算分公式为每个包含该词项的文档进行相关度算分</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search{"query":{"term":{"address":{"value":"广州白云"}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>可以通过&nbsp;Constant&nbsp;Score&nbsp;将查询转换成一个&nbsp;Filtering，避免算分，并利用缓存，提高性能<ul><li>将 Query&nbsp;转成&nbsp;Filter，忽略 TF-IDF 计算，避免相关性算分的开销</li><li>Filter 可以有效利用缓存</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search{"query":{"constant_score":{"filter":{"term":{"address.keyword":"广州白云山公园"}}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="结构化搜索">结构化搜索</h3><ul><li>结构化搜索 (Structured&nbsp;search) 是指对结构化数据的搜索</li><li>结构化数据<ul><li>日期，布尔类型和数字都是结构化的</li><li>文本也可以是结构化的<ul><li>如彩色笔可以有离散的颜色集合：红 (red)&nbsp;、绿 (green)、蓝 (blue)</li><li>一个博客可能被标记了标签，例如，分布式 (distributed) 和搜索 (search)</li><li>电商网站上的商品都有 UPC (通用产品码 Universal&nbsp;Product&nbsp;Code) 或其他的唯一</li><li>它们都需要遵从严格规定的、结构化的格式</li></ul></li></ul></li><li>应用场景：对 bool，日期，数字，结构化的文本可以利用 term 做精确匹配</li><li>term 处理多值字段<ul><li>term查询是包含，不是等于</li></ul></li></ul><h2 id="前缀查询-prefix">前缀查询 prefix</h2><ul><li>它会对分词后的 term 进行前缀搜索<ul><li>它不会分析要搜索的字符串，传入的前缀就是想要查找的前缀</li><li>默认状态下，前缀查询不做相关度分数计算，它只是将所有匹配的文档返回，然后赋予所有相关分数值为1</li><li>它的行为更像是一个过滤器而不是查询</li><li>两者实际的区别就是过滤器是可以被缓存的，而前缀查询不行</li></ul></li><li>prefix 的原理：需要遍历所有倒排索引，并比较每个 term 是否已所指定的前缀开头</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search {"query":{"prefix":{"address":{"value":"广州"}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="通配符查询-wildcard">通配符查询 wildcard</h2><ul><li>通配符查询：工作原理和 prefix 相同，只不过它不是只比较开头，它能支持更为复杂的匹配模式</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search {"query":{"wildcard":{"address":{"value":"<span class="token inline"><span class="token punctuation">*</span><span class="token italic">白</span><span class="token punctuation">*</span></span>"}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="范围查询-range">范围查询 range</h2><ul><li>range 范围关键字<ul><li>gte 大于等于</li><li>lte  小于等于</li><li>gt 大于</li><li>lt 小于</li><li>now 当前时间</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">POST /es_db/_search {"query":{"range":{"age":{"gte":25,"lte":28}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>日期 range</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /product/_search {"query":{"range":{"date":{"gte":"now‐2y" }}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="多-id-查询-ids">多 id 查询 ids</h2><ul><li>ids&nbsp;关键字&nbsp;:&nbsp;值为数组类型，用来根据一组 id 获取多个对应的文档</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search {"query":{"ids":{"values":[1,2]}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="模糊查询-fuzzy">模糊查询 fuzzy</h2><ul><li>使用 fuzziness 属性来进行模糊查询，从而达到搜索有错别字的情形</li><li>fuzzy&nbsp;查询会用到两个很重要的参数，fuzziness，prefix_length<ul><li>fuzziness：表示输入的关键字通过几次操作可以转变成为 ES 库里面的对应 field 的字段<ul><li>操作是指：新增一个字符，删除一个字符，修改一个字符，每次操作可以记做编辑距离为 1<ul><li>如中文集团到中威集团编辑距离就是1，只需要修改一个字符</li></ul></li><li>该参数默认值为0，即不开启模糊查询</li></ul></li><li>prefix_length：表示限制输入关键字和ES对应查询field的内容开头的第n个字符必须完全匹配，不允许错别字匹配<ul><li>如这里等于1，则表示开头的1个字必须匹配，不匹配则不返回</li><li>默认值也是0</li><li>加大prefix_length的值可以提高效率和准确率</li></ul></li></ul></li><li>fuzzy&nbsp;模糊查询&nbsp;最大模糊错误&nbsp;必须在0-2之间<ul><li>搜索关键词长度为&nbsp;2，不允许存在模糊</li><li>搜索关键词长度为3-5，允许1次模糊</li><li>搜索关键词长度大于5，允许最大2次模糊</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search{"query":{"fuzzy":{"address":{"value":"白运山","fuzziness":1}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="高亮-highlight">高亮 highlight</h2><ul><li>highlight&nbsp;关键字：可以让符合条件的文档中的关键词高亮<ul><li>pre_tags&nbsp;前缀标签</li><li>post_tags&nbsp;后缀标签</li><li>tags_schema&nbsp;设置为styled可以使用内置高亮样式</li><li>require_field_match&nbsp;多字段高亮需要设置为false</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /products/_search {"query":{"term":{"name":{"value":"牛仔"}}},"highlight":{"fields":{"*":{}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>自定义高亮 html 标签<ul><li>可以在 highlight 中使用 pre_tags 和 post_tags</li><li>多字段高亮：require_field_match 设置为 false</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /products/_search{"query":{"term":{"name":{"value": "牛仔"}}}, "highlight":{  "post_tags":["&lt;/span&gt;"],  "pre_tags":["&lt;span style='color:red'&gt;"],   # 多字段高亮  "require_field_match":"false",  "fields":{"*":{}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;查询所有 match_all
&lt;ul&gt;
&lt;li&gt;分页查询 from&amp;nbsp;+&amp;nbsp;size&lt;/li&gt;
&lt;li&gt;深分页查询 Scroll&lt;/li&gt;
&lt;li&gt;指定字段排序 sort&lt;/li&gt;
&lt;li&gt;返回指定字段&lt;code&gt;_source&lt;/code&gt;&lt;/</summary>
      
    
    
    
    <category term="ElasticSearch" scheme="https://jxch.github.io/categories/ElasticSearch/"/>
    
    
    <category term="ElasticSearch" scheme="https://jxch.github.io/tags/ElasticSearch/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch-相关性查询</title>
    <link href="https://jxch.github.io/2024/06/13/architect/elasticsearch/elasticsearch-xiang-guan-xing-cha-xun/"/>
    <id>https://jxch.github.io/2024/06/13/architect/elasticsearch/elasticsearch-xiang-guan-xing-cha-xun/</id>
    <published>2024-06-13T03:14:49.000Z</published>
    <updated>2024-06-13T03:16:14.883Z</updated>
    
    <content type="html"><![CDATA[<ul><li>相关性算分<ul><li>TF-IDF</li><li>BM25</li><li>Boosting</li></ul></li><li>布尔查询 bool&nbsp;Query</li><li>Boosting&nbsp;Query</li><li>单字符串多字段查询</li></ul><hr><h2 id="相关性算分">相关性算分</h2><ul><li>如何衡量相关性<ul><li>Precision(查准率)―尽可能返回较少的无关文档</li><li>Recall(查全率)–尽量返回较多的相关文档</li><li>Ranking&nbsp;-是否能够按照相关度进行排序</li></ul></li><li>相关性（Relevance）：搜索的相关性算分，描述了一个文档和查询语句匹配的程度<ul><li>ES&nbsp;会对每个匹配查询条件的结果进行算分<code>_score</code></li><li>打分的本质是排序，需要把最符合用户需求的文档排在前面</li><li>ES5之前，默认的相关性算分采用TF-IDF，现在采用BM25</li></ul></li><li>TF-IDF（term&nbsp;frequency–inverse&nbsp;document&nbsp;frequency）是一种用于信息检索与数据挖掘的常用加权技术<ul><li>TF-IDF被公认为是信息检索领域最重要的发明</li><li>现代搜索引擎，对TF-IDF进行了大量细微的优化</li></ul></li><li>Lucene中的TF-IDF评分公式<ul><li><img src="/static/architect/ElasticSearch-%E7%9B%B8%E5%85%B3%E6%80%A7%E6%9F%A5%E8%AF%A2-1.png" alt=""></li><li>TF是词频 (Term&nbsp;Frequency)：检索词在文档中出现的频率越高，相关性也越高</li><li>IDF是逆向文本频率 (Inverse&nbsp;Document&nbsp;Frequency)：每个检索词在索引中出现的频率越高，相关性越低</li><li>字段长度归一值 (field-length&nbsp;norm)：字段越短，字段的权重越高<ul><li>检索词出现在一个内容短的&nbsp;title&nbsp;要比同样的词出现在一个内容长的&nbsp;content&nbsp;字段权重更大</li></ul></li></ul></li><li>BM25 就是对&nbsp;TF-IDF&nbsp;算法的改进<ul><li>对于&nbsp;TF-IDF&nbsp;算法，TF(t)&nbsp;部分的值越大，整个公式返回的值就会越大</li><li>BM25&nbsp;就针对这点进行来优化，随着TF(t)&nbsp;的逐步加大，该算法的返回值会趋于一个数值</li><li><img src="/static/architect/ElasticSearch-%E7%9B%B8%E5%85%B3%E6%80%A7%E6%9F%A5%E8%AF%A2-2.png" alt=""></li></ul></li><li>BM25的公式<ul><li><img src="/static/architect/ElasticSearch-%E7%9B%B8%E5%85%B3%E6%80%A7%E6%9F%A5%E8%AF%A2-3.png" alt=""></li></ul></li><li>通过Explain&nbsp;API查看TF-IDF</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /test_score/_search{"explain":true,"query":{"match":{"content":"elasticsearch"}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>Boosting 是控制相关度的一种手段<ul><li>当 boost&nbsp;&gt;&nbsp;1 时，打分的权重相对性提升</li><li>当 0&nbsp;&lt;&nbsp;boost&nbsp;&lt;1 时，打分的权重相对性降低</li><li>当 boost&nbsp;&lt;0 时，贡献负分</li></ul></li><li>应用场景：希望包含了某项内容的结果不是不出现，而是排序靠后</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest"># 返回匹配 positive 查询的文档并降低匹配 negative 查询的文档相似度分GET /test_score/_search{"query":{"boosting":{  "positive":{"term":{"content":"elasticsearch"}},  "negative":{"term":{"content":"like"}},  "negative_boost":0.2}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="布尔查询-bool-Query">布尔查询 bool&nbsp;Query</h2><ul><li>一个 bool 查询，是一个或者多个查询子句的组合，总共包括4种子句，其中2种会影响算分，2种不影响算分<ul><li>must:&nbsp;相当于&amp;&amp;&nbsp;，必须匹配，贡献算分</li><li>should:&nbsp;相当于||&nbsp;，选择性匹配，贡献算分</li><li>must_not:&nbsp;&nbsp;相当于!&nbsp;，必须不能匹配，不贡献算分</li><li>filter:&nbsp;必须匹配，不贡献算法</li></ul></li><li>在Elasticsearch中，有Query和&nbsp;Filter两种不同的Context<ul><li>Query&nbsp;Context:&nbsp;相关性算分</li><li>Filter&nbsp;Context:&nbsp;不需要算分&nbsp;,可以利用Cache，获得更好的性能</li></ul></li><li>相关性并不只是全文本检索的专利，也适用于 yes&nbsp;|&nbsp;no&nbsp;的子句，匹配的子句越多，相关性评分越高<ul><li>如果多条查询子句被合并为一条复合查询语句</li><li>比如&nbsp;bool 查询，则每个查询子句计算得出的评分会被合并到总的相关性评分中</li></ul></li><li>bool 查询语法<ul><li>子查询可以任意顺序出现</li><li>可以嵌套多个查询</li><li>如果你的bool查询中，没有must条件，should中必须至少满足一条查询</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /es_db/_search{"query":{"bool":{  "must":{"match":{"remark":"java developer"}},  "filter":{"term":{"sex":"1"}},  "must_not":{"range":{"age":{"gte":30}}},  "should":[    {"term":{"address.keyword":{"value":"广州天河公园"}}},    {"term":{"address.keyword":{"value":"广州白云山公园"}}}],  "minimum_should_match":1}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>解决结构化查询“包含而不是相等”的问题<ul><li>增加count字段，使用bool查询解决</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">POST /employee/_bulk{"index":{"_id":1}}{"name":"小明","interest":["跑步","篮球"],"interest_count":2}{"index":{"_id":2}}{"name":"小红","interest":["跑步"],"interest_count":1}"query":{"bool":{"must":[   {"term":{"interest.keyword":{"value":"跑步"}}},   {"term":{"interest_count":{"value":1}}}]}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>利用bool嵌套实现should&nbsp;not逻辑</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">"query":{"bool":{ "must":{"match":{"remark":"java developer"}}, "should":[{"bool":{"must_not":[{"term":{"sex":1}}]}}], "minimum_should_match":1}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="Boosting-Query">Boosting&nbsp;Query</h2><ul><li>控制字段的 Boosting：Boosting是控制相关的一种手段。可以通过指定字段的boost值影响查询结果<ul><li>当boost&gt;1时，打分的权重相对性提升</li><li>当0&lt;boost&lt;1时，打分的权重相对性降低</li><li>当boost&lt;0时，贡献负分</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /blogs/_search {"query":{"bool":{"should":[ {"match":{"title":{"query":"apple,ipad","boost":1}}}, {"match":{"content":{"query":"apple,ipad","boost":4}}}]}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li>利用must&nbsp;not排除</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /news/_search{"query":{"bool":{ "must":{"match":{"content":"apple"}}, "must_not":{"match":{"content":"pie"}}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li>利用negative_boost降低相关性<ul><li>negative_boost&nbsp;对&nbsp;negative部分query生效</li><li>计算评分时，boosting部分评分不修改，negative部分query乘以negative_boost值</li><li>negative_boost取值: 0-1.0</li></ul></li><li>对某些返回结果不满意，但又不想排除掉可以考虑boosting&nbsp;query的negative_boost</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /news/_search {"query":{"boosting":{  "positive":{"match":{"content":"apple"}},  "negative":{"match":{"content":"pie"}},  "negative_boost":0.2}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><hr><h2 id="单字符串多字段查询">单字符串多字段查询</h2><ul><li>三种场景<ul><li>最佳字段 (Best&nbsp;Fields)：当字段之间相互竞争，又相互关联。例如，对于博客的&nbsp;title和&nbsp;body这样的字段，评分来自最匹配字段</li><li>多数字段 (Most&nbsp;Fields)：处理英文内容时的一种常见的手段是，在主字段(&nbsp;English&nbsp;Analyzer)，抽取词干，加入同义词，以匹配更多的文档。相同的文本，加入子字段（Standard&nbsp;Analyzer），以提供更加精确的匹配。其他字段作为匹配文档提高相关度的信号，匹配字段越多则越好</li><li>混合字段 (Cross&nbsp;Field)：对于某些实体，例如人名，地址，图书信息。需要在多个字段中确定信息，单个字段只能作为整体的一部分。希望在任何这些列出的字段中找到尽可能多的词</li></ul></li><li>bool&nbsp;should的算法过程<ul><li>查询should语句中的两个查询</li><li>加两个查询的评分</li><li>乘以匹配语句的总数</li><li>除以所有语句的总数</li></ul></li><li>最佳字段查询 Dis&nbsp;Max&nbsp;Query<ul><li>将任何与任一查询匹配的文档作为结果返回，采用字段上最匹配的评分最终评分返回</li><li>竞争关系的字段，不应该讲分数简单叠加，而是应该找到单个最佳匹配的字段的评分</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">POST blogs/_search{"query":{"dis_max":{"queries":[  {"match":{"title":"Brown fox"}},   {"match":{"body":"Brown fox"}}]}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li>可以通过tie_breaker参数调整<ul><li>Tier&nbsp;Breaker是一个介于0-1之间的浮点数</li><li>0代表使用最佳匹配；1代表所有语句同等重要</li></ul></li><li>流程<ul><li>获得最佳匹配语句的评分_score</li><li>将其他匹配语句的评分与tie_breaker相乘</li><li>对以上评分求和并规范化</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">POST /blogs/_search{"query":{"dis_max":{"queries":[  {"match":{"title":"Quick pets"}},  {"match":{"body":"Quick pets"}}],"tie_breaker":0.2}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><ul><li>Multi&nbsp;Match&nbsp;Query 最佳字段 (Best&nbsp;Fields) 搜索</li><li>Best&nbsp;Fields 是默认类型，可以不用指定</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">POST /blogs/_search{"query":{"multi_match":{  "type":"best_fields",  "query":"Quick pets",  "fields":["title","body"],"tie_breaker":0.2}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>使用多数字段（Most&nbsp;Fields）搜索<ul><li>每个字段对于最终评分的贡献可以通过自定义值boost&nbsp;来控制</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest"># 增加 title 的权重GET /titles/_search {"query":{"multi_match":{  "query":"barking dogs",   "type":"most_fields",  "fields":["title^10","title.std"]}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>跨字段（Cross&nbsp;Field）搜索<ul><li>与copy_to相比，其中一个优势就是它可以在搜索时为单个字段提升权重</li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">GET /address/_search{"query":{"multi_match":{ "query":"湖南常德", "type":"cross_fields", "operator":"and", "fields":["province","city"]}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>可以用copy_to解决，但是需要额外的存储空间</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">PUT /address{"mappings":{"properties":{  "province":{"type":"keyword","copy_to":"full_address"},  "city":{"type":"text","copy_to":"full_address"}}}, "settings":{"index":{"analysis.analyzer.default.type":"ik_max_word"}}}GET /address/_search {"query":{"match":{"full_address":{"query":"湖南常德","operator":"and"}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;相关性算分
&lt;ul&gt;
&lt;li&gt;TF-IDF&lt;/li&gt;
&lt;li&gt;BM25&lt;/li&gt;
&lt;li&gt;Boosting&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;布尔查询 bool&amp;nbsp;Query&lt;/li&gt;
&lt;li&gt;Boosting&amp;nbsp;Query&lt;/li&gt;
&lt;</summary>
      
    
    
    
    <category term="ElasticSearch" scheme="https://jxch.github.io/categories/ElasticSearch/"/>
    
    
    <category term="ElasticSearch" scheme="https://jxch.github.io/tags/ElasticSearch/"/>
    
  </entry>
  
  <entry>
    <title>ElasticSearch-数据建模</title>
    <link href="https://jxch.github.io/2024/06/13/architect/elasticsearch/elasticsearch-shu-ju-jian-mo/"/>
    <id>https://jxch.github.io/2024/06/13/architect/elasticsearch/elasticsearch-shu-ju-jian-mo/</id>
    <published>2024-06-13T03:13:49.000Z</published>
    <updated>2024-06-13T03:13:51.334Z</updated>
    
    <content type="html"><![CDATA[<ul><li>处理关联关系<ul><li>Object:&nbsp;优先考虑反范式（Denormalization）</li><li>Nested:&nbsp;当数据包含多数值对象，同时有查询需求</li><li>Child/Parent：关联文档更新非常频繁时</li></ul></li><li>避免过多字段<ul><li>一个文档中，最好避免大量的字段<ul><li>过多的字段数不容易维护</li><li>Mapping&nbsp;信息保存在 Cluster&nbsp;State&nbsp;中，数据量过大，对集群性能会有影响</li><li>删除或者修改数据需要reindex</li></ul></li><li>默认最大字段数是1000，可以设置<code>index.mapping.total_fields.limit</code>限定最大字段数</li><li>生产环境中，尽量不要打开&nbsp;Dynamic，可以使用Strict控制新增字段的加入<ul><li>true&nbsp;：未知字段会被自动加入</li><li>false&nbsp;：新字段不会被索引，但是会保存在_source</li><li>strict&nbsp;：新增字段不会被索引，文档写入失败</li></ul></li><li>对于多属性的字段，比如cookie，商品属性，可以考虑使用Nested</li></ul></li><li>避免正则，通配符，前缀查询<ul><li>正则，通配符查询，前缀查询属于Term查询，但是性能不够好</li><li>特别是将通配符放在开头，会导致性能的灾难</li></ul></li><li>避免空值引起的聚合不准</li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">PUT /scores{"mappings":{"properties":{"score":{"type":"float","null_value":0}}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>为索引的 Mapping 加入 Meta&nbsp;信息<ul><li>Mappings设置非常重要，需要从两个维度进行考虑<ul><li>功能︰搜索，聚合，排序</li><li>性能︰存储的开销;&nbsp;内存的开销;&nbsp;搜索的性能</li></ul></li><li>Mappings设置是一个迭代的过程<ul><li>加入新的字段很容易（必要时需要update_by_query）</li><li>更新删除字段不允许（需要Reindex重建数据）</li><li>最好能对 Mappings&nbsp;加入 Meta&nbsp;信息，更好的进行版本管理</li><li>可以考虑将 Mapping 文件上传 git 进行管理</li></ul></li></ul></li></ul><pre class="line-numbers language-rest" data-language="rest"><code class="language-rest">PUT /my_index{"mappings":{"_meta":{"index_version_mapping":"1.1"}}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;处理关联关系
&lt;ul&gt;
&lt;li&gt;Object:&amp;nbsp;优先考虑反范式（Denormalization）&lt;/li&gt;
&lt;li&gt;Nested:&amp;nbsp;当数据包含多数值对象，同时有查询需求&lt;/li&gt;
&lt;li&gt;Child/Parent：关联文档更新非常频繁时&lt;/l</summary>
      
    
    
    
    <category term="ElasticSearch" scheme="https://jxch.github.io/categories/ElasticSearch/"/>
    
    
    <category term="ElasticSearch" scheme="https://jxch.github.io/tags/ElasticSearch/"/>
    
  </entry>
  
</feed>

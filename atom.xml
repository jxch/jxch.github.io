<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>PA &amp; CODING</title>
  
  <subtitle>求仁得仁</subtitle>
  <link href="https://jxch.github.io/atom.xml" rel="self"/>
  
  <link href="https://jxch.github.io/"/>
  <updated>2024-09-11T01:46:48.840Z</updated>
  <id>https://jxch.github.io/</id>
  
  <author>
    <name>钱不寒</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MySQL-SQL执行过程</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-sql-zhi-xing-guo-cheng/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-sql-zhi-xing-guo-cheng/</id>
    <published>2024-09-11T01:43:00.000Z</published>
    <updated>2024-09-11T01:46:48.840Z</updated>
    
    <content type="html"><![CDATA[<ul><li>MySQL的内部组件结构：MySQL&nbsp;可以分为&nbsp;Server&nbsp;层和存储引擎层两部分<ul><li>Server层：主要包括连接器、查询缓存、分析器、优化器、执行器等，涵盖&nbsp;MySQL&nbsp;的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等</li><li>Store层：存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持&nbsp;InnoDB、MyISAM、Memory&nbsp;等多个存储引擎。现在最常用的存储引擎是&nbsp;InnoDB，也就是说如果我们在create&nbsp;table时不指定表的存储引擎类型，默认会给你设置存储引擎为InnoDB</li></ul></li><li>连接器：支持多种类的客户端建立连接的工作<ul><li>连接器负责跟客户端建立连接、获取权限、维持和管理连接</li><li>由于连接对象中缓存了连接信息：所以一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。用户的权限表在系统表空间的mysql的user表中</li><li>连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在&nbsp;show&nbsp;processlist&nbsp;命令中看到它</li><li>客户端如果长时间不发送command到Server端，连接器就会自动将它断开。这个时间是由参数&nbsp;wait_timeout&nbsp;控制的，默认值是&nbsp;8&nbsp;小时</li><li>如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒：Lost&nbsp;connection&nbsp;to&nbsp;MySQL&nbsp;server&nbsp;during query。这时候如果你要继续，就需要重连，然后再执行请求了</li><li>长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接<ul><li>长连接有些时候会导致&nbsp;MySQL&nbsp;占用内存涨得特别快，这是因为&nbsp;MySQL&nbsp;在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是&nbsp;MySQL&nbsp;异常重启了</li></ul></li><li>短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个</li></ul></li><li>查询缓存（mysql8.0已经移除了查询缓存功能）：query_cache_type=DEMAND<ul><li><code>select&nbsp;SQL_CACHE&nbsp;*&nbsp;from&nbsp;t</code></li><li>查询缓存往往弊大于利：查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。对于更新压力大的数据库来说，查询缓存的命中率会非常低</li><li>一般建议在静态表里使用查询缓存（极少更新的表）：系统配置表、字典表</li></ul></li><li>分析器：如果没有命中查询缓存，就要开始真正执行语句了<ul><li>语法分析；语法分析；语义分析；构造执行树；生成执行计划；计划的执行</li></ul></li><li>优化器：优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序</li><li>执行器：开始执行的时候，要先判断一下你对这个表有没有执行查询的权限，如果没有，就会返回没有权限的错误<ul><li>如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口</li></ul></li><li>bin-log 归档是Server层实现的二进制日志<ul><li>Binlog在MySQL的Server层实现（引擎共用）</li><li>Binlog为逻辑日志，记录的是一条语句的原始逻辑</li><li>Binlog不限大小,追加写入，不会覆盖以前的日志</li></ul></li></ul><hr><p><img src="/static/IT/MySQL/MySQL-SQL%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B-1.png" alt=""><br><img src="/static/IT/MySQL/MySQL-SQL%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B-2.png" alt=""><br><img src="/static/IT/MySQL/MySQL-SQL%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B-3.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;MySQL的内部组件结构：MySQL&amp;nbsp;可以分为&amp;nbsp;Server&amp;nbsp;层和存储引擎层两部分
&lt;ul&gt;
&lt;li&gt;Server层：主要包括连接器、查询缓存、分析器、优化器、执行器等，涵盖&amp;nbsp;MySQL&amp;nbsp;的大多数核心服务功能，以</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-MVCC</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-mvcc/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-mvcc/</id>
    <published>2024-09-11T01:39:00.000Z</published>
    <updated>2024-09-11T01:42:33.240Z</updated>
    
    <content type="html"><![CDATA[<ul><li>MVCC(Multi-Version&nbsp;Concurrency&nbsp;Control) 机制可以保证可重复读隔离级别下的隔离性</li><li>undo日志版本链与read&nbsp;view机制<ul><li>undo日志版本链是指一行数据被多个事务依次修改过后，在每个事务修改完后，Mysql会保留修改前的数据undo回滚日志，并且用两个隐藏字段trx_id和roll_pointer把这些undo日志串联起来形成一个历史记录版本链</li></ul></li><li>版本链比对流程<ul><li>当事务开启，执行任何查询sql时会生成当前事务的一致性视图read-view</li><li>该视图在事务结束之前都不会变化（如果是读已提交隔离级别在每次执行查询sql时都会重新生成）</li><li>这个视图由执行查询时所有未提交事务id数组（数组里最小的id为min_id）和已创建的最大事务id（max_id）组成，事务里的任何sql查询结果需要从对应版本链里的最新数据开始逐条跟read-view做比对从而得到最终的快照结果</li></ul></li><li>版本链比对规则<ul><li>如果&nbsp;row&nbsp;的&nbsp;trx_id&nbsp;落在绿色部分(&nbsp;trx_id&lt;min_id&nbsp;)，表示这个版本是已提交的事务生成的，这个数据是可见的</li><li>如果&nbsp;row&nbsp;的&nbsp;trx_id&nbsp;落在红色部分(&nbsp;trx_id&gt;max_id&nbsp;)，表示这个版本是由将来启动的事务生成的，是不可见的（自己可见）</li><li>如果&nbsp;row&nbsp;的&nbsp;trx_id&nbsp;落在黄色部分(min_id&nbsp;&lt;=trx_id&lt;=&nbsp;max_id)，那就包括两种情况<ul><li>若&nbsp;row&nbsp;的&nbsp;trx_id&nbsp;在视图数组中，表示这个版本是由还没提交的事务生成的，不可见（自己可见）</li><li>若&nbsp;row&nbsp;的&nbsp;trx_id&nbsp;不在视图数组中，表示这个版本是已经提交了的事务生成的，可见</li></ul></li></ul></li><li>对于删除的情况可以认为是update的特殊情况<ul><li>会将版本链上最新的数据复制一份，然后将trx_id修改成删除操作的trx_id</li><li>同时在该条记录的头信息（record&nbsp;header）里的（deleted_flag）标记位写上true，来表示当前记录已经被删除</li><li>在查询时按照上面的规则查到对应的记录如果delete_flag标记位为true，意味着记录已被删除，则不返回数据</li></ul></li><li>begin/start&nbsp;transaction&nbsp;命令并不是一个事务的起点<ul><li>在执行到它们之后的第一个修改操作InnoDB表的语句，事务才真正启动，才会向mysql申请事务id，mysql内部是严格按照事务的启动顺序来分配事务id的</li></ul></li><li>MVCC机制的实现就是通过read-view机制与undo版本链比对机制，使得不同的事务会根据数据版本链对比规则读取同一条数据在版本链上的不同版本数据</li></ul><p><img src="/static/IT/MySQL/MySQL-MVCC-1.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;MVCC(Multi-Version&amp;nbsp;Concurrency&amp;nbsp;Control) 机制可以保证可重复读隔离级别下的隔离性&lt;/li&gt;
&lt;li&gt;undo日志版本链与read&amp;nbsp;view机制
&lt;ul&gt;
&lt;li&gt;undo日志版本链是指一行数据被</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-InnoDB-BufferPool</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-innodb-bufferpool/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-innodb-bufferpool/</id>
    <published>2024-09-11T01:37:00.000Z</published>
    <updated>2024-09-11T01:38:02.131Z</updated>
    
    <content type="html"><![CDATA[<ul><li>为什么Mysql不能直接更新磁盘上的数据而且设置这么一套复杂的机制来执行SQL<ul><li>因为来一个请求就直接对磁盘文件进行随机读写，然后更新磁盘文件里的数据性能可能相当差</li><li>Mysql这套机制看起来复杂，但它可以保证每个更新请求都是更新内存BufferPool，然后顺序写日志文件，同时还能保证各种异常情况下的数据一致性</li><li>更新内存的性能是极高的，然后顺序写磁盘上的日志文件的性能也是非常高的，要远高于随机读写磁盘文件</li></ul></li></ul><p><img src="/static/IT/MySQL/MySQL-InnoDB-BufferPool-1.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;为什么Mysql不能直接更新磁盘上的数据而且设置这么一套复杂的机制来执行SQL
&lt;ul&gt;
&lt;li&gt;因为来一个请求就直接对磁盘文件进行随机读写，然后更新磁盘文件里的数据性能可能相当差&lt;/li&gt;
&lt;li&gt;Mysql这套机制看起来复杂，但它可以保证每个更新请求都是更新</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-InnoDB-日志&amp;事务</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-innodb-ri-zhi-shi-wu/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-innodb-ri-zhi-shi-wu/</id>
    <published>2024-09-11T01:27:00.000Z</published>
    <updated>2024-09-11T01:32:23.634Z</updated>
    
    <content type="html"><![CDATA[<ul><li>redo 日志</li><li>undo 日志</li><li>事务流程</li><li>Redo &amp; binlog &amp; Undo 日志的关系</li></ul><hr><h2 id="redo-日志">redo 日志</h2><ul><li>redo 日志（保证了事务的持久性）：只是记录了一下事务对数据库做了哪些修改<ul><li>redo 日志占用的空间非常小</li><li>redo 日志是顺序写入磁盘的</li><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E6%97%A5%E5%BF%97-%E4%BA%8B%E5%8A%A1-1.png" alt=""><ul><li>type：该条redo 日志的类型，redo 日志设计大约有 53 种不同的类型日志</li><li>space ID：表空间 ID</li><li>page number：页号</li><li>data：该条 redo 日志的具体内容</li></ul></li></ul></li><li>redo 日志的写入过程<ul><li>日志缓冲区 <code> innodb_log_buffer_size</code> 默认16MB</li><li>redo log block 512 字节：向log buffer 中写入redo 日志的过程是顺序的，也就是先往前边的block 中写，当该block 的空闲空间用完之后再往下一个 block 中写</li><li>redo 日志刷盘时机<ul><li>log buffer 总容量的大约一半左右</li><li>事务提交时</li><li>后台有一个线程，大约每秒都会刷新一次 log buffer 中的 redo 日志到磁盘</li><li>正常关闭服务器时</li></ul></li></ul></li><li>redo 日志文件组 ib_logfile0 和 ib_logfile1<ul><li>innodb_log_file_size 每个 redo 日志文件的大小，默认 48MB</li><li>innodb_log_files_in_group 指定 redo 日志文件的个数，默认值为 2，最大值为100</li><li>循环写入：在覆盖写之前，总是要保证对应的脏页已经刷到了磁盘<ul><li>在非常大的负载下，为避免错误的覆盖，InnoDB 会强制的 flush 脏页</li></ul></li></ul></li><li>redo 日志文件格式<ul><li>前 4 个 block 是用来存储一些管理信息</li><li>往后是用来存储 log buffer 中的 block 镜像的</li></ul></li><li>LSN<ul><li>Log Sequence Number 日志序列号，简称 LSN，初始值  8704<ul><li>redo 日志都有一个唯一的 LSN 值与其对应，LSN 值越小，说明 redo 日志产生的越早</li><li>包括了写到log buffer而没有刷新到磁盘的日志</li></ul></li><li>Log flushed up to (flushed_to_disk_lsn) 表示刷新到磁盘中的 redo 日志量的全局变量</li><li>Pages flushed up to：代表 flush 链表中被最早修改的那个页面对应的 oldest_modification 属性值</li><li>Last checkpoint at：当前系统的 checkpoint_lsn 值</li></ul></li><li>innodb_flush_log_at_trx_commit 默认值1<ul><li>0：表示在事务提交时不立即向磁盘中同步redo 日志，交给后台线程做</li><li>1：表示在事务提交时需要将 redo 日志同步到磁盘，可以保证事务的持久性</li><li>2：表示在事务提交时需要将 redo 日志写到操作系统的缓冲区中，但并不需要保证将日志真正的刷新到磁盘</li></ul></li></ul><hr><h2 id="undo-日志">undo 日志</h2><ul><li>undo 日志（保证事务的原子性）</li><li>事务id<ul><li>给事务分配id 的时机<ul><li>如果某个事务执行过程中对某个表执行了增、删、改操作，会给它分配一个独一无二的事务 id</li><li>只读事务：对某个用户创建的临时表执行增、删、改操作时才会为这个事务分配一个事务 id<ul><li>在执行 SELECT 语句用到内部临时表时并不会为它分配事务id</li></ul></li><li>否则的话是不分配事务 id 的</li></ul></li><li>事务 id 生成机制：本质上就是一个数字，全局变量自增<ul><li>当这个变量的值为 256 的倍数时，就会将该变量的值刷新到系统表空间的页号为5 的页面中一个称之为 Max Trx ID 的属性处（8字节）</li><li>下一次重新启动时，会将上边提到的 Max Trx ID 属性加载到内存中，将该值加上256 之后赋值给我们前边提到的全局变量</li></ul></li></ul></li><li>trx_id 隐藏列：聚簇索引的记录除了会保存完整的用户数据以外，而且还会自动添加名为 trx_id、roll_pointer 的隐藏列，如果用户没有在表中定义主键以及 UNIQUE 键，还会自动添加一个名为 row_id 的隐藏列<ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E6%97%A5%E5%BF%97-%E4%BA%8B%E5%8A%A1-2.png" alt=""></li><li>trx_id 列就是某个对这个聚簇索引记录做改动的语句所在的事务对应的事务id 而已（此处的改动可以是 INSERT、DELETE、UPDATE 操作）</li><li>roll_pointer 本质上就是一个指向记录对应的 undo 日志的一个指针</li></ul></li><li>undo 日志的格式：一般每对一条记录做一次改动，就对应着一条 undo 日志<ul><li>INSERT 操作对应的 undo 日志 TRX_UNDO_INSERT_REC<ul><li>记录 undo 日志时，只需要考虑向聚簇索引插入记录时的情况就好了，因为其实聚簇索引记录和二级索引记录是一一对应的，我们在回滚插入操作时，只需要知道这条记录的主键信息，然后根据主键信息做对应的删除操作，做删除操作时就会顺带着把所有二级索引中相应的记录也删除掉</li></ul></li><li>DELETE 操作对应的 undo 日志 TRX_UNDO_DEL_MARK_REC<ul><li>被删除的记录会根据记录头信息中的 next_record 属性组成一个链表（垃圾链表），Page Header 部分称之为PAGE_FREE 的属性指向由被删除记录组成的垃圾链表中的头节点<ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E6%97%A5%E5%BF%97-%E4%BA%8B%E5%8A%A1-3.png" alt=""></li></ul></li><li><ol><li>将记录的delete_mask 标识位设置为1，这个阶段称之为delete mark</li></ol><ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E6%97%A5%E5%BF%97-%E4%BA%8B%E5%8A%A1-4.png" alt=""></li></ul></li><li><ol start="2"><li>当该删除语句所在的事务提交之后，会有专门的线程后来真正的把记录删除掉，这个阶段称之为 purge</li></ol><ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E6%97%A5%E5%BF%97-%E4%BA%8B%E5%8A%A1-5.png" alt=""></li><li>所谓真正的删除就是把该记录从正常记录链表中移除，并且加入到垃圾链表中</li><li>然后还要调整一些页面的其他信息</li></ul></li><li>在删除语句所在的事务提交之前，只会经历阶段一，也就是 delete mark 阶段，所以只需考虑对删除操作的阶段一做的影响进行回滚</li><li>版本链：在对一条记录进行 delete mark 操作前，需要把该记录的旧的 trx_id 和roll_pointer 隐藏列的值都给记到对应的undo 日志中来，可以通过 undo 日志的old roll_pointer 找到记录在修改之前对应的 undo 日志<ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E6%97%A5%E5%BF%97-%E4%BA%8B%E5%8A%A1-6.png" alt=""></li></ul></li></ul></li><li>UPDATE 操作对应的 undo 日志<ul><li>不更新主键的情况 TRX_UNDO_UPD_EXIST_REC<ul><li>占用的存储空间不发生变化：就地更新（in-place update）</li><li>占用的存储空间发生变化：先删除掉旧记录，再插入新记录<ul><li>并不是 delete mark 操作，而是真正的删除掉，也就是把这条记录从正常记录链表中移除并加入到垃圾链表中（同步执行），并且修改页面中相应的统计信息</li><li>如果新创建的记录占用的存储空间大小不超过旧记录占用的空间，那么可以直接重用被加入到垃圾链表中的旧记录所占用的存储空间</li></ul></li></ul></li><li>更新主键的情况 TRX_UNDO_DEL_MARK_REC &amp; TRX_UNDO_INSERT_REC （会记录 2 条 undo 日志）<ul><li>将旧记录进行 delete mark 操作（MVCC）</li><li>创建一条新记录</li></ul></li></ul></li></ul></li></ul><hr><h2 id="事务流程">事务流程</h2><ul><li>事务的流程：分为事务的执行流程和事务恢复流程</li><li>事务执行：事务主要主要是通过 Redo Log 和 Undo Log 实现的<ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E6%97%A5%E5%BF%97-%E4%BA%8B%E5%8A%A1-7.png" alt=""><ul><li>只有当前事务相关的所有 Redo Log 刷盘成功，事务才算提交成功</li></ul></li></ul></li><li>事务恢复<ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E6%97%A5%E5%BF%97-%E4%BA%8B%E5%8A%A1-8.png" alt=""><ul><li>如果事务提交之前崩溃或者宕机，先使用Redo Log 恢复数据，然后使用Undo Log 回滚数据</li><li>如果事务提交之后崩溃或者宕机，会使用Redo Log 恢复数据</li></ul></li><li>恢复机制：根据 redo 日志中的各种LSN 值，来确定恢复的起点和终点<ul><li>将redo 日志中的数据，以哈希表的形式，将一个页面下的放到哈希表的一个槽中。之后就可以遍历哈希表，所以可以一次性将一个页面修复好</li></ul></li></ul></li></ul><hr><h2 id="Redo-binlog-Undo-日志的关系">Redo &amp; binlog &amp; Undo 日志的关系</h2><ul><li>Redo 日志和 binlog 日志的关系<ul><li>binlog 是用作人工恢复数据</li><li>redo log 是 InnoDB 引擎特有的，binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用</li><li>redo log 是物理日志，记录的是“在某个数据页上做了什么修改”，恢的速度更快；binlog 是逻辑日志，记录的是这个语句的原始逻辑</li><li>redo log 是“循环写”的日志文件，redo log 只会记录未刷盘的日志，已经刷入磁盘的数据都会从 redo log 这个有限大小的日志文件里删除。binlog 是追加日志，保存的是全量的日志</li><li>当数据库 crash 后，想要恢复未刷盘但已经写入 redo log 和binlog 的数据到内存时，binlog 是无法恢复的。虽然 binlog 拥有全量的日志，但没有一个标志让 innoDB 判断哪些数据已经入表(写入磁盘)，哪些数据还没有</li><li>redo log 不一样，只要刷入磁盘的数据，都会从 redo log 中抹掉，数据库重启后，直接把 redo log 中的数据都恢复至内存就可以了</li></ul></li><li>Redo 日志和 Undo 日志的关系<ul><li>undo log 日志的完整性和可靠性需要 redo log 日志来保证<ul><li>undo log 的写入也会伴随着redo log 的产生，因为undo log也需要持久化的保护</li></ul></li><li>数据库崩溃需要先做 redo log 数据恢复，然后做undo log 回滚（未提交的事务）</li></ul></li><li>同时写 Redo 和 Binlog 怎么保持一致：两阶段事务 2PC（准备阶段；提交阶段）<ul><li>当事务提交时 InnoDB 存储引擎进行 prepare 操作</li><li>MySQL 上层会将数据库、数据表和数据表中的数据的更新操作写入 BinLog</li><li>InnoDB 存储引擎将事务日志写入 Redo Log 文件中</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;redo 日志&lt;/li&gt;
&lt;li&gt;undo 日志&lt;/li&gt;
&lt;li&gt;事务流程&lt;/li&gt;
&lt;li&gt;Redo &amp;amp; binlog &amp;amp; Undo 日志的关系&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;redo-日志&quot;&gt;redo 日志&lt;/h2&gt;
&lt;u</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-InnoDB-存储结构&amp;Buffer Pool</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-innodb-cun-chu-jie-gou-buffer-pool/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-innodb-cun-chu-jie-gou-buffer-pool/</id>
    <published>2024-09-11T01:17:00.000Z</published>
    <updated>2024-09-11T01:24:18.812Z</updated>
    
    <content type="html"><![CDATA[<ul><li>数据结构</li><li>体系结构</li><li>双写机制</li><li>Buffer Pool</li></ul><hr><h2 id="数据结构">数据结构</h2><ul><li>InnoDB 的三大特性：双写缓冲区/双写机制；Buffer Pool；自适应 Hash 索引</li><li>行格式<ul><li>可以在创建或修改表的语句中指定行格式：CREATE TABLE 表名 (列的信息) ROW_FORMAT=行格式名称</li><li>COMPACT<ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-Buffer-Pool-1.png" alt=""></li><li>delete_mask 1 标记该记录是否被删除</li><li>min_rec_mask 1 B+树的每层非叶子节点中的最小记录都会添加该标记</li><li>n_owned 4 表示当前记录拥有的记录数</li><li>heap_no 13 表示当前记录在页的位置信息</li><li>record_type 3 表示当前记录的类型，0 表示普通记录，1 表示B+树非叶子节点记录，2 表示最小记录，3 表示最大记录</li><li>next_record 16 表示下一条记录的相对位置</li><li>DB_ROW_ID(row_id)：非必须，6 字节，表示行ID，唯一标识一条记录</li><li>DB_TRX_ID：必须，6 字节，表示事务ID</li><li>DB_ROLL_PTR：必须，7 字节，表示回滚指针</li><li>InnoDB 表对主键的生成策略是：<ul><li>优先使用用户自定义主键作为主键</li><li>如果用户没有定义主键，则选取一个Unique 键作为主键</li><li>如果表中连Unique 键都没有定义的话，则 InnoDB 会为表默认添加一个名为 row_id 的隐藏列作为主键</li></ul></li></ul></li><li>Redundant MySQL5.0 之前用的一种行格式</li><li>Dynamic &amp; Compressed<ul><li>MySQL5.7 的默认行格式就是Dynamic</li><li>Dynamic 和Compressed 行格式和Compact 行格式挺像，只不过在处理行溢出数据时有所不同<ul><li>数据溢出：一个页存放不了一条记录的情况</li><li>Dynamic 和Compressed 行格式，不会在记录的真实数据处存储字段真实数据，而是把所有的字节都存储到其他页面中，只在记录的真实数据处存储其他页面的地址</li></ul></li><li>Compressed 行格式和 Dynamic 不同的一点是：Compressed 行格式会采用压缩算法对页面进行压缩，以节省空间</li></ul></li></ul></li><li>索引页格式：一个页的大小一般是16KB<ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-Buffer-Pool-2.png" alt=""><ul><li>File Header 文件头部， 38 字节，   页的一些通用信息</li><li>Page Header 页面头部， 56 字节， 数据页专有的一些信息</li><li>Infimum + Supremum 最小记录和最大记录， 26 字节， 两个虚拟的行记录</li><li>User Records 用户记录， 大小不确定， 实际存储的行记录内容</li><li>Free Space 空闲空间， 大小不确定， 页中尚未使用的空间</li><li>Page Directory 页面目录， 大小不确定， 页中的某些记录的相对位置</li><li>File Trailer 文件尾部， 8 字节， 校验页是否完整</li></ul></li><li>生成页的时候，其实并没有User Records 这个部分，每当插入一条记录，都会从Free Space 部分，也就是尚未使用的存储空间中申请一个记录大小的空间划分到User Records 部分</li><li>按照主键从小到大的顺序形成了一个单链表，记录被删除，则从这个链表上摘除</li><li>Page Directory 主要是解决记录链表的查找问题：为页中的记录再制作了一个目录<ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-Buffer-Pool-3.png" alt=""><ul><li>将所有正常的记录（包括最大和最小记录，不包括标记为已删除的记录）划分为几个组</li><li>每个组的最后一条记录（也就是组内最大的那条记录）的头信息中的 n_owned 属性表示该记录拥有多少条记录</li><li>将每个组的最后一条记录的地址偏移量单独提取出来按顺序存储到靠近页的尾部的地方，这个地方就是所谓的Page Directory，也就是页目录页面目录中的这些地址偏移量被称为槽（英文名：Slot），所以这个页面目录就是由槽组成 的</li><li>每个分组中的记录条数是有规定的：对于最小记录所在的分组只能有 1 条记录，最大记录所在的分组拥有的记录条数只能在 1~8 条之间，剩下的分组中记录的条数范围只能在是 4~8 条之间</li></ul></li><li>通过二分法确定该记录所在的槽，并找到该槽所在分组中主键值最小的那条记录</li><li>通过记录的next_record 属性遍历该槽所在的组中的各个记录</li></ul></li><li>Page Header：一个数据页中存储的记录的状态信息，比如本页中已经存储了多少条记录，第一条记录的地址是什么，页目录中存储了多少个槽等</li><li>File Header：针对各种类型的页都通用，比如页的类型，这个页的编号是多少，它的上一个页、下一个页是谁，页的校验和等</li><li>File Trailer：为了检测一个页是否完整<ul><li>前4 个字节代表页的校验和，这个部分是和File Header 中的校验和相对应的</li><li>后4 个字节代表页面被最后修改时对应的日志序列位置（LSN），这个也和校验页的完整性有关</li></ul></li></ul></li></ul><hr><h2 id="体系结构">体系结构</h2><ul><li>InnoDB 的体系结构<ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-Buffer-Pool-4.png" alt=""><ul><li>Insert/Change Buffer 主要是用于对二级索引的写入优化</li><li>Undo 空间则是undo 日志一般放在系统表空间，但是通过参数配置后，也可以用独立表空间存放</li><li>通用表空间和独立表空间不同，通用表空间是允许多个表存储数据的共享表空间。</li></ul></li><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-Buffer-Pool-5.png" alt=""></li></ul></li><li>InnoDB 的表空间：任何类型的页都有专门的地方保存页属于哪个表空间，同时表空间中的每一个页都对应着一个页号，这个页号由4 个字节组成，也就是32 个比特位，所以一个表空间最多可以拥有232 个页，如果按照页的默认大小16KB 来算，一个表空间最多支持64TB 的数据<ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-Buffer-Pool-6.png" alt=""></li><li>独立表空间结构<ul><li>区（extent）：连续的64 个页就是一个区，也就是说一个区默认占用1MB 空间大小<ul><li>如果是以页为单位来分配存储空间的话，双向链表相邻的两个页之间的物理位置可能离得非常远</li><li>可以消除很多的随机I/O</li></ul></li><li>组：每256个区又被划分成一个组<ul><li>第一个组最开始的3 个页面的类型是固定的：用来登记整个表空间的一些整体属性以及本组所有的区被称为FSP_HDR，也就是extent 0 ~ extent 255 这256个区，整个表空间只有一个FSP_HDR</li><li>其余各组最开始的2 个页面的类型是固定的，一个XDES 类型，用来登记本组256 个区的属性，FSP_HDR 类型的页面其实和XDES 类型的页面的作用类似，只不过FSP_HDR 类型的页面还会额外存储一些表空间的属性</li></ul></li><li>段（segment）：存放叶子节点的区的集合就算是一个段，存放非叶子节点的区的集合也算是一个段<ul><li>也就是说一个索引会生成2 个段，一个叶子节点段，一个非叶子节点段</li><li>段其实不对应表空间中某一个连续的物理区域，而是一个逻辑上的概念</li></ul></li></ul></li></ul></li><li>系统表空间结构：系统表空间的结构和独立表空间基本类似，只不过由于整个MySQL 进程只有一个系统表空间</li><li>InnoDB 数据字典 (Data Dictionary Header)<ul><li>SYS_TABLES 整个InnoDB 存储引擎中所有的表的信息</li><li>SYS_COLUMNS 整个InnoDB 存储引擎中所有的列的信息</li><li>SYS_INDEXES 整个InnoDB 存储引擎中所有的索引的信息</li><li>SYS_FIELDS 整个InnoDB 存储引擎中所有的索引对应的列的信息</li><li>SYS_FOREIGN 整个InnoDB 存储引擎中所有的外键的信息</li><li>SYS_FOREIGN_COLS 整个InnoDB 存储引擎中所有的外键对应列的信息</li><li>SYS_TABLESPACES 整个InnoDB 存储引擎中所有的表空间信息</li><li>SYS_DATAFILES 整个InnoDB 存储引擎中所有的表空间对应文件系统的文件路径信息</li><li>SYS_VIRTUAL 整个InnoDB 存储引擎中所有的虚拟生成列的信息</li><li>这些表的元数据，用一个固定的页面来记录<ul><li>这个页面就是页号为7 的页面Data Dictionary Header，类型为SYS，记录了数据字典的头部信息</li><li>整个InnoDB 存储引擎的一些全局属性，比如Row ID：Max Row ID 字段是全局共享的</li></ul></li><li>用户是不能直接访问InnoDB 的这些内部系统表的，在information_schema 数据库中的这些以INNODB_SYS 开头的表并不是真正的内部系统表（内部系统表是以SYS 开头的那些表），而是在存储引擎启动时读取这些以SYS 开头的系统表，然后填充到这些以INNODB_SYS 开头的表中</li></ul></li></ul><hr><h2 id="双写机制">双写机制</h2><ul><li>双写缓冲区/双写机制：一种特殊文件flush 技术，大小是2MB，性能降低了大概5-10%左右<ul><li>在把页写到数据文件之前，InnoDB 先把它们写到一个叫doublewrite buffer（双写缓冲区）的连续区域内，在写doublewrite buffer 完成后，InnoDB 才会把页写到数据文件的适当的位置</li><li>如果在写页的过程中发生意外崩溃，InnoDB在稍后的恢复过程中在doublewrite buffer 中找到完好的page 副本用于恢复</li><li>为了解决部分页写入问题，当MySQL 将脏数据flush到数据文件的时候, 先使用memcopy 将脏数据复制到内存中的一个区域（也是2M），之后通过这个内存区域再分2 次，每次写入1MB 到系统表空间，然后马上调用fsync 函数，同步到磁盘上。在这个过程中是顺序写，开销并不大，在完成doublewrite 写入后，再将数据写入各数据文件文件，这时是离散写入</li></ul></li></ul><hr><h2 id="Buffer-Pool">Buffer Pool</h2><ul><li>Buffer Pool：默认 128M<ul><li>一般分配机器内存的 70%~75%（分配的总空间比指定的缓冲池大小大约大 10%）</li><li><code>show engine innodb status</code> 对于读取多的情况,如果没达到98%以上，都说明buffer 不够</li></ul></li><li>缓存页<ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-Buffer-Pool-7.png" alt=""></li><li>一页 16KB</li><li>每个缓存页对应的控制信息占用的内存大小是相同的，我们称为控制块</li><li>控制块和缓存页是一一对应的</li><li>每个控制块大约占用缓存页大小的5%，而我们设置的innodb_buffer_pool_size并不包含这部分控制块占用的内存空间大小</li></ul></li><li>free 链表<ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-Buffer-Pool-8.png" alt=""></li><li>可以把所有空闲的缓存页对应的控制块作为一个节点放到一个链表中，这个链表也可以被称作free 链表（或者说空闲链表）。刚刚完成初始化的Buffer Pool 中所有的缓存页都是空闲的</li><li>每当需要从磁盘中加载一个页到Buffer Pool 中时就从free 链表中取一个空闲的缓存页，并且把该缓存页对应的控制块的信息填上（就是该页所在的表空间、页号之类的信息），然后把该缓存页对应的free链表节点从链表中移除，表示该缓存页已经被使用了</li></ul></li><li>flush 链表：再创建一个存储脏页的链表，凡是修改过的缓存页对应的控制块都会作为一个节点加入到一个链表中</li><li>缓存页的哈希处理：用表空间号 + 页号作为 key，缓存页作为 value 创建一个哈希表</li><li>划分区域的LRU 链表<ul><li>预读<ul><li>线性预读 <code>innodb_read_ahead_threshold</code>（默认56）：如果顺序访问了某个区（extent）的页面超过这个系统变量的值，就会触发一次异步读取下一个区中全部的页面到Buffer Pool 的请求</li><li>随机预读 <code>innodb_random_read_ahead</code>（长期有效性很低，默认关闭）：如果Buffer Pool 中已经缓存了某个区的13 个连续的页面，不论这些页面是不是顺序读取的，都会触发一次异步读取本区中所有其他的页面到Buffer Pool 的请求</li></ul></li><li>分区：按照某个比例将LRU 链表分成两半 <code>innodb_old_blocks_pct</code><ul><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-Buffer-Pool-9.png" alt=""></li><li>一部分存储使用频率非常高的缓存页，所以这一部分链表也叫做热数据，或者称young 区域<ul><li>只有被访问的缓存页位于young 区域的1/4 的后边，才会被移动到LRU 链表头部（在young 区域的缓存页都是热点数据）</li></ul></li><li>一部分存储使用频率不是很高的缓存页，所以这一部分链表也叫做冷数据，或者称old 区域<ul><li>当磁盘上的某个页面在初次加载到 Buffer Pool 中的某个缓存页时，该缓存页对应的控制块会被放到 old 区域的头部</li><li><code>innodb_old_blocks_time</code>：在对某个处在old 区域的缓存页进行第一次访问时就在它对应的控制块中记录下来这个访问时间，如果后续的访问时间与第一次访问的时间在某个时间间隔内，那么该页面就不会被从old 区域移动到young 区域的头部</li></ul></li></ul></li></ul></li><li>刷新脏页到磁盘：后台有专门的线程每隔一段时间负责把脏页刷新到磁盘<ul><li>从 LRU 链表的冷数据中刷新一部分页面到磁盘（innodb_lru_scan_depth）：后台线程会定时从LRU 链表尾部开始扫描一些页面</li><li>从 flush 链表中刷新一部分页面到磁盘</li><li>用户线程没有可用的缓存页时，会尝试看看LRU 链表尾部有没有可以直接释放掉的未修改页面，如果没有的话会不得不将 LRU 链表尾部的一个脏页同步刷新到磁盘</li><li>系统特别繁忙时，也可能出现用户线程批量的从flush 链表中刷新脏页的情况</li></ul></li><li>多个 Buffer Pool 实例（<code>innodb_buffer_pool_instances</code>）：在多线程环境下，访问 Buffer Pool 中的各种链表都需要加锁处理<ul><li>innodb_buffer_pool_instances 能设置的最大值是64</li><li>当innodb_buffer_pool_size（默认128M）的值小于1G 的时候设置多个实例是无效的</li><li>让每个 Buffer Pool 实例达到 1 个G</li></ul></li><li>chunk 的大小 <code>innodb_buffer_pool_chunk_size</code>  只能在服务器启动时指定<ul><li>一个 Buffer Pool 实例其实是由若干个 chunk 组成的</li><li>在服务器运行期间调整Buffer Pool的大小时就是以chunk 为单位增加或者删除内存空间</li><li>Buffer Pool 的缓存页除了用来缓存磁盘上的页面以外，还可以存储锁信息、自适应哈希索引等信息</li><li><img src="/static/IT/MySQL/MySQL-InnoDB-%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84-Buffer-Pool-10.png" alt=""></li></ul></li><li>查看 Buffer Pool 的状态信息 <code>SHOW ENGINE INNODB STATUS</code></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;数据结构&lt;/li&gt;
&lt;li&gt;体系结构&lt;/li&gt;
&lt;li&gt;双写机制&lt;/li&gt;
&lt;li&gt;Buffer Pool&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;数据结构&quot;&gt;数据结构&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;InnoDB 的三大特性：双写缓冲区/双写机制；Buff</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-Explain</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-explain/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-explain/</id>
    <published>2024-09-11T01:12:00.000Z</published>
    <updated>2024-09-11T01:14:48.089Z</updated>
    
    <content type="html"><![CDATA[<ul><li>EXPLAIN</li><li>EXPLAIN 列</li><li>索引最佳实践</li></ul><hr><h2 id="EXPLAIN">EXPLAIN</h2><ul><li>EXPLAIN关键字可以模拟优化器执行SQL语句：执行查询会返回执行计划的信息，而不是执行这条SQL<ul><li>如果 from 中包含子查询，仍会执行该子查询，将结果放入临时表中</li></ul></li><li>explain extended：会在 explain  的基础上额外提供一些查询优化的信息<ul><li>filtered：rows * filtered/100 可以估算出将要和 explain 中前一个表进行连接的行数（前一个表指 explain 中的id值比当前表id值小的表）</li><li>show warnings：得到优化后的查询语句</li></ul></li><li>explain partitions<ul><li>partitions：如果查询是基于分区表的话，会显示查询将访问的分区</li></ul></li></ul><hr><h2 id="EXPLAIN-列">EXPLAIN 列</h2><p>explain中的列：</p><ul><li>id 列<ul><li>id 列的编号是 select 的序列号，有几个 select 就有几个id，并且id的顺序是按 select 出现的顺序增长的</li><li>id 列越大执行优先级越高，id相同则从上往下执行，id为NULL最后执行</li></ul></li><li>select_type 列表示对应行是简单还是复杂的查询<ul><li>simple：简单查询。查询不包含子查询和 union</li><li>primary：复杂查询中最外层的 select</li><li>subquery：包含在 select 中的子查询（不在 from 子句中）</li><li>derived：包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表中，也称为派生表（derived的英文含义）</li><li>union：在 union 中的第二个和随后的 select</li></ul></li><li>table 列表示 explain 的一行正在访问哪个表<ul><li>当 from 子句中有子查询时，table列是 <code>&lt;derivenN&gt;</code> 格式，表示当前查询依赖 id=N 的查询，于是先执行 id=N 的查询</li><li>当有 union 时，UNION RESULT 的 table 列的值为 <code>&lt;union1,2&gt;</code>，1和2表示参与 union 的 select 行 id</li></ul></li><li>partitions 列：如果查询是基于分区表的话，partitions 字段会显示查询将访问的分区</li><li>type 列表示关联类型或访问类型，即 MySQL 决定如何查找表中的行，查找数据行记录的大概范围<ul><li>system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL</li><li>一般来说，得保证查询达到range级别，最好达到 ref</li><li>NULL：mysql能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引。例如：在索引列中选取最小值，可以单独查找索引来完成，不需要在执行时访问表</li><li>const, system：mysql能对查询的某部分进行优化并将其转化成一个常量（可以看show warnings 的结果）。用于primary key 或 unique key 的所有列与常数比较时，所以表最多有一个匹配行，读取1次，速度比较快。system是const的特例，表里只有一条元组匹配时为system</li><li>eq_ref：primary key 或 unique key 索引的所有部分被连接使用 ，最多只会返回一条符合条件的记录。这可能是在const 之外最好的联接类型了，简单的 select 查询不会出现这种 type</li><li>ref：相比 eq_ref，不使用唯一索引，而是使用普通索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会找到多个符合条件的行</li><li>range：范围扫描通常出现在 <code>in(), between ,&gt; ,&lt;, &gt;=</code> 等操作中。使用一个索引来检索给定范围的行</li><li>index：扫描全索引就能拿到结果，一般是扫描某个二级索引，这种扫描不会从索引树根节点开始快速查找，而是直接对二级索引的叶子节点遍历和扫描，速度还是比较慢的，这种查询一般为使用覆盖索引，二级索引一般比较小，所以这种通常比ALL快一些</li><li>ALL：即全表扫描，扫描你的聚簇索引的所有叶子节点。通常情况下这需要增加索引来进行优化了</li></ul></li><li>possible_keys 列显示查询可能使用哪些索引来查找<ul><li>explain 时可能出现 possible_keys 有列，而 key 显示 NULL 的情况，这种情况是因为表中数据不多，mysql认为索引对此查询帮助不大，选择了全表查询</li><li>如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查 where 子句看是否可以创造一个适当的索引来提高查询性能，然后用 explain 查看效果</li></ul></li><li>key 列显示mysql实际采用哪个索引来优化对该表的访问<ul><li>如果没有使用索引，则该列是 NULL。如果想强制mysql使用或忽视possible_keys列中的索引，在查询中使用 force index、ignore index</li></ul></li><li>key_len 列显示了mysql在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列<ul><li>比如说，通过结果中的key_len可推断出查询使用了联合索引的第几列</li><li>key_len 计算规则<ul><li>字符串，char(n)和varchar(n)，n均代表字符数，而不是字节数，如果是utf-8，一个数字或字母占1个字节，一个汉字占3个字节<ul><li>char(n)：如果存汉字长度就是 3n 字节</li><li>varchar(n)：如果存汉字则长度是 3n + 2 字节，加的2字节用来存储字符串长度，因为varchar是变长字符串</li></ul></li><li>数值类型：tinyint：1字节；smallint：2字节；int：4字节；bigint：8字节</li><li>时间类型：date：3字节；timestamp：4字节；datetime：8字节</li><li>如果字段允许为 NULL，需要1字节记录是否为 NULL</li><li>索引最大长度是768字节，当字符串过长时，mysql会做一个类似左前缀索引的处理，将前半部分的字符提取出来做索引</li></ul></li></ul></li><li>ref 列显示了在key列记录的索引中，表查找值所用到的列或常量，常见的有：const（常量），字段名</li><li>rows 列是mysql估计要读取并检测的行数，注意这个不是结果集里的行数</li><li>filtered 列是一个百分比的值，<code>rows * filtered/100</code> 可以估算出将要和 explain 中前一个表进行连接的行数（前一个表指 explain 中的id值比当前表id值小的表）</li><li>Extra 列展示的是额外信息<ul><li>Using index：使用覆盖索引<ul><li>mysql执行计划explain结果里的key有使用索引，如果select后面查询的字段都可以从这个索引的树中获取，这种情况一般可以说是用到了覆盖索引，extra里一般都有using index</li><li>覆盖索引一般针对的是辅助索引，整个查询结果只通过辅助索引就能拿到结果，不需要通过辅助索引树找到主键，再通过主键去主键索引树里获取其它字段值</li></ul></li><li>Using where：使用 where 语句来处理结果，并且查询的列未被索引覆盖</li><li>Using index condition：查询的列不完全被索引覆盖，where 条件中是一个前导列的范围</li><li>Using temporary：mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索引来优化</li><li>Using filesort：将用外部排序而不是索引排序，数据较小时从内存排序，否则需要在磁盘完成排序。这种情况下一般也是要考虑使用索引来优化的</li><li>Select tables optimized away：使用某些聚合函数（比如 max、min）来访问存在索引的某个字段</li></ul></li></ul><hr><h2 id="索引最佳实践">索引最佳实践</h2><p>索引最佳实践：</p><ul><li>全值匹配</li><li>最左前缀法则</li><li>不在索引列上做任何操作（计算、函数、（自动or手动）类型转换），会导致索引失效而转向全表扫描</li><li>存储引擎不能使用索引中范围条件右边的列</li><li>尽量使用覆盖索引（只访问索引的查询，索引列包含查询列），减少 select * 语句</li><li>mysql在使用不等于（!=或者&lt;&gt;），not in ，not exists 的时候无法使用索引会导致全表扫描</li><li>&lt; 小于、 &gt; 大于、 &lt;=、&gt;= 这些，mysql内部优化器会根据检索比例、表大小等多个因素整体评估是否使用索引</li><li><code>is null, is not null</code> 一般情况下也无法使用索引</li><li>like 以通配符开头（‘$abc…’）mysql索引失效会变成全表扫描操作<ul><li>使用覆盖索引，查询字段必须是建立覆盖索引字段</li><li>如果不能使用覆盖索引则可能需要借助搜索引擎</li></ul></li><li>字符串不加单引号索引失效</li><li>少用or或in，用它查询时，mysql不一定使用索引，mysql内部优化器会根据检索比例、表大小等多个因素整体评估是否使用索引</li><li>范围查询优化<ul><li>mysql内部优化器会根据检索比例、表大小等多个因素整体评估是否使用索引，比如由于单次数据量查询过大导致优化器最终选择不走索引</li><li>可以将大的范围拆分成多个小范围</li></ul></li><li>like KK%相当于=常量，%KK和%KK% 相当于范围</li></ul><p><img src="/static/IT/MySQL/MySQL-Explain-1.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;EXPLAIN&lt;/li&gt;
&lt;li&gt;EXPLAIN 列&lt;/li&gt;
&lt;li&gt;索引最佳实践&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;EXPLAIN&quot;&gt;EXPLAIN&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;EXPLAIN关键字可以模拟优化器执行SQL语句：执行查询会返回执</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-性能优化</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-xing-neng-you-hua/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-xing-neng-you-hua/</id>
    <published>2024-09-11T01:08:00.000Z</published>
    <updated>2024-09-11T01:11:18.956Z</updated>
    
    <content type="html"><![CDATA[<ul><li>慢查询<ul><li><code>set GLOBAL slow_query_log=1;</code></li><li>`set global long_query_time=0;</li><li><code>set global log_output='FILE,TABLE'</code></li><li><code>mysqldumpslow -s r -t 10 slow-mysql.log</code></li></ul></li><li>优化SQL查询方法论<ul><li>查询不需要的记录</li><li>重复查询相同的数据</li><li>是否在扫描额外的记录：响应时间、扫描的行数、返回的行数</li></ul></li><li>重构SQL查询的方法论<ul><li>切分查询</li><li>分解关联查询</li></ul></li><li>MySQL执行全流程<ol><li>客户端发送一条查询给服务器</li><li>服务器先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段（当然从MySQL8.0开始，这个部分就没有了）</li><li>服务器端进行SQL解析、预处理，再由优化器生成对应的执行计划</li><li>MySQL根据优化器生成的执行计划，调用存储引擎的API来执行查询</li><li>将结果返回给客户端</li></ol></li><li>MySQL客户端/服务器通信<ul><li>半双工：在任何一个时刻，要么是由服务器向客户端发送数据，要么是由客户端向服务器发送数据</li><li>一旦一端开始发生消息，另一端要接收完整个消息才能响应它<ul><li>当客户端从服务器取数据时，看起来是一个拉数据的过程，但实际上是MySQL在向客户端推送数据的过程。客户端不断地接收从服务器推送的数据，客户端也没法让服务器停下来</li></ul></li><li>服务器端游标，每次从服务器取fetch_size条数据<ul><li><code>setResultSetType(ResultSet.TYPE_FORWARD_ONLY); setFetchSize(Integer.MIN_VALUE)</code></li><li><code>statement#enableStreamingResults</code></li><li><code>useCursorFetch=true</code></li></ul></li></ul></li><li><code>show processlist</code> 查看线程状态</li><li><code>show profile</code><ul><li><code>set profiling=1;</code></li><li><code>show profile for query 1;</code></li><li><code>show profile all for query 1</code></li></ul></li></ul><hr><p><img src="/static/IT/MySQL/MySQL-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-1.png" alt=""><br><img src="/static/IT/MySQL/MySQL-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-2.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;慢查询
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;set GLOBAL slow_query_log=1;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;`set global long_query_time=0;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;set global log_output</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-索引优化</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-suo-yin-you-hua/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-suo-yin-you-hua/</id>
    <published>2024-09-11T01:03:00.000Z</published>
    <updated>2024-09-11T01:07:27.693Z</updated>
    
    <content type="html"><![CDATA[<ul><li>索引优化原则</li><li>SQL 索引优化</li><li>索引设计规范</li><li>SQL 优化</li><li>数据类型选择</li></ul><hr><h2 id="索引优化原则">索引优化原则</h2><ul><li>联合索引第一个字段用范围不会走索引</li><li>强制走索引：最终查找效率不一定比全表扫描高，因为回表效率不高</li><li>覆盖索引优化</li><li>in和or在表数据量比较大的情况会走索引，在表记录不多的情况下会选择全表扫描</li><li>like&nbsp;KK%&nbsp;一般情况都会走索引</li><li>索引下推：可以在索引遍历过程中，对索引中包含的所有字段先做判断，过滤掉不符合条件的记录之后再回表</li><li>MRR (Disk-Sweep Multi-Range Read) 多范围读取<ul><li>即先读取一部分二级索引记录，将它们的主键值排好序之后再统一执行回表操作</li></ul></li></ul><h2 id="SQL-索引优化">SQL 索引优化</h2><ul><li>覆盖索引优化</li><li>trace工具<ul><li><code>set&nbsp;session&nbsp;optimizer_trace="enabled=on",end_markers_in_json=on;</code></li><li><code>SELECT&nbsp;*&nbsp;FROM&nbsp;information_schema.OPTIMIZER_TRACE;</code></li></ul></li><li>Order&nbsp;by与Group&nbsp;by：也要按照联合索引的顺序<ul><li>Mysql8以上才有降序排序，否则会产生 Using&nbsp;filesort，因为默认升序</li><li>对于排序来说，多个相等条件也是范围查询</li><li>MySQL支持两种方式的排序filesort和index<ul><li>Using&nbsp;index是指MySQL扫描索引本身完成排序</li><li>order&nbsp;by满足两种情况会使用Using&nbsp;index<ul><li>order&nbsp;by语句使用索引最左前列</li><li>使用where子句与order&nbsp;by子句条件列组合满足索引最左前列</li></ul></li></ul></li><li>尽量在索引列上完成排序，遵循索引建立（索引创建的顺序）时的最左前缀法则</li><li>如果order&nbsp;by的条件不在索引列上，就会产生Using&nbsp;filesort</li><li>能用覆盖索引尽量用覆盖索引</li><li>group&nbsp;by与order&nbsp;by很类似，其实质是先排序后分组，遵照索引创建顺序的最左前缀法则</li><li>group&nbsp;by如果不需要排序的可以加上order&nbsp;by&nbsp;null禁止排序</li><li>where高于having，能写在where中的限定条件就不要去having限定了</li></ul></li><li>filesort文件排序方式：<ul><li>单路排序：是一次性取出满足条件行的所有字段，然后在sort&nbsp;buffer中进行排序；用trace工具可以看到sort_mode信息里显示<code>&lt;&nbsp;sort_key,&nbsp;additional_fields&nbsp;&gt;</code>或者<code>&lt;&nbsp;sort_key,&nbsp;packed_additional_fields&nbsp;&gt;</code></li><li>双路排序（又叫回表排序模式）：是首先根据相应的条件取出相应的排序字段和可以直接定位行数据的行&nbsp;ID，然后在&nbsp;sort&nbsp;buffer&nbsp;中进行排序，排序完后需要再次取回其它需要的字段；用trace工具可以看到sort_mode信息里显示<code>&lt;&nbsp;sort_key,&nbsp;rowid&nbsp;&gt;</code></li><li>通过比较系统变量&nbsp;max_length_for_sort_data(默认1024字节)&nbsp;的大小和需要查询的字段总大小来判断使用哪种排序模式：小于用单路；大于用双路</li></ul></li></ul><hr><h2 id="索引设计规范">索引设计规范</h2><p>索引设计原则</p><ul><li>代码先行，索引后上</li><li>联合索引尽量覆盖条件</li><li>不要在小基数字段上建立索引</li><li>长字符串我们可以采用前缀索引（前20个字符）</li><li>where与order&nbsp;by冲突时优先where</li><li>基于慢sql查询做优化</li><li>尽量利用一两个复杂的多字段联合索引，抗下80%以上的查询，然后用一两个辅助索引尽量抗下剩余的一些非典型查询，保证大数据量表的查询尽可能多的都能充分利用索引</li></ul><p>索引设计策略</p><ul><li>索引列的类型尽量小</li><li>利用索引选择性（索引值的离散程度）和前缀索引</li><li>只为用于搜索、排序或分组的列创建索引</li><li>合理设计多列索引</li><li>尽可能设计三星索引（覆盖索引）：索引中所包含了这个查询所需的所有列</li><li>主键尽量是很少改变的列</li><li>避免创建冗余和重复索引</li><li>删除未使用的索引</li></ul><hr><h2 id="SQL-优化">SQL 优化</h2><ul><li>分页查询优化<ul><li>根据自增且连续的主键排序的分页查询（主键空缺，导致结果不一致）</li><li>根据非主键字段排序的分页查询<ul><li>让排序时返回的字段尽可能少：比如用<code>inner&nbsp;join</code></li><li>select&nbsp;*&nbsp;from&nbsp;employees&nbsp;e&nbsp;inner&nbsp;join&nbsp;(select&nbsp;id&nbsp;from&nbsp;employees&nbsp;order&nbsp;by&nbsp;name&nbsp;limit&nbsp;90000,5)&nbsp;ed on&nbsp;<a href="http://e.id">e.id</a>&nbsp;=&nbsp;<a href="http://ed.id">ed.id</a>;</li></ul></li></ul></li><li>Join关联查询优化<ul><li>嵌套循环连接&nbsp;Nested-Loop&nbsp;Join(NLJ)&nbsp;算法：优化器一般会优先选择小表做驱动表，被驱动表的关联字要走索引<ul><li>一次一行循环地从第一张表（称为驱动表）中读取行，在这行数据中取到关联字段，根据关联字段在另一张表（被驱动表）里取出满足条件的行，然后取出两张表的结果合集</li><li>如果执行计划&nbsp;Extra&nbsp;中未出现&nbsp;Using&nbsp;join&nbsp;buffer&nbsp;则表示使用的&nbsp;join&nbsp;算法是&nbsp;NLJ</li></ul></li><li>基于块的嵌套循环连接&nbsp;Block&nbsp;Nested-Loop&nbsp;Join(BNL) 算法：被驱动表的关联字段没索引（用BNL磁盘扫描次数少很多）<ul><li>把驱动表的数据读入到&nbsp;join_buffer&nbsp;中，然后扫描被驱动表，把被驱动表每一行取出来跟&nbsp;join_buffer&nbsp;中的数据做对比<ul><li>要是 join_buffer&nbsp;放不下，就分段放</li></ul></li><li>Extra&nbsp;中&nbsp;的Using&nbsp;join&nbsp;buffer&nbsp;(Block&nbsp;Nested&nbsp;Loop)说明该关联查询使用的是&nbsp;BNL&nbsp;算法</li></ul></li></ul></li><li>对于关联sql的优化<ul><li>关联字段加索引，让mysql做join操作时尽量选择NLJ算法</li><li>小表驱动大表，写多表连接sql时如果明确知道哪张表是小表可以用straight_join写法固定连接驱动方式，省去mysql优化器自己判断的时间<ul><li>straight_join功能同join类似，但能让左边的表来驱动右边的表，能改表优化器对于联表查询的执行顺序</li><li>straight_join只适用于inner&nbsp;join，并不适用于left&nbsp;join，right&nbsp;join<ul><li>left&nbsp;join，right&nbsp;join已经代表指定了表的执行顺序</li></ul></li><li>尽可能让优化器去判断</li></ul></li></ul></li><li>小表定义：两个表按照各自的条件过滤，过滤完成之后，计算参与&nbsp;join&nbsp;的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表</li><li>in和exsits优化：小表驱动大表<ul><li>in：当B表的数据集小于A表的数据集时，in优于exists<ul><li><code>select&nbsp;*&nbsp;from&nbsp;A&nbsp;where&nbsp;id&nbsp;in&nbsp;(select&nbsp;id&nbsp;from&nbsp;B)</code></li></ul></li><li>exists：当A表的数据集小于B表的数据集时，exists优于in<ul><li><code>select&nbsp;*&nbsp;from&nbsp;A&nbsp;where&nbsp;exists&nbsp;(select&nbsp;1&nbsp;from&nbsp;B&nbsp;where&nbsp;B.id&nbsp;=&nbsp;A.id)</code></li><li>将主查询A的数据，放到子查询B中做条件验证，根据验证结果（true或false）来决定主查询的数据是否保留</li><li>EXISTS&nbsp;(subquery)只返回TRUE或FALSE,因此子查询中的SELECT&nbsp;*&nbsp;也可以用SELECT&nbsp;1替换,官方说法是实际执行时会忽略SELECT清单,因此没有区别</li><li>EXISTS子查询的实际执行过程可能经过了优化而不是我们理解上的逐条对比</li><li>EXISTS子查询往往也可以用JOIN来代替，何种最优需要具体问题具体分析</li></ul></li></ul></li><li><code>count(*)</code>查询优化<ul><li>字段有索引：<code>count(*)≈count(1)&gt;count(字段)&gt;count(主键&nbsp;id)</code><ul><li>count(字段)统计走二级索引，二级索引存储数据比主键索引少</li></ul></li><li>字段无索引：<code>count(*)≈count(1)&gt;count(主键&nbsp;id)&gt;count(字段)</code></li><li>count(1)跟count(字段)执行过程类似，不过count(1)不需要取出字段统计，就用常量1做统计，count(字段)还需要取出字段，所以理论上count(1)比count(字段)会快一点</li><li><code>count(*)&nbsp;</code>是例外，mysql并不会把全部字段取出来，而是专门做了优化，不取值，按行累加，效率很高，所以不需要用count(列名)或count(常量)来替代&nbsp;<code>count(*)</code></li><li>对于count(id)，mysql最终选择辅助索引而不是主键聚集索引：因为二级索引相对主键索引存储数据更少</li></ul></li><li>查询mysql自己维护的总行数<ul><li>myisam存储引擎的表做不带where条件的count查询性能是很高的，因为myisam存储引擎的表的总行数会被mysql存储在磁盘上，查询不需要计算</li><li>innodb存储引擎的表mysql不会存储表的总记录行数（因为有MVCC机制），查询count需要实时计算</li><li>表总行数的估计值：show&nbsp;table&nbsp;status</li></ul></li><li>将总数维护到Redis里</li><li>增加数据库计数表</li></ul><hr><h2 id="数据类型选择">数据类型选择</h2><ul><li>MySQL数据类型选择：尽量用更小的数据类型，字段避免使用NULL<ol><li>确定合适的大类型：数字、字符串、时间、二进制</li><li>确定具体的类型：有无符号、取值范围、变长定长等</li></ol></li><li>数值类型<ul><li>如果整形数据没有负数，如ID号，建议指定为UNSIGNED无符号类型，容量可以扩大一倍</li><li>建议使用TINYINT代替ENUM、BITENUM、SET</li><li>避免使用整数的显示宽度，也就是说，不要用INT(10)类似的方法指定字段显示宽度，直接用INT<ul><li>INT显示宽度：这里的长度并非是存储的最大长度，而是显示的最大长度<ul><li>在查询结果前填充0时，命令中加上ZEROFILL就可以实现</li></ul></li><li>如果类型是 TINYINT 但是存入值大于255，如500，那么MySQL会自动保存为TINYINT类型的最大值255</li></ul></li><li>DECIMAL最适合保存准确度要求高，而且用于计算的数据，比如价格。但是在使用DECIMAL类型的时候，注意长度设置</li><li>建议使用整形类型来运算和存储实数，方法是，实数乘以相应的倍数后再操作</li><li>整数通常是最佳的数据类型，因为它速度快，并且能使用 AUTO_INCREMENT 自增</li></ul></li><li>日期和时间<ul><li>MySQL能存储的最小时间粒度为秒</li><li>建议用DATE数据类型来保存日期。MySQL中默认的日期格式是yyyy-mm-dd</li><li>用MySQL的内建类型DATE、TIME、DATETIME来存储时间，而不是使用字符串</li><li>当数据格式为TIMESTAMP和DATETIME时，可以用CURRENT_TIMESTAMP作为默认，MySQL会自动返回记录插入的确切时间</li><li>TIMESTAMP是UTC时间戳，与时区相关</li><li>DATETIME的存储格式是一个YYYYMMDD&nbsp;HH:MM:SS的整数，与时区无关，你存了什么，读出来就是什么</li><li>除非有特殊需求，一般的公司建议使用TIMESTAMP，它比DATETIME更节约空间，但是像阿里这样的公司一般会用DATETIME，因为不用考虑TIMESTAMP将来的时间上限问题</li><li>有时人们把Unix的时间戳保存为整数值，但是这通常没有任何好处，这种格式处理起来不太方便，我们并不推荐它</li></ul></li><li>字符串<ul><li>字符串的长度相差较大用VARCHAR；字符串短，且所有值都接近一个长度用CHA</li><li>CHAR和VARCHAR适用于包括人名、邮政编码、电话号码和不超过255个字符长度的任意字母数字组合<ul><li>那些要用来计算的数字不要用VARCHAR类型保存，因为可能会导致一些与计算相关的问题，影响到计算的准确性和完整性</li></ul></li><li>尽量少用BLOB和TEXT，如果实在要用可以考虑将BLOB和TEXT字段单独存一张表，用id关联</li><li>BLOB系列存储二进制字符串，与字符集无关。TEXT系列存储非二进制字符串，与字符集相关</li><li>BLOB和TEXT都不能有默认值</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;索引优化原则&lt;/li&gt;
&lt;li&gt;SQL 索引优化&lt;/li&gt;
&lt;li&gt;索引设计规范&lt;/li&gt;
&lt;li&gt;SQL 优化&lt;/li&gt;
&lt;li&gt;数据类型选择&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;索引优化原则&quot;&gt;索引优化原则&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;联合索引第</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-索引数据结构</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-suo-yin-shu-ju-jie-gou/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-suo-yin-shu-ju-jie-gou/</id>
    <published>2024-09-11T00:57:00.000Z</published>
    <updated>2024-09-11T00:59:48.774Z</updated>
    
    <content type="html"><![CDATA[<ul><li>B-Tree</li><li>B+Tree（B-Tree变种）</li><li>Hash</li><li>MyISAM 非聚集索引</li><li>InnoDB 聚集索引</li></ul><hr><ul><li>B-Tree<ul><li>叶节点具有相同的深度，叶节点的指针为空</li><li>所有索引元素不重复</li><li>节点中的数据索引从左到右递增排列</li></ul></li></ul><p><img src="/static/IT/MySQL/MySQL-%E7%B4%A2%E5%BC%95%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-1.png" alt=""></p><ul><li>B+Tree（B-Tree变种）<ul><li>非叶子节点不存储data，只存储索引(冗余)，可以放更多的索引</li><li>叶子节点包含所有索引字段</li><li>叶子节点用指针连接，提高区间访问的性能</li></ul></li></ul><p><img src="/static/IT/MySQL/MySQL-%E7%B4%A2%E5%BC%95%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-2.png" alt=""></p><ul><li>Hash<ul><li>对索引的key进行一次hash计算就可以定位出数据存储的位置</li><li>很多时候Hash索引要比 B+ 树索引更高效</li><li>仅能满足 “=”，“IN”，不支持范围查询</li><li>hash 冲突问题</li></ul></li></ul><p><img src="/static/IT/MySQL/MySQL-%E7%B4%A2%E5%BC%95%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-3.png" alt=""></p><hr><p>MyISAM索引文件和数据文件是分离的（非聚集索引）</p><p><img src="/static/IT/MySQL/MySQL-%E7%B4%A2%E5%BC%95%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-4.png" alt=""></p><ul><li>InnoDB 聚集索引<ul><li>表数据文件本身就是按 B+Tree 组织的一个索引结构文件</li><li>聚集索引-叶节点包含了完整的数据记录</li><li>建议InnoDB表必须建主键，并且推荐使用整型的自增主键</li><li>非主键索引结构叶子节点存储的是主键值（一致性和节省存储空间）</li></ul></li></ul><p>主键索引：<br><img src="/static/IT/MySQL/MySQL-%E7%B4%A2%E5%BC%95%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-5.png" alt="主键索引"></p><p>非主键索引：<br><img src="/static/IT/MySQL/MySQL-%E7%B4%A2%E5%BC%95%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-6.png" alt="非主键索引"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;B-Tree&lt;/li&gt;
&lt;li&gt;B+Tree（B-Tree变种）&lt;/li&gt;
&lt;li&gt;Hash&lt;/li&gt;
&lt;li&gt;MyISAM 非聚集索引&lt;/li&gt;
&lt;li&gt;InnoDB 聚集索引&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;B-Tree
&lt;ul&gt;
&lt;li&gt;</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-索引合并</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-suo-yin-he-bing/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-suo-yin-he-bing/</id>
    <published>2024-09-11T00:56:00.000Z</published>
    <updated>2024-09-11T00:56:12.513Z</updated>
    
    <content type="html"><![CDATA[<ul><li>索引合并：一般情况下执行一个查询时最多只会用到单个二级索引，但特殊情况下也可能在一个查询中使用到多个二级索引</li><li>Intersection 交集合并<ul><li>等值匹配<ul><li>在联合索引中的每个列都必须等值匹配，不能出现只匹配部分列的情况</li></ul></li><li>主键列可以是范围匹配<ul><li><code>SELECT * FROM order_exp WHERE id &gt; 100 AND order_no = 'a';</code><ul><li>假设这个查询可以采用 Intersection 索引合并：先获取二级索引 order_no 的主键集，再运行过滤条件 id &gt; 100</li></ul></li></ul></li></ul></li><li>Union 并集合并<ul><li>等值匹配</li><li>主键列可以是范围匹配</li><li>使用Intersection索引合并的搜索条件</li></ul></li><li>Sort-Union 合并：多了一步对二级索引记录的主键值排序的过程<ul><li>因为Union索引合并的使用条件太苛刻，必须保证各个二级索引列在进行等值匹配的条件下才可能被用到</li></ul></li><li>联合索引替代Intersection索引合并</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;索引合并：一般情况下执行一个查询时最多只会用到单个二级索引，但特殊情况下也可能在一个查询中使用到多个二级索引&lt;/li&gt;
&lt;li&gt;Intersection 交集合并
&lt;ul&gt;
&lt;li&gt;等值匹配
&lt;ul&gt;
&lt;li&gt;在联合索引中的每个列都必须等值匹配，不能出现只匹配部</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-事务</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-shi-wu/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-shi-wu/</id>
    <published>2024-09-11T00:52:00.000Z</published>
    <updated>2024-09-11T00:53:46.567Z</updated>
    
    <content type="html"><![CDATA[<ul><li>事务</li><li>锁</li></ul><hr><h2 id="事务">事务</h2><ul><li>ACID<ul><li>原子性(Atomicity)&nbsp;：事务是一个原子操作单元,其对数据的修改,要么全都执行,要么全都不执行</li><li>一致性(Consistent)&nbsp;：在事务开始和完成时,数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改,以保持数据的完整性</li><li>隔离性(Isolation)&nbsp;：数据库系统提供一定的隔离机制,保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的,反之亦然</li><li>持久性(Durable)&nbsp;：事务完成之后,它对于数据的修改是永久性的,即使出现系统故障也能够保持</li></ul></li><li>并发事务处理带来的问题<ul><li>更新丢失(Lost&nbsp;Update)或脏写：最后的更新覆盖了由其他事务所做的更新</li><li>脏读（Dirty&nbsp;Reads）：事务A读取到了事务B已经修改但尚未提交的数据</li><li>不可重读（Non-Repeatable&nbsp;Reads）&nbsp;：事务A内部的相同查询语句在不同时刻读出的结果不一致，不符合隔离性</li><li>幻读（Phantom&nbsp;Reads）：事务A读取到了事务B提交的新增数据，不符合隔离性</li></ul></li><li>事务隔离级别：事务隔离越严格,并发副作用越小,但付出的代价也就越大,因为事务隔离实质上就是使事务在一定程度上“串行化”进行,这显然与“并发”是矛盾的<ul><li>不同的应用对读一致性和事务隔离程度的要求也是不同的</li><li>当前数据库的事务隔离级别:&nbsp;<code>show&nbsp;variables&nbsp;like&nbsp;'tx_isolation';</code></li><li>设置事务隔离级别：<code>set&nbsp;tx_isolation='REPEATABLE-READ';</code></li><li>Mysql默认的事务隔离级别是可重复读</li></ul></li><li>4 种事务隔离级别<ul><li>读未提交：<code>set&nbsp;tx_isolation='read-uncommitted';</code></li><li>读已提交：<code>set&nbsp;tx_isolation='read-committed';</code></li><li>可重复读：<code>set&nbsp;tx_isolation='repeatable-read';</code><ul><li>使用了MVCC(multi-version&nbsp;concurrency&nbsp;control)机制<ul><li>select操作不会更新版本号，是快照读（历史版本）</li><li>insert、update和delete会更新版本号，是当前读（当前版本）</li></ul></li><li>使用了间隙锁</li></ul></li><li>串行化：<code>set&nbsp;tx_isolation='serializable';</code><ul><li>这种隔离级别并发性极低，开发中很少会用到</li><li>读操作会获得共享锁，写操作会获得排他锁</li><li>使用了间隙锁</li></ul></li></ul></li></ul><p><img src="/static/IT/MySQL/MySQL-%E4%BA%8B%E5%8A%A1-1.png" alt=""></p><hr><h2 id="锁">锁</h2><ul><li>锁分类<ul><li>乐观锁：version</li><li>悲观锁：读锁（共享锁，S锁(Shared)）和写锁（排它锁，X锁(eXclusive)）<ul><li>读锁会阻塞写，但是不会阻塞读</li><li>写锁则会把读和写都阻塞</li></ul></li><li>从对数据操作的粒度分：表锁和行锁<ul><li>无索引行锁会升级为表锁：InnoDB的行锁是针对索引加的锁，不是针对记录加的锁。并且该索引不能失效，否则都会从行锁升级为表锁</li></ul></li></ul></li><li>表锁<ul><li><code>lock&nbsp;table&nbsp;表名称&nbsp;read(write)</code></li><li><code>show&nbsp;open&nbsp;tables;</code> 查看表上加过的锁</li><li><code>unlock&nbsp;tables;</code></li></ul></li><li>行锁：InnoDB支持事务（TRANSACTION）和行级锁<ul><li>MyISAM在执行查询语句SELECT前，会自动给涉及的所有表加读锁,在执行update、insert、delete操作会自 动给涉及的表加写锁</li><li>InnoDB在执行查询语句SELECT时(非串行隔离级别)，不会加锁。但是update、insert、delete操作会加行锁</li><li>锁定某一行还可以用 lock&nbsp;in&nbsp;share&nbsp;mode(共享锁)&nbsp;和 for&nbsp;update(排它锁)</li><li>通过检查InnoDB_row_lock状态变量来分析系统上的行锁的争夺情况<ul><li>Innodb_row_lock_time_avg&nbsp;（等待平均时长）</li><li>Innodb_row_lock_waits&nbsp;（等待总次数）</li><li>Innodb_row_lock_time（等待总时长）</li></ul></li></ul></li><li>间隙锁：就是两个值之间的空隙，在某些情况下可以解决幻读问题<ul><li>这个范围所包含的所有行记录（包括间隙行记录）以及行记录所在的间隙里</li><li>临键锁(Next-key&nbsp;Locks)：是行锁与间隙锁的组合</li></ul></li><li>查看INFORMATION_SCHEMA系统库锁相关数据表<ul><li><code>INFORMATION_SCHEMA.INNODB_TRX</code> 查看事务</li><li><code>INFORMATION_SCHEMA.INNODB_LOCKS</code> 查看锁</li><li><code>INFORMATION_SCHEMA.INNODB_LOCK_WAITS</code> 查看锁等待</li><li><code>kill&nbsp;trx_mysql_thread_id</code> 释放锁，<code>trx_mysql_thread_id</code> 可以从 <code>INNODB_TRX</code> 表里查看到</li><li><code>show&nbsp;engine&nbsp;innodb&nbsp;status</code> 查看锁等待详细信息</li></ul></li><li>死锁 <code>show&nbsp;engine&nbsp;innodb&nbsp;status</code><ul><li>大多数情况mysql可以自动检测死锁并回滚产生死锁的那个事务</li></ul></li><li>锁优化<ul><li>尽可能让所有数据检索都通过索引来完成，避免无索引行锁升级为表锁</li><li>合理设计索引，尽量缩小锁的范围</li><li>尽可能减少检索条件范围，避免间隙锁</li><li>尽量控制事务大小，减少锁定资源量和时间长度，涉及事务加锁的sql尽量放在事务最后执行</li><li>尽可能低级别事务隔离</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;事务&lt;/li&gt;
&lt;li&gt;锁&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;事务&quot;&gt;事务&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;ACID
&lt;ul&gt;
&lt;li&gt;原子性(Atomicity)&amp;nbsp;：事务是一个原子操作单元,其对数据的修改,要么全都执行,要么全都不执行&lt;/l</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-内核查询优化规则</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-nei-he-cha-xun-you-hua-gui-ze/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-nei-he-cha-xun-you-hua-gui-ze/</id>
    <published>2024-09-11T00:51:00.000Z</published>
    <updated>2024-09-11T00:51:39.096Z</updated>
    
    <content type="html"><![CDATA[<ul><li>条件化简<ul><li>移除不必要的括号</li><li>常量传递（constant_propagation）</li><li>移除没用的条件（trivial_condition_removal）</li><li>表达式计算</li><li>常量表检测<ul><li>使用主键等值匹配或者唯一二级索引列等值匹配作为搜索条件来查询某个表，通过这两种方式查询的表称之为常量表</li><li>优化器在分析一个查询语句时，先首先执行常量表查询，然后把查询中涉及到该表的条件全部替换成常数，最后再分析其余表的查询成本</li></ul></li><li>外连接消除<ul><li>有ON子句关联且空值拒绝的时候：外连接和内连接的查询结果没区别</li></ul></li></ul></li><li>子查询MySQL内部优化规则<ul><li>按返回的结果集区分子查询<ul><li>标量子查询</li><li>行子查询</li><li>列子查询</li><li>表子查询</li></ul></li><li>按与外层查询关系来区分子查询<ul><li>不相关子查询：子查询可以单独运行出结果，而不依赖于外层查询的值</li><li>相关子查询：子查询的执行需要依赖于外层查询的值</li><li><code>[NOT] IN/ANY/SOME/ALL</code> 子查询<ul><li>IN<ul><li>Memory存储引擎的临时表，而且会为该表建立哈希索引</li><li>物化表：结果集太多，可能内存中都放不下，同时建立了索引</li><li>物化表转连接<ul><li>将子查询转换为 semi-join 半连接：对于s1表的某条记录来说，我们只关心在s2表中是否存在与之匹配的记录，而不关心具体有多少条记录与之匹配，最终的结果集中只保留s1表的记录<ul><li>Table pullout （子查询中的表上拉）：子查询的查询列表处只有主键或者唯一索引列时，可以直接把子查询中的表上拉到外层查询的FROM子句中，并把子查询中的搜索条件合并到外层查询的搜索条件中</li><li>DuplicateWeedout execution strategy （重复值消除）、LooseScan execution strategy （松散扫描）、Semi-join Materializationa半连接物化、FirstMatch execution strategy （首次匹配）等等</li></ul></li><li>不能转为semi-join查询的子查询优化<ul><li>对于不相关子查询来说，会尝试把它们物化之后再参与查询</li><li>不管子查询是相关的还是不相关的，都可以把IN子查询尝试转为EXISTS子查询</li></ul></li></ul></li></ul></li><li>ANY/ALL：转为 select min/max</li></ul></li><li>EXISTS子查询<ul><li>不相关子查询，可以先执行子查询的结果是TRUE还是FALSE，并重写原先的查询语句</li></ul></li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;条件化简
&lt;ul&gt;
&lt;li&gt;移除不必要的括号&lt;/li&gt;
&lt;li&gt;常量传递（constant_propagation）&lt;/li&gt;
&lt;li&gt;移除没用的条件（trivial_condition_removal）&lt;/li&gt;
&lt;li&gt;表达式计算&lt;/li&gt;
&lt;li&gt;常量表检</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-高可用架构</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-gao-ke-yong-jia-gou/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-gao-ke-yong-jia-gou/</id>
    <published>2024-09-11T00:45:00.000Z</published>
    <updated>2024-09-11T00:48:38.689Z</updated>
    
    <content type="html"><![CDATA[<ul><li>基础集群<ul><li>MySQL主从同步原理</li><li>搭建主从集群<ul><li>配置master主服务</li><li>配置slave从服务</li></ul></li><li>部分同步</li><li>GTID同步集群</li><li>集群扩容与MySQL数据迁移</li></ul></li><li>复杂集群<ul><li>半同步复制</li><li>主从集群与读写分离</li><li>更复杂的集群结构</li></ul></li><li>高可用集群<ul><li>MMM</li><li>MHA</li><li>MGR</li></ul></li><li>分库分表<ul><li>分库分表的方式</li><li>分库分表要解决哪些问题</li><li>什么时候需要分库分表</li></ul></li></ul><hr><h2 id="基础集群">基础集群</h2><ul><li>MySQL主从同步原理（MySQL的Binlog默认是不打开的）<ul><li><img src="/static/IT/MySQL/MySQL-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84-1.png" alt=""><ul><li>在主库上打开Binlog日志，记录对数据的每一步操作</li><li>然后在从库上打开RelayLog日志，用来记录跟主库一样的Binlog日志</li><li>将RelayLog中的操作日志在自己数据库中进行异步的重演</li></ul></li></ul></li><li>搭建集群的两个必要条件<ul><li>MySQL版本必须一致</li><li>集群中各个服务器的时间需要同步</li></ul></li><li>搭建主从集群<ul><li>配置master主服务：<code>/etc/my.cnf</code><ul><li>打开binlog日志：<code>log_bin log_bin-index</code></li><li>指定severId：<code>server-id</code></li><li>重启服务：<code>service mysqld restart</code></li><li>给root用户分配一个replication slave的权限<ul><li><code>GRANT REPLICATION SLAVE ON *.* TO 'root'@'%';</code></li><li><code>flush privileges;</code></li><li><code>show master status;</code> 查看主节点同步状态<ul><li>这个指令结果中的File和Position记录的是当前日志的binlog文件以及文件中的索引</li><li>后面的Binlog_Do_DB和Binlog_Ignore_DB这两个字段是表示需要记录binlog文件的库以及不需要记录binlog文件的库<ul><li>没有进行配置，就表示是针对全库记录日志</li></ul></li><li>开启binlog后，数据库中所有操作都会被记录到datadir中，以一组轮询文件的方式循环记录<ul><li>指令查到的File和Position就是当前日志的文件和位置</li><li>后面配置从服务时，就需要通过这个File和Position通知从服务从哪个地方开始记录binLog</li></ul></li></ul></li><li>在实际生产环境中，通常不会直接使用root用户，而会创建一个拥有全部权限的用户来负责主从同步</li></ul></li></ul></li><li>配置slave从服务：<code>/etc/my.cnf</code><ul><li><code>server-id relay-log relay-log-index log-bin</code></li><li>启动mysqls的服务，并设置他的主节点同步状态<ul><li>设置同步主节点<ul><li><code>CHANGE MASTER TO</code><ul><li><code>MASTER_HOST='192.168.232.128',</code></li><li><code>MASTER_PORT=3306,</code></li><li><code>MASTER_USER='root',</code></li><li><code>MASTER_PASSWORD='root',</code></li><li><code>MASTER_LOG_FILE='master-bin.000004',</code></li><li><code>MASTER_LOG_POS=156,</code></li><li><code>GET_MASTER_PUBLIC_KEY=1;</code></li></ul></li><li>MASTER_LOG_FILE和MASTER_LOG_POS必须与主服务中查到的保持一致</li><li>后续如果要检查主从架构是否成功，也可以通过检查主服务与从服务之间的File和Position这两个属性是否一致来确定</li></ul></li><li><code>start slave;</code> 开启slave</li><li><code>show slave status \G;</code> 查看主从同步状态<ul><li>查看MASTER_LOG_FILE和READ_MASTER_LOG_POS与主节点保持一致，就表示这个主从同步搭建是成功的</li><li>Replicate_开头的属性，指定了两个服务之间要同步哪些数据库、哪些表的配置</li></ul></li></ul></li></ul></li><li><code>Slave_SQL_Running=no</code> 如果在slave从服务上查看slave状态，发现这个属性，就表示主从同步失败了<ul><li>有可能是因为在从数据库上进行了写操作，与同步过来的SQL操作冲突了</li><li>也有可能是slave从服务重启后有事务回滚了<ul><li>重启主从同步<ul><li><code>stop slave;</code></li><li><code>set GLOBAL SQL_SLAVE_SKIP_COUNTER=1;</code></li><li><code>start slave;</code></li></ul></li><li>另一种解决方式就是重新记录主节点的binlog文件消息（不太常用）<ul><li><code>stop slave;</code></li><li><code>change master to .....</code></li><li><code>start slave;</code></li><li>这种方式要注意binlog的文件和位置，如果修改后和之前的同步接不上，那就会丢失部分数据。所以不太常用</li></ul></li></ul></li></ul></li></ul></li><li>部分同步<ul><li>Master端：<code>/etc/my.cnf</code><ul><li><code>binlog-do-db</code> 需要同步的数据库名</li><li><code>binlog-ignore-db</code> 不备份的数据库</li></ul></li><li>Slave端：<code>/etc/my.cnf</code><ul><li><code>replicate-do-db</code> salve库名称与master库名相同<ul><li><code>replicate-rewrite-db = masterdemo -&gt; masterdemo01</code> 主从库名映射</li></ul></li><li><code>replicate-wild-do-table</code> 指定需要同步的表</li></ul></li></ul></li><li>GTID同步集群：也是基于Binlog来实现主从同步，只是他会基于一个全局的事务ID来标识同步进度<ul><li>GTID即全局事务ID，全局唯一并且趋势递增，他可以保证为每一个在主节点上提交的事务在复制集群中可以生成一个唯一的ID</li><li>流程<ul><li>首先从服务器会告诉主服务器已经在从服务器执行完了哪些事务的GTID值</li><li>然后主库会有把所有没有在从库上执行的事务，发送到从库上进行执行</li><li>并且使用GTID的复制可以保证同一个事务只在指定的从库上执行一次<ul><li>这样可以避免由于偏移量的问题造成数据不一致</li></ul></li></ul></li><li>Master端：<code>/etc/my.cnf</code><ul><li><code>gtid_mode=on</code></li><li><code>enforce_gtid_consistency=on</code></li><li><code>log_bin=on</code></li><li><code>server_id=单独设置一个</code></li><li><code>binlog_format=row</code></li></ul></li><li>Slave端：<code>/etc/my.cnf</code><ul><li><code>gtid_mode=on</code></li><li><code>enforce_gtid_consistency=on</code></li><li><code>log_slave_updates=1</code></li><li><code>server_id</code></li></ul></li><li>然后分别重启主服务和从服务，就可以开启GTID同步复制方式</li></ul></li><li>集群扩容与MySQL数据迁移<ul><li>扩展到一主多从的集群架构只需要增加一个binlog复制就行了</li><li>运行过程中扩展新的从节点（数据全量复制）：mysqldump 数据备份恢复操作<ul><li><code>mysqldump -u root -p --all-databases &gt; backup.sql</code> 导出</li><li><code>mysql -u root -p &lt; backup.sql</code> 导入</li><li>然后配置Slave从服务的数据同步</li></ul></li></ul></li></ul><hr><h2 id="复杂集群">复杂集群</h2><ul><li>异步复制：MySQL主从集群默认采用的是一种异步复制的机制。主服务在执行用户提交的事务后，写入binlog日志，然后就给客户端返回一个成功的响应了。而binlog会由一个dump线程异步发送给Slave从服务<ul><li><img src="/static/IT/MySQL/MySQL-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84-2.png" alt=""><ul><li>由于这个发送binlog的过程是异步的。主服务在向客户端反馈执行结果时，是不知道binlog是否同步成功了的</li><li>这时候如果主服务宕机了，而从服务还没有备份到新执行的binlog，那就有可能会丢数据</li></ul></li></ul></li><li>半同步复制：介于异步复制和全同步复制之前的机制<ul><li><img src="/static/IT/MySQL/MySQL-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84-3.png" alt=""><ul><li>主库在执行完客户端提交的事务后，并不是立即返回客户端响应，而是等待至少一个从库接收并写到relay log中，才会返回给客户端</li><li>MySQL在等待确认时，默认会等10秒，如果超过10秒没有收到ack，就会降级成为异步复制</li></ul></li><li>这种半同步复制相比异步复制，能够有效的提高数据的安全性<ul><li>但是这种安全性也不是绝对的，他只保证事务提交后的binlog至少传输到了一个从库，并且并不保证从库应用这个事务的binlog是成功的</li></ul></li><li>半同步复制机制也会造成一定程度的延迟，这个延迟时间最少是一个TCP/IP请求往返的时间<ul><li>当从服务出现问题时，主服务需要等待的时间就会更长，要等到从服务的服务恢复或者请求超时才能给用户响应</li></ul></li></ul></li><li>搭建半同步复制集群：基于特定的扩展模块（semisync_master.so  semisync_slave.so）<ul><li>在主服务上安装 semisync_master 模块<ul><li><code>install plugin rpl_semi_sync_master soname 'semisync_master.so';</code></li><li><code>set global rpl_semi_sync_master_enabled=ON;</code> 打开半同步复制</li><li><code>show global variables like 'rpl_semi%';</code><ul><li>rpl_semi_sync_master_timeout就是半同步复制时等待应答的最长等待时间，默认是10秒</li><li>半同步复制有两种方式，默认AFTER_SYNC方式<ul><li>AFTER_SYNC：主库把日志写入binlog，并且复制给从库，然后开始等待从库的响应。从库返回成功后，主库再提交事务，接着给客户端返回一个成功响应</li><li>AFTER_COMMIT：在主库写入binlog后，等待binlog复制到从库，主库就提交自己的本地事务，再等待从库返回给自己一个成功响应，然后主库再给客户端返回响应<ul><li><code>rpl_semi_sync_master_wait_point</code></li></ul></li></ul></li></ul></li></ul></li><li>在从服务上安装 semisync_slave 模块<ul><li>`install plugin rpl_semi_sync_slave soname ‘semisync_slave.so’;</li><li>`set global rpl_semi_sync_slave_enabled = on;``</li><li><code>show global variables like 'rpl_semi%';</code></li><li>安装完slave端的半同步插件后，需要重启下slave服务<ul><li><code>stop slave;</code></li><li><code>start slave;</code></li></ul></li></ul></li></ul></li><li>主从集群与读写分离：MySQL主从集群是单向的，也就是只能从主服务同步到从服务，而从服务的数据表更是无法同步到主服务的<ul><li>为了保证数据一致，通常会需要保证数据只在主服务上写，而从服务只进行数据读取（读写分离）<ul><li>MySQL主从本身是无法提供读写分离的服务的，需要由业务自己来实现</li><li>在MySQL主从架构中，是需要严格限制从服务的数据写入的，一旦从服务有数据写入，就会造成数据不一致</li><li>并且从服务在执行事务期间还很容易造成数据同步失败</li></ul></li><li>可以在从服务中将read_only参数的值设为1( <code>set global read_only=1;</code> )<ul><li><code>read_only=1</code>设置的只读模式，不会影响slave同步复制的功能</li><li><code>read_only=1</code>设置的只读模式， 限定的是普通用户进行数据修改的操作，但不会限定具有super权限的用户的数据修改操作</li></ul></li><li>限定super权限的用户写数据，可以设置<code>super_read_only=0</code><ul><li>如果要想连super权限用户的写操作也禁止，就使用<code>flush tables with read lock;</code><ul><li>这样设置也会阻止主从同步复制！</li></ul></li></ul></li></ul></li><li>扩展更复杂的集群结构<ul><li>为了进一步提高整个集群的读能力，可以扩展出一主多从<ul><li>为了减轻主节点进行数据同步的压力，可以继续扩展出多级从的主从集群</li></ul></li><li>为了提高这个集群的写能力，可以搭建互主集群（两个服务互为主从）<ul><li>在主服务上打开一个slave进程，并且指向slave节点的binlog当前文件地址和位置</li></ul></li><li>可以扩展出多主多从的集群，全方位提升集群的数据读写能力<ul><li>也可以扩展出环形的主从集群，实现MySQL多活部署</li></ul></li></ul></li><li>主从复制延迟（读写分离后更容易体现出来），主库刚插入了数据但是从库查不到<ul><li>面向业务的主服务数据都是多线程并发写入的，而从服务是单个线程慢慢拉取binlog</li><li>并行复制：<ul><li>在从服务上设置<code>slave_parallel_workers</code>为一个大于0的数</li><li>然后把<code>slave_parallel_type</code>参数设置为<code>LOGICAL_CLOCK</code></li></ul></li></ul></li></ul><hr><h2 id="高可用集群">高可用集群</h2><ul><li>如果是MySQL主服务挂了，从服务是没办法自动切换成主服务的，如果要实现MySQL的高可用，需要借助一些第三方工具实现：MMM、MHA、MGR<ul><li>对主从复制集群中的Master节点进行监控</li><li>自动的对Master进行迁移，通过VIP</li><li>重新配置集群中的其它slave对新的Master进行同步</li></ul></li><li>MMM (Master-Master replication managerfor Mysql)：Mysql主主复制管理器，可以对mysql集群进行监控和故障迁移<ul><li><img src="/static/IT/MySQL/MySQL-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84-4.png" alt=""></li><li>需要两个Master，同一时间只有一个Master对外提供服务，可以说是主备模式</li><li>通过一个VIP (虚拟IP) 的机制来保证集群的高可用<ul><li>在主节点上会通过一个VIP地址来提供数据读写服务</li><li>当出现故障时，VIP就会从原来的主节点漂移到其他节点，由其他节点提供服务</li></ul></li><li>优点<ul><li>提供了读写VIP的配置，使读写请求都可以达到高可用</li><li>工具包相对比较完善，不需要额外的开发脚本</li><li>完成故障转移之后可以对MySQL集群进行高可用监控</li></ul></li><li>缺点<ul><li>故障简单粗暴，容易丢失事务，建议采用半同步复制方式，减少失败的概率</li><li>目前MMM社区已经缺少维护</li><li>不支持基于GTID的复制</li></ul></li><li>适用场景<ul><li>读写都需要高可用的</li><li>基于日志点的复制方式</li></ul></li></ul></li><li>MHA (Master High Availability Manager and Tools for MySQL)：专门用于监控主库的状态，当发现master节点故障时，会提升其中拥有新数据的slave节点成为新的master节点，在此期间，MHA会通过其他从节点获取额外的信息来避免数据一致性方面的问题<ul><li>MHA还提供了mater节点的在线切换功能，即按需切换master-slave节点</li><li>MHA能够在30秒内实现故障切换，并能在故障切换过程中，最大程度的保证数据一致性</li><li><img src="/static/IT/MySQL/MySQL-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84-5.png" alt=""><ul><li>MHA是需要单独部署的，分为Manager节点和Node节点，两种节点<ul><li>Manager节点一般是单独部署的一台机器</li><li>Node节点一般是部署在每台MySQL机器上的</li><li>Node节点得通过解析各个MySQL的日志来进行一些操作</li></ul></li><li>Manager节点会通过探测集群里的Node节点去判断各个Node所在机器上的MySQL运行是否正常<ul><li>如果发现某个Master故障了，就直接把他的一个Slave提升为Master</li><li>然后让其他Slave都挂到新的Master上去，完全透明</li></ul></li></ul></li><li>优点<ul><li>MHA除了支持日志点的复制还支持GTID的方式</li><li>同MMM相比，MHA会尝试从旧的Master中恢复旧的二进制日志，只是未必每次都能成功。如果希望更少的数据丢失场景，建议使用MHA架构</li></ul></li><li>缺点<ul><li>MHA需要自行开发VIP转移脚本</li><li>MHA只监控Master的状态，未监控Slave的状态</li></ul></li></ul></li><li>MGR (MySQL Group Replication)：官方组复制机制，解决传统异步复制和半同步复制的数据一致性问题<ul><li>由若干个节点共同组成一个复制组，一个事务提交后，必须经过超过半数节点的决议并通过后，才可以提交<ul><li>MGR依靠分布式一致性协议(Paxos协议的一个变体)，实现了分布式下数据的最终一致性，提供了真正的数据高可用方案</li></ul></li><li><img src="/static/IT/MySQL/MySQL-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84-6.png" alt=""></li><li>优点<ul><li>高一致性<ul><li>基于原生复制及paxos协议的组复制技术，并以插件的方式提供，提供一致数据安全保证</li></ul></li><li>高容错性：只要不是大多数节点坏掉就可以继续工作<ul><li>有自动检测机制，当不同节点产生资源争用冲突时，按照先到者优先原则进行处理</li><li>并且内置了自动化脑裂防护机制</li></ul></li><li>高扩展性：节点的新增和移除都是自动的<ul><li>新节点加入后，会自动从其他节点上同步状态，直到新节点和其他节点保持一致</li><li>如果某节点被移除了，其他节点自动更新组信息，自动维护新的组信息</li></ul></li><li>高灵活性：有单主模式和多主模式（但官方推荐单主模式）<ul><li>单主模式下，会自动选主，所有更新操作都在主上进行<ul><li>MGR集群会选出primary节点负责写请求</li><li>primary节点与其它节点都可以进行读请求处理</li></ul></li><li>多主模式下，所有server都可以同时处理更新操作<ul><li>客户端可以随机向MySQL节点写入数据</li></ul></li></ul></li></ul></li><li>缺点<ul><li>仅支持InnoDB引擎，并且每张表一定要有一个主键，用于做write set的冲突检测</li><li>必须打开GTID特性，二进制日志格式必须设置为ROW，用于选主与write set；<ul><li>主从状态信息存于表中<ul><li><code>--master-info-repository=TABLE</code></li><li><code>--relay-log-info-repository=TABLE</code></li></ul></li><li><code>--log-slave-updates</code> 打开</li></ul></li><li>COMMIT可能会导致失败，类似于快照事务隔离级别的失败场景</li><li>目前一个MGR集群最多支持9个节点</li><li>不支持外键于save point特性，无法做全局间的约束检测与部分事务回滚</li></ul></li><li>适用场景<ul><li>对主从延迟比较敏感</li><li>希望对写服务提供高可用，又不想安装第三方软件</li><li>数据强一致的场景</li></ul></li></ul></li><li>基于云原生的思路，完全可以让MySQL服务与数据物理分离，MySQL无状态自然就可以高可用</li></ul><hr><h2 id="分库分表">分库分表</h2><ul><li>垂直分片： 按照业务来对数据进行分片，又称为纵向分片（专库专用）<ul><li>往往需要对架构和设计进行调整<ul><li>通常来讲，是来不及应对业务需求快速变化的</li></ul></li><li>无法真正的解决单点数据库的性能瓶颈<ul><li>可以缓解数据量和访问量带来的问题，但无法根治</li></ul></li></ul></li><li>水平分片：又称横向分片<ul><li>常用的分片策略有<ul><li>取余\取模：均匀存放数据；扩容非常麻烦</li><li>按照范围分片：比较好扩容；数据分布不够均匀</li><li>按照时间分片： 比较容易将热点数据区分出来</li><li>按照枚举值分片： 例如按地区分片</li><li>按照目标字段前缀指定进行分区：自定义业务规则分片</li><li>不需要数据迁移的取模分片扩容方案<ul><li>共享内存中维护一个分片表<ul><li>扩容后的数据路由直接查表</li><li>旧数据可以查表中之前的记录版本</li></ul></li><li>数据均衡：动态加入的节点尽量多写数据<ul><li>比如5个节点新增2个一共7个节点，可以按9取模，多的两个分配给新节点</li></ul></li></ul></li></ul></li><li>水平分片从理论上突破了单机数据量处理的瓶颈，并且扩展相对自由，是分库分表的标准解决方案</li></ul></li><li>一般来说，在系统设计阶段就应该根据业务耦合松紧来确定垂直分库<ul><li>在数据量及访问压力不是特别大的情况，首先考虑缓存、读写分离、索引技术等方案</li><li>若数据量极大，且持续增长，再考虑水平分库水平分表方案</li></ul></li><li>分库分表要解决哪些问题<ul><li>事务一致性问题</li><li>跨节点关联查询问题</li><li>跨节点分页、排序函数</li><li>主键避重问题</li><li>公共表处理<ul><li>参数表、数据字典表等都是数据量较小，变动少，而且属于高频联合查询的依赖表</li><li>这一类表一般就需要在每个数据库中都保存一份，并且所有对公共表的操作都要分发到所有的分库去执行</li></ul></li><li>运维工作量</li></ul></li><li>什么时候需要分库分表<ul><li>MySQL单表记录如果达到500W这个级别</li><li>单表容量达到2GB</li><li>一般对于用户数据这一类后期增长比较缓慢的数据，一般可以按照三年左右的业务量来预估使用人数，按照标准预设好分库分表的方案</li><li>对于业务数据这一类增长快速且稳定的数据，一般则需要按照预估量的两倍左右预设分库分表方案</li><li>由于分库分表的后期扩容是非常麻烦的，所以在进行分库分表时，尽量根据情况，多分一些表<ul><li>最好是计算一下数据增量，永远不用增加更多的表</li></ul></li><li>在设计分库分表方案时，要尽量兼顾业务场景和数据分布<ul><li>在支持业务场景的前提下，尽量保证数据能够分得更均匀</li></ul></li><li>尽量在分库分表的同时，再补充设计一个降级方案<ul><li>将数据转存一份到ES，ES可以实现更灵活的大数据聚合查询</li></ul></li></ul></li><li>常见的分库分表组件<ul><li>shardingsphere：Sharding-JDBC、Sharding-Proxy、Sharding-Sidecar</li><li>mycat</li><li>DBLE</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;基础集群
&lt;ul&gt;
&lt;li&gt;MySQL主从同步原理&lt;/li&gt;
&lt;li&gt;搭建主从集群
&lt;ul&gt;
&lt;li&gt;配置master主服务&lt;/li&gt;
&lt;li&gt;配置slave从服务&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;部分同步&lt;/li&gt;
&lt;li&gt;GTID同步集群&lt;/li&gt;</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-成本计算</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-cheng-ben-ji-suan/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-cheng-ben-ji-suan/</id>
    <published>2024-09-11T00:43:00.000Z</published>
    <updated>2024-09-11T00:43:55.362Z</updated>
    
    <content type="html"><![CDATA[<ul><li>一条查询语句的执行成本由下边这两个方面组成<ul><li>I/O成本：把数据或者索引加载到内存</li><li>CPU成本：读取以及检测记录是否满足对应的搜索条件、对结果集进行排序等操作</li></ul></li><li>基于索引统计数据的成本<ul><li>IN语句中的参数个数: <code>eq_range_index_dive_limit</code></li><li><code>show index from table</code> 为表中的每一个索引维护一份统计数据</li></ul></li><li>单表查询的成本<ol><li>根据搜索条件，找出所有可能使用的索引</li><li>计算全表扫描的代价</li><li>计算使用不同索引执行查询的代价<ol><li>范围区间数量</li><li>需要回表的记录数</li><li>是否有可能使用索引合并</li></ol></li><li>对比各种方案，找出成本最低的那一个</li></ol></li><li>连接查询的成本<ul><li>单次查询驱动表的成本</li><li>多次查询被驱动表的成本<ul><li>具体查询多少次取决于对驱动表查询的结果集中有多少条</li><li>对驱动表进行查询后得到的记录条数称之为驱动表的扇出（英文名：fanout）</li></ul></li><li>Condition&nbsp;filtering 就是这个猜的过程<ul><li>如果使用的是全表扫描的方式执行的单表查询，那么计算驱动表扇出时需要猜满足搜索条件的记录到底有多少条</li><li>如果使用的是索引执行的单表扫描，那么计算驱动表扇出的时候需要猜满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条</li></ul></li><li>两表连接的成本分析</li><li>多表连接的成本分析<ul><li>optimizer_search_depth 如果连接表的个数小于该值，那么就继续穷举分析每一种连接顺序的成本，否则只对与 optimizer_search_depth 值相同数量的表进行穷举分析</li><li>optimizer_prune_level 启发式规则</li></ul></li></ul></li><li>调节成本常数 server_cost engine_cost</li><li>InnoDB中的统计数据<ul><li>统计数据存储方式 innodb_stats_persistent<ul><li>永久性的统计数据，这种统计数据存储在磁盘上<ul><li>innodb_table_stats 存储了关于表的统计数据，每条记录对应一个表的统计数据</li><li>innodb_index_stats 存储了关于索引的统计数据，每条记录对应一个索引的一个统计项</li></ul></li><li>非永久性的统计数据，这种统计数据存储在内存中</li></ul></li><li>更新统计数据<ul><li>innodb_stats_auto_recalc 自动重新计算统计数据，该功能默认是开启的</li><li>手动调用ANALYZE TABLE语句来更新统计信息，会立即重新计算统计数据</li><li>手动更新innodb_table_stats和innodb_index_stats表<ul><li><code>FLUSH TABLE table</code> 让MySQL查询优化器重新加载我们更改过的数据</li></ul></li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;一条查询语句的执行成本由下边这两个方面组成
&lt;ul&gt;
&lt;li&gt;I/O成本：把数据或者索引加载到内存&lt;/li&gt;
&lt;li&gt;CPU成本：读取以及检测记录是否满足对应的搜索条件、对结果集进行排序等操作&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;基于索引统计数据的成本
&lt;</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>MySQL-8新特性</title>
    <link href="https://jxch.github.io/2024/09/11/architect/mysql/mysql-8-xin-te-xing/"/>
    <id>https://jxch.github.io/2024/09/11/architect/mysql/mysql-8-xin-te-xing/</id>
    <published>2024-09-11T00:41:00.000Z</published>
    <updated>2024-09-11T00:42:38.179Z</updated>
    
    <content type="html"><![CDATA[<ul><li>账户与安全<ul><li>用户的创建与授权</li><li>密码管理</li><li>认证插件</li><li>角色管理</li></ul></li><li>索引<ul><li>隐藏索引：被隐藏的索引不会被优化器使用，但依然真实存在</li></ul></li><li>系统方面<ul><li>原子 ddl 操作：保证了 <code>drop table t1,t2</code> 时两个表的原子性</li><li>自增列持久化：将自增计数器的最大值写入 redo log，同时在每次检查点将其写入引擎私有的系统表</li><li>重构 SQL 分析器</li><li>新的系统字典表：所有的元数据都用 InnoDB 引擎进行存储</li><li>支持资源管理：支持创建和管理资源组，并允许将服务器运行的线程分配给特定的组</li></ul></li><li>更好支持文档型数据库和JSON<ul><li>聚合函数<ul><li>JSON_ARRAYAGG()，将多行数据组合成 json 数组</li><li>JSON_OBJECTAGG()，用于生成 json 对象</li></ul></li><li>JSON 实用函数<ul><li>JSON_PRETTY() 输出 json 数据的时候，格式化</li><li>JSON_STORAGE_SIZE() json 数据所占用的存储空间(单位：字节)</li><li>JSON_STORAGE_FREE() json 数据更新后所释放的空间(单位：字节)</li></ul></li><li>JSON 合并函数：废弃了 JSON_MERGE() 函数<ul><li>JSON_MERGE_PATCH()</li><li>JSON_MERGE_PRESERV()</li></ul></li><li>JSON 表函数： JSON_TABLE() 将 JSON 数据转换成关系表，可以将该函数的返回结果当做一个普通的临时表进行 sql 查询</li><li>支持 RANK(),、LAG()、NTILE() 等函数<ul><li>rank() 按照某字段的排序结果添加排名，但它是跳跃的、间断的排名<ul><li>例如两个并列第一名后，下一个是第三名，1、1、3、4</li></ul></li></ul></li></ul></li><li>正则表达式增强：REGEXP_LIKE()、EGEXP_INSTR()、REGEXP_REPLACE()、REGEXP_SUBSTR()</li><li>新增备份锁：允许在线备份期间的 DML，同时防止可能导致快照不一致的操作<ul><li>备份锁由 LOCK INSTANCE FOR BACKUP 和 UNLOCK INSTANCE 语法支持</li></ul></li><li>默认字符集 ：由 latin1 变为 utf8mb4</li><li>配置参数<ul><li>全局参数持久化：支持在线修改全局参数持久化，通过加上 PERSIST 关键字，可以将调整持久化到新的配置文件中，再次重启 db 还可以应用到最新的参数</li><li>支持会话级别动态调整部分参数：有利于提升语句性能<ul><li><code>select /*+ SET_VAR(sort_buffer_size = 16M) */ id from test order id ;</code></li><li><code>insert /*+ SET_VAR(foreign_key_checks=OFF) */ into test(name) values(1);</code></li></ul></li><li>默认参数的调整<ul><li>调整 back_log 的默认值，保持和 max_connections 一致，增强突发流量带来的连接处理容量</li><li>修改 event_scheduler 默认为ON，之前默认是关闭的</li><li>调整 max_allowed_packet 的默认值，从 4M 增加到 64M</li><li>调整 bin_log, log_slave_updates 默认值为 on</li><li>调整 expire_logs_days 的过期时间为 30 天，老版本是 7 天，生产环境时，检查该参数，防止 binlog 过多造成空间紧张</li><li>调整 innodb_undo_log_truncate 默认为 ON</li><li>调整 innodb_undo_tablespaces 默认值为 2</li><li>调整 innodb_max_dirty_pages_pct_lwm 默认值10</li><li>调整 innodb_max_dirty_pages_pct 默认值为90</li><li>新增 innodb_autoinc_lock_mode 默认值为 2</li></ul></li></ul></li><li>InnoDB<ul><li>innodb 增强<ul><li>新增 INFORMATION_SCHEMA.INNODB_CACHED_INDEXES，查看每个索引缓存在InnoDB 缓冲池中的索引页数</li><li>InnoDB 临时表都将在共享临时表空间 ibtmp1 中创建</li><li>对于SELECT … FOR SHARE 和SELECT … FOR UPDATE 语句，InnoDB 支持NOWAIT和SKIP LOCKED</li><li>innodb_undo_tablespaces 的最小值为 2，并且不再允许将设置为 0，最小值 2 确保回滚段始终在撤消表空间中创建，而不是在系统表空间中创建</li><li>支持 ALTER TABLESPACE … RENAME TO 语法</li><li>新增 innodb_dedicated_server，让 InnoDB 根据服务器上检测到的内存量自动配置innodb_buffer_pool_size，innodb_log_file_size，innodb_flush_method</li><li>新增 INFORMATION_SCHEMA.INNODB_TABLESPACES_BRIEF 视图</li><li>新增了动态配置项 innodb_deadlock_detect，用来禁用死锁检查，因为在高并发系统中，当大量线程等待同一个锁时，死锁检查会大大拖慢数据库</li><li>支持使用 innodb_directories 选项在服务器脱机时将表空间文件移动或恢复到新位置</li></ul></li><li>InnoDB 性能提升<ul><li>并发<ul><li>废除 buffer pool mutex，将原来一个 mutex 拆分成多个，提高并发</li><li>拆分 LOCK_thd_list 和 LOCK_thd_remove 这两个 mutex，大约可提高线程链接效率5%</li></ul></li><li>行缓存<ul><li>优化器可以估算将要读取的行数，因此可以提供给存储引擎一个合适大小的row buffer 来存储需要的数据。大批量的连续数据扫描的性能将受益于更大的 record buffer</li></ul></li><li>改进扫描性能：改进 InnoDB 范围查询的性能，可提升全表查询和范围查询 5-20%的性能</li><li>改进成本模型<ul><li>可以估算缓存区中的有多少表和索引，这可以让优化器选择访问方式时知道数据是否可以存储在内存中还是必须存储到磁盘上</li></ul></li></ul></li></ul></li><li>废弃特性<ul><li>移除了一些功能，例如 query cache</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;账户与安全
&lt;ul&gt;
&lt;li&gt;用户的创建与授权&lt;/li&gt;
&lt;li&gt;密码管理&lt;/li&gt;
&lt;li&gt;认证插件&lt;/li&gt;
&lt;li&gt;角色管理&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;索引
&lt;ul&gt;
&lt;li&gt;隐藏索引：被隐藏的索引不会被优化器使用，但依然真实存在&lt;/li&gt;</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="MySQL" scheme="https://jxch.github.io/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>Redis-7.0新特性</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-7.0-xin-te-xing/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-7.0-xin-te-xing/</id>
    <published>2024-09-10T08:04:00.000Z</published>
    <updated>2024-09-10T08:06:04.008Z</updated>
    
    <content type="html"><![CDATA[<ul><li>共享复制缓存区<ul><li>老版本多从库时主库内存占用过多（每个从库都有一个从库复制缓冲区）<ul><li><img src="/static/IT/Redis/Redis-7.0%E6%96%B0%E7%89%B9%E6%80%A7-1.png" alt=""></li></ul></li><li>Redis 为了提升多从库全量复制的效率和减少 fork 产生RDB 的次数，会尽可能的让多个从库共用一个 RDB<ul><li>将 ReplicationBuffer 数据切割为多个 16KB 的数据块 (replBufBlock)，然后使用链表来维护起来</li><li><img src="/static/IT/Redis/Redis-7.0%E6%96%B0%E7%89%B9%E6%80%A7-2.png" alt=""></li><li>ReplicationBuffer 由多个 replBufBlock 组成链表，当复制积压区或从库对某个block 使用时，便对正在使用的 replBufBlock 增加引用计数</li><li>当从库使用完当前的 replBufBlock（已经将数据发送给从库）时，就会对其 refcount 减 1 而且移动到下一个 replBufBlock，并对其refcount 加1</li></ul></li></ul></li><li>ReplicationBuffer 的裁剪和释放<ul><li>ReplicationBuffer 不可能无限增长，Redis 有相应的逻辑对其进行裁剪，简单来说，Redis 会从头访问 replBufBlock 链表，如果发现 replBufBlock refcount为0，则会释放它，直到迭代到第一个 replBufBlock refcount 不为0 才停止<ul><li>当从库使用完当前的 replBufBlock 会对其refcount 减1</li><li>当从库断开链接时会对正在引用的 replBufBlock refcount 减1，无论是因为超过client-output-buffer-limit 导致的断开还是网络原因导致的断开</li><li>当 ReplicationBacklog 引用的replBufBlock 数据量超过设置的该值大小时，会对正在引用的 replBufBlock refcount 减1，以尝试释放内存</li></ul></li><li>当一个从库引用的 replBufBlock 过多，它断开时释放的 replBufBlock 可能很多，也可能造成堵塞问题，所以Redis7 里会限制一次释放的个数，未及时释放的内存在系统的定时任务中渐进式释放</li></ul></li><li>使用 Rax 树实现对 replBufBlock 固定区间间隔的索引，每 64 个记录一个索引点<ul><li>Rax 索引占用的内存较少；查询效率也是非常高</li><li>streams 里面的 consumer group (消费者组) 的名称还有和 Redis 集群名称的存储也是使用的 Rax 树</li></ul></li><li>Trie 字典树（前缀树）<ul><li><img src="/static/IT/Redis/Redis-7.0%E6%96%B0%E7%89%B9%E6%80%A7-3.png" alt=""></li></ul></li><li>Rax 基数树（前缀压缩树）：压缩后的 Trie 树<ul><li><img src="/static/IT/Redis/Redis-7.0%E6%96%B0%E7%89%B9%E6%80%A7-4.png" alt=""></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;共享复制缓存区
&lt;ul&gt;
&lt;li&gt;老版本多从库时主库内存占用过多（每个从库都有一个从库复制缓冲区）
&lt;ul&gt;
&lt;li&gt;&lt;img src=&quot;/static/IT/Redis/Redis-7.0%E6%96%B0%E7%89%B9%E6%80%A7-1.png&quot; a</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-6.0新特性</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-6.0-xin-te-xing/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-6.0-xin-te-xing/</id>
    <published>2024-09-10T08:02:00.000Z</published>
    <updated>2024-09-10T08:04:01.427Z</updated>
    
    <content type="html"><![CDATA[<ul><li>多线程</li><li>Client&nbsp;Side&nbsp;Cache</li><li>Acls</li></ul><hr><h2 id="多线程">多线程</h2><ul><li>redis&nbsp;6.0&nbsp;提供了多线程的读写IO,&nbsp;但是最终执行用户命令的线程依然是单线程的<ul><li><code>io‐threads&nbsp;4</code> 有三个IO线程，还有一个线程是main线程，，main线程负责IO读写和命令执行操作<ul><li>这三个IO线程只会执行&nbsp;IO中的write操作，也就是说， read和命令执行都由main线程执行。最后多线程将数据写回到客户端</li><li><img src="/static/IT/Redis/Redis-6.0%E6%96%B0%E7%89%B9%E6%80%A7-1.png" alt=""></li></ul></li><li><code>io‐threads‐do‐reads&nbsp;yes</code> 将支持IO线程执行读写任务<ul><li><img src="/static/IT/Redis/Redis-6.0%E6%96%B0%E7%89%B9%E6%80%A7-2.png" alt=""></li></ul></li></ul></li></ul><hr><h2 id="Client-Side-Cache">Client&nbsp;Side&nbsp;Cache</h2><ul><li>客户端缓存：redis&nbsp;6&nbsp;提供了服务端追踪key的变化，客户端缓存数据的特性<ul><li><img src="/static/IT/Redis/Redis-6.0%E6%96%B0%E7%89%B9%E6%80%A7-3.png" alt=""></li></ul></li><li>执行流程<ul><li>当客户端访问某个key时，服务端将记录key&nbsp;和&nbsp;client</li><li>客户端拿到数据后，进行客户端缓存</li><li>这时，当key再次被访问时，key将被直接返回，避免了与redis服务器的再次交互</li><li>当数据被其他请求修改时，服务端将主动通知客户端失效的key</li><li>客户端进行本地失效</li><li>下次请求时，重新获取最新数据</li></ul></li><li>目前只有lettuce对其进行了支持</li></ul><hr><h2 id="ACL">ACL</h2><ul><li>ACL&nbsp;是对于命令的访问和执行权限的控制</li><li>ACL 设置有两种方式<ul><li>命令方式：ACL&nbsp;SETUSER&nbsp;+&nbsp;具体的权限规则，&nbsp;通过&nbsp;ACL&nbsp;SAVE&nbsp;进行持久化</li><li>对&nbsp;ACL&nbsp;配置文件进行编写，并且执行&nbsp;ACL&nbsp;LOAD&nbsp;进行加载</li></ul></li><li>ACL存储有两种方式，但是两种方式不能同时配置，否则直接报错退出进程<ul><li>redis&nbsp;配置文件：&nbsp;redis.conf</li><li>ACL 配置文件，在redis.conf&nbsp;中通过&nbsp;aclfile&nbsp;&nbsp;/path&nbsp;&nbsp;配置acl文件的路径</li></ul></li><li><code>ACL&nbsp;SETUSER&nbsp;alice</code> 创建一个用户名为&nbsp;alice的用户<ul><li>用户alice&nbsp;没有任何意义</li><li>处于&nbsp;off&nbsp;状态，&nbsp;它是被禁用的，不能用auth进行认证</li><li>不能访问任何命令</li><li>不能访问任意的key</li><li>没有密码</li></ul></li><li><code>acl&nbsp;setuser&nbsp;alice&nbsp;on&nbsp;&gt;pass123&nbsp;~cached:*&nbsp;+get</code> 创建一个对&nbsp;cached:&nbsp;前缀具有get命令执行权限的用户，并且设置密码</li><li>切换其他用户进行登录<ul><li><code>ACL&nbsp;GETUSER&nbsp;alice</code></li><li><code>ACL&nbsp;SETUSER&nbsp;alice&nbsp;~objects:*&nbsp;~items:*&nbsp;~public:*</code> 添加多个访问模式，空格分隔</li></ul></li><li><code>ACL&nbsp;SETUSER&nbsp;alice&nbsp;on&nbsp;+@all&nbsp;‐@dangerous&nbsp;&gt;密码&nbsp;~*</code> 针对类型命令的约束<ul><li>+@all:&nbsp;&nbsp;包含所有得命令</li><li>然后用 -@&nbsp;去除在 redis&nbsp;command&nbsp;table&nbsp;中定义的&nbsp;dangerous&nbsp;命令</li></ul></li><li>查看具体有哪些命令属于某个类别<ul><li><code>acl&nbsp;cat</code></li><li><code>acl&nbsp;cat&nbsp;dangerous</code></li></ul></li><li>开放子命令<ul><li><code>ACL&nbsp;SETUSER&nbsp;myuser&nbsp;‐client&nbsp;+client|setname&nbsp;+client|getname</code><ul><li>禁用client&nbsp;命令，但是开放&nbsp;client&nbsp;命令中的子命令&nbsp;&nbsp;setname&nbsp;和&nbsp;getname<ul><li>只能是先禁用，后追加子命令，因为后续可能会有新的命令增加</li></ul></li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;多线程&lt;/li&gt;
&lt;li&gt;Client&amp;nbsp;Side&amp;nbsp;Cache&lt;/li&gt;
&lt;li&gt;Acls&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;多线程&quot;&gt;多线程&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;redis&amp;nbsp;6.0&amp;nbsp;提供了多线程的读写</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-分布式锁</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-fen-bu-shi-suo/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-fen-bu-shi-suo/</id>
    <published>2024-09-10T08:01:00.000Z</published>
    <updated>2024-09-10T08:01:56.494Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Redisson -&gt; LUA<ul><li>自旋信号量拿锁<ul><li>解锁后信号量唤醒尝试再加锁（Redis订阅发布通知唤醒）</li></ul></li><li>锁续命<ul><li>避免线程执行时锁失效，导致该线程释放其它线程的锁</li></ul></li><li>锁重入</li><li>读写锁</li><li>锁失效：无解，Redis的可用性高于可靠性</li><li>原子加锁&amp;原子解锁：避免加解锁中途服务故障</li></ul></li><li>RedLock（超半数加锁）锁失效:<ul><li>主从锁失效<ul><li>复制延迟</li><li>主从切换</li></ul></li><li>1s持久化数据丢失</li><li>网络分区</li><li>时钟漂移</li><li>宕机（无法在足够多的实例上获取锁）</li></ul></li><li>锁粒度：分段锁（将一个key拆分成多个，供不同的服务器读写）</li><li>Redisson 分布式锁<ul><li><img src="/static/IT/Redis/Redis-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81-1.png" alt="Redisson 分布式锁"></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Redisson -&amp;gt; LUA
&lt;ul&gt;
&lt;li&gt;自旋信号量拿锁
&lt;ul&gt;
&lt;li&gt;解锁后信号量唤醒尝试再加锁（Redis订阅发布通知唤醒）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;锁续命
&lt;ul&gt;
&lt;li&gt;避免线程执行时锁失效，导致该线程释放其它线程的</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-Stream</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-stream/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-stream/</id>
    <published>2024-09-10T07:56:00.000Z</published>
    <updated>2024-09-10T07:56:59.001Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Stream：支持多播的可持久化的消息队列<ul><li>常用操作命令</li></ul></li><li>Stream 消息队列的问题</li></ul><hr><h2 id="Stream">Stream</h2><p><img src="/static/IT/Redis/Redis-Stream-1.png" alt="Stream"></p><ul><li>每一个Stream都有一个消息链表，将所有加入的消息都串起来，每个消息都有一个唯一的 ID 和对应的内容</li><li>消息是持久化的，Redis 重启后，内容还在</li><li>每个 Stream 都有唯一的名称，它就是 Redis 的 key，在我们首次使用xadd指令追加消息时自动创建</li><li>每个 Stream 都可以挂多个消费组，每个消费组会有个游标last_delivered_id在Stream 数组之上往前移动，表示当前消费组已经消费到哪条消息了<ul><li>每个消费组都有一个 Stream 内唯一的名称，消费组不会自动创建，它需要单独的指令xgroup create进行创建，需要指定从 Stream 的某个消息 ID 开始消费，这个ID 用来初始化last_delivered_id变量</li><li>每个消费组 (Consumer Group) 的状态都是独立的，相互不受影响<ul><li>同一份 Stream 内部的消息会被每个消费组都消费到</li></ul></li><li>同一个消费组 (Consumer Group) 可以挂接多个消费者 (Consumer)<ul><li>这些消费者之间是竞争关系，任意一个消费者读取了消息都会使游标last_delivered_id往前移动</li><li>每个消费者有一个组内唯一名称</li></ul></li></ul></li><li>消费者 (Consumer) 内部会有个状态变量 pending_ids，它记录了当前已经被客户端读取,但是还没有 ack 的消息<ul><li>如果客户端没有 ack，这个变量里面的消息 ID 会越来越多，一旦某个消息被 ack，它就开始减少</li><li>这个 pending_ids 变量在 Redis 官方被称之为 PEL，也就是 Pending Entries List，这是一个很核心的数据结构<ul><li>它用来确保客户端至少消费了消息一次，而不会在网络传输的中途丢失了没处理</li></ul></li></ul></li><li>消息 ID 的形式是 timestampInMillis-sequence，例如1527846880572-5，它表示当前的消息在毫米时间戳 1527846880572 时产生，并且是该毫秒内产生的第5 条消息<ul><li>消息 ID 可以由服务器自动生成，也可以由客户端自己指定，但是形式必须是整数-整数，而且必须是后面加入的消息的 ID 要大于前面的消息 ID</li></ul></li><li>消息内容就是键值对，形如 hash 结构的键值对</li></ul><hr><h2 id="常用操作命令">常用操作命令</h2><ul><li>生产端<ul><li>命令<ul><li>xadd 追加消息</li><li>xdel 删除消息，这里的删除仅仅是设置了标志位，不会实际删除消息</li><li>xrange 获取消息列表，会自动过滤已经删除的消息</li><li>xlen 消息长度</li><li>del 删除 Stream</li></ul></li><li><code>xadd streamtest * name mark age 18</code><ul><li>streamtest 表示当前这个队列的名字，也就是我们一般意义上Redis 中的key</li><li><code>*</code> 号表示服务器自动生成 ID</li><li><code>name mark age 18</code> 是 key/value 的存储形式</li><li>返回值 1626705954593-0 则是生成的消息 ID，由两部分组成：时间戳-序号<ul><li>时间戳时毫秒级单位，是生成消息的 Redis 服务器时间，它是个 64 位整型</li><li>序号是在这个毫秒时间点内的消息序号。它也是个 64 位整型</li></ul></li><li>为了保证消息是有序的，因此 Redis 生成的ID 是单调递增有序的<ul><li>由于 ID中包含时间戳部分，为了避免服务器时间错误而带来的问题（例如服务器时间延后了），Redis 的每个 Stream 类型数据都维护一个 latest_generated_id 属性，用于记录最后一个消息的 ID</li><li>若发现当前时间戳退后（小于 latest_generated_id 所记录的），则采用时间戳不变而序号递增的方案来作为新消息 ID（这也是序号为什么使用int64 的原因，保证有足够多的的序号），从而保证 ID 的单调递增性质</li></ul></li><li>如果不是非常特别的需求，强烈建议使用 Redis 的方案生成消息ID，因为这种时间戳+序号的单调递增的 ID 方案，几乎可以满足全部的需求，但 ID 是支持自定义的</li></ul></li><li><code>xrange streamtest - +</code><ul><li><code>-</code> 表示最小值 , <code>+</code> 表示最大值</li></ul></li><li><code>xrange streamtest - 1626705954593-0</code> 指定消息 ID</li><li><code>xdel streamtest 1626706380924-0</code></li><li><code>xlen streamtest</code></li><li><code>del streamtest</code> 删除整个 Stream</li></ul></li><li>单消费者<ul><li>虽然 Stream 中有消费者组的概念，但是可以在不定义消费组的情况下进行Stream 消息的独立消费，当 Stream 没有新消息时，甚至可以阻塞等待</li><li>Redis 设计了一个单独的消费指令 xread，可以将 Stream 当成普通的消息队列 (list) 来使用<ul><li>使用 xread 时，我们可以完全忽略消费组 (Consumer Group) 的存在，就好比 Stream 就是一个普通的列表 (list)</li></ul></li><li><code>xread count 1 streams stream2 0-0</code><ul><li>“count 1” 表示从 Stream 读取1 条消息，缺省当然是头部</li><li>“streams”  可以理解为Redis 关键字</li><li>“stream2” 指明了要读取的队列名称</li><li>“0-0” 指从头开始</li></ul></li><li><code>xread count 2 streams stream2 1626710882927-0</code><ul><li>指定从 streams 的消息Id 开始(不包括命令中的消息 id)</li></ul></li><li><code>xread count 1 streams stream2 $</code><ul><li><code>$</code> 代表从尾部读取，上面的意思就是从尾部读取最新的一条消息</li><li>此时默认不返回任何消息，所以最好以阻塞的方式读取尾部最新的一条消息，直到新的消息的到来<ul><li><code>xread block 0 count 1 streams stream2 $</code><ul><li>block 后面的数字代表阻塞时间，单位毫秒</li></ul></li></ul></li></ul></li><li>一般来说客户端如果想要使用 xread 进行顺序消费，一定要记住当前消费到哪里了，也就是返回的消息 ID<ul><li>下次继续调用 xread 时，将上次返回的最后一个消息 ID 作为参数传递进去，就可以继续消费后续的消息</li></ul></li></ul></li><li>消费组<ul><li>xgroup create 指令创建消费组 (Consumer Group)，需要传递起始消息 ID 参数用来初始化 last_delivered_id 变量<ul><li><code>xgroup create stream2 cg1 0-0</code><ul><li>“stream2” 指明了要读取的队列名称</li><li>“cg1” 表示消费组的名称</li><li>“0-0” 表示从头开始消费</li></ul></li><li><code>xgroup create stream2 cg2 $</code><ul><li><code>$</code> 表示从尾部开始消费，只接受新消息，当前 Stream 消息会全部忽略</li></ul></li><li><code>xinfo stream stream2</code></li><li><code>xinfo groups stream2</code></li></ul></li><li>xreadgroup 指令可以进行消费组的组内消费，需要提供消费组名称、消费者名称和起始消息 ID；同 xread 一样，也可以阻塞等待新消息。读到新消息后，对应的消息 ID 就会进入消费者的 PEL(正在处理的消息) 结构里，客户端处理完毕后使用 xack 指令通知服务器，本条消息已经处理完毕，该消息 ID 就会从 PEL 中移除<ul><li><code>xreadgroup GROUP cg1 c1 count 1 streams stream2 &gt;</code><ul><li>“GROUP”属于关键字</li><li>“cg1”是消费组名称</li><li>“c1”是消费者名称</li><li>“count 1”指明了消费数量</li><li><code>&gt;</code> 号表示从当前消费组的 last_delivered_id 后面开始读<ul><li>每当消费者读取一条消息，last_delivered_id 变量就会前进</li></ul></li></ul></li><li><code>xreadgroup GROUP cg1 c1 block 0 count 1 streams stream2 &gt;</code> 阻塞等待</li></ul></li><li>如果同一个消费组有多个消费者，我们还可以通过 xinfo consumers 指令观察每个消费者的状态<ul><li><code>xinfo consumers stream2 cg1</code></li><li><code>xack stream2 cg1 1626751586744-0</code> 确认一条消息<ul><li>xack 允许带多个消息 id</li></ul></li></ul></li><li>XPENDIING 用来获消费组或消费内消费者的未处理完毕的消息<ul><li>每个 Pending 的消息有 4 个属性：消息 ID；所属消费者；IDLE 已读取时长；delivery counter 消息被读取次数</li></ul></li><li>XCLAIM 用以进行消息转移的操作，将某个消息转移到自己的 Pending列表中。需要设置组、转移的目标消费者和消息 ID，同时需要提供 IDLE（已被读取时长），只有超过这个时长，才能被转移</li></ul></li></ul><hr><h2 id="Stream-消息队列的问题">Stream 消息队列的问题</h2><ul><li>消息太多<ul><li>定长 Stream ：在 xadd 的指令提供一个定长长度 maxlen，就可以将老的消息干掉，确保最多不超过指定长度</li></ul></li><li>消费者忘记 ACK<ul><li>导致 PEL 列表不断增长，如果有很多消费组的话，那么这个 PEL 占用的内存就会放大</li></ul></li><li>PEL 如何避免消息丢失<ul><li>在客户端消费者读取 Stream 消息时，Redis 服务器将消息回复给客户端的过程中，客户端突然断开了连接，消息就丢失了</li><li>但是 PEL 里已经保存了发出去的消息 ID</li><li>客户端重新连上之后，可以再次收到 PEL 中的消息 ID 列表<ul><li>不过此时 xreadgroup 的起始消息 ID 不能为参数 <code>&gt;</code>，而必须是任意有效的消息ID，一般将参数设为 0-0，表示读取所有的 PEL 消息以及自last_delivered_id之后的新消息</li></ul></li></ul></li><li>死信问题<ul><li>如果某个消息，不能被消费者处理，也就是不能被 XACK，这是要长时间处于 Pending 列表中，即使被反复的转移给各个消费者也是如此</li><li>此时该消息的delivery counter（通过XPENDING 可以查询到）就会累加，当累加到某个我们预设的临界值时，我们就认为是坏消息（也叫死信，DeadLetter，无法投递的消息）<ul><li>将坏消息处理掉即可，删除即可，使用XDEL 语法<ul><li>注意，这个命令并没有删除 Pending 中的消息，因此查看 Pending，消息还会在，可以在执行执行 XDEL 之后，XACK 这个消息标识其处理完毕</li></ul></li></ul></li></ul></li><li>Stream 的高可用<ul><li>Stream 的高可用是建立主从复制基础上的，它和其它数据结构的复制机制没有区别</li><li>也就是说在 Sentinel 和 Cluster 集群环境下 Stream 是可以支持高可用的</li><li>不过鉴于 Redis 的指令复制是异步的，在 failover 发生时，Redis 可能会丢失极小部分数据<ul><li>这点 Redis 的其它数据结构也是一样的</li></ul></li></ul></li><li>分区 Partition<ul><li>Redis 的服务器没有原生支持分区能力，如果想要使用分区，那就需要分配多个 Stream<ul><li>提供不同的 Stream 名称，对消息进行 hash 取模来选择往哪个 Stream 里塞</li></ul></li><li>然后在客户端使用一定的策略来生产消息到不同的 Stream</li></ul></li><li>使用场景<ul><li>如果是中小项目和企业，在工作中已经使用了 Redis，在业务量不是很大，而又需要消息中间件功能的情况下，可以考虑使用 Redis 的Stream 功能</li><li>如果并发量很高，资源足够支持下，还是以专业的消息中间件，比如RocketMQ、Kafka 等来支持业务更好</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Stream：支持多播的可持久化的消息队列
&lt;ul&gt;
&lt;li&gt;常用操作命令&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stream 消息队列的问题&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Stream&quot;&gt;Stream&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis-IO模型&amp;多线程</title>
    <link href="https://jxch.github.io/2024/09/10/architect/redis/redis-io-mo-xing-duo-xian-cheng/"/>
    <id>https://jxch.github.io/2024/09/10/architect/redis/redis-io-mo-xing-duo-xian-cheng/</id>
    <published>2024-09-10T07:50:00.000Z</published>
    <updated>2024-09-10T07:53:47.063Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Redis I/O 多路复用</li><li>Redis 多线程</li><li>Reactor 模式</li></ul><hr><h2 id="I-O-多路复用">I/O 多路复用</h2><ul><li>Redis 基于 Reactor 模式开发了自己的网络事件处理器</li><li>I/O 多路复用同时监听多个 socket，根据socket 当前执行的事件来为 socket 选择对应的事件处理器<ul><li>当被监听的 socket 准备好执行accept、read、write、close 等操作时，和操作对应的文件事件就会产生，这时 FEH 就会调用 socket 之前关联好的事件处理器来处理对应事件</li></ul></li><li>文件事件处理器（file event handler，后文简称为 FEH）是单线程的，所以 redis 设计为单线程模型</li><li>虽然 FEH 是单线程运行，但通过 I/O 多路复用监听多个 socket，不仅实现高性能的网络通信模型，又能和 Redis 服务器中其它同样单线程运行的模块交互，保证了Redis 内部单线程模型的简洁设计</li><li><img src="/static/IT/Redis/Redis-IO%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E7%BA%BF%E7%A8%8B-1.png" alt=""></li><li>socket<ul><li>文件事件就是对 socket 操作的抽象， 每当一个 socket 准备好执行连接accept、read、write、close 等操作时， 就会产生一个文件事件。一个服务器通常会连接多个socket， 多个 socket 可能并发产生不同操作，每个操作对应不同文件事件</li></ul></li><li>I/O 多路复用程序：负责监听多个 socket<ul><li>尽管文件事件可能并发出现， 但 I/O 多路复用程序会将所有产生事件的socket 放入队列， 通过该队列以有序、同步且每次一个 socket 的方式向文件事件分派器传送 socket</li><li>当上一个 socket 产生的事件被对应事件处理器执行完后， I/O 多路复用程序才会向文件事件分派器传送下个 socket</li><li><img src="/static/IT/Redis/Redis-IO%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E7%BA%BF%E7%A8%8B-2.png" alt=""></li></ul></li><li>Redis 的 I/O 多路复用程序的所有功能都是通过包装常见的 select、epoll、evport 和 kqueue 这些 I/O 多路复用函数库实现的<ul><li>编译时自动选择系统中性能最高的I/O 多路复用函数库作为 Redis 的 I/O 多路复用程序的底层实现：性能降序排列<ul><li>Evport，Epoll 和 KQueue 具有 O(1)描述符选择算法复杂度，可以提供很多(数十万个)文件描述符</li><li>select 复杂性是 O(n)，最多只能提供 1024 个描述符</li></ul></li></ul></li><li>文件事件分派器：接收 I/O 多路复用程序传来的 socket， 并根据 socket 产生的事件类型， 调用相应的事件处理器</li><li>文件事件处理器：服务器会为执行不同任务的套接字关联不同的事件处理器</li><li>文件事件的类型<ul><li>I/O 多路复用程序可以监听多个 socket 的 ae.h/AE_READABLE 事件和ae.h/AE_WRITABLE 事件</li><li>当 socket 可读（比如客户端对 Redis 执行write/close 操作），或有新的可应答的socket 出现时（即客户端对 Redis 执行 connect 操作），socket 就会产生一个AE_READABLE 事件</li><li>当 socket 可写时（比如客户端对 Redis 执行read 操作），socket 会产生一个AE_WRITABLE 事件</li><li>I/O 多路复用程序可以同时监听 AE_REABLE 和AE_WRITABLE 两种事件，要是一个socket 同时产生这两种事件，那么文件事件分派器优先处理 AE_REABLE 事件</li><li>即一个socket 又可读又可写时， Redis 服务器先读后写 socket</li></ul></li><li>客户端和 Redis 服务器通信的整个过程<ul><li>Redis 启动初始化时，将连接应答处理器跟 AE_READABLE 事件关联</li><li>若一个客户端发起连接，会产生一个 AE_READABLE 事件，然后由连接应答处理器负责和客户端建立连接，创建客户端对应的 socket，同时将这个 socket的AE_READABLE 事件和命令请求处理器关联，使得客户端可以向主服务器发送命令请求</li><li>当客户端向 Redis 发请求时（不管读还是写请求），客户端 socket 都会产生一个AE_READABLE 事件，触发命令请求处理器。处理器读取客户端的命令内容，然后传给相关程序执行</li><li>当 Redis 服务器准备好给客户端的响应数据后，会将 socket 的AE_WRITABLE事件和命令回复处理器关联，当客户端准备好读取响应数据时，会在 socket 产生一个AE_WRITABLE 事件，由对应命令回复处理器处理，即将准备好的响应数据写入socket，供客户端读取</li><li>命令回复处理器全部写完到 socket 后，就会删除该socket 的AE_WRITABLE事件和命令回复处理器的映射</li></ul></li></ul><hr><h2 id="Redis-多线程">Redis 多线程</h2><ul><li>严格来讲从 Redis4.0 之后并不是单线程，除了主线程外，它也有后台线程在处理一些较为缓慢的操作，例如清理脏数据、无用连接的释放、大 key 的删除等等</li><li>通过AE 事件模型以及 IO 多路复用等技术，处理性能非常高，因此没有必要使用多线程<ul><li>Hash 的惰性Rehash、Lpush 等等 “线程不安全” 的命令都可以无锁进行</li></ul></li><li>读写网络的 read/write 系统调用占用了Redis执行期间大部分CPU 时间，瓶颈主要在于网络的 IO 消耗</li><li><code>io-threads-do-reads yes</code> 开启多线程<ul><li><code>io-threads 4</code> 开启多线程后，还需要设置线程数，否则是不生效的</li><li>线程数一定要小于机器核数<ul><li>4 核的机器建议设置为2 或3 个线程</li><li>8 核的建议设置为 6 个线程</li><li>超过了 8 个基本就没什么意义了</li></ul></li></ul></li><li>至少要 4 核的机器，且 Redis 实例已经占用相当大的 CPU耗时的时候才建议采用，否则使用多线程没有意义</li><li>多线程的实现机制<ul><li>主线程负责接收建立连接请求，获取 socket 放入全局等待读处理队列</li><li>主线程处理完读事件之后，通过 RR(Round Robin) 将这些连接分配给这些 IO 线程</li><li>主线程阻塞等待 IO 线程读取 socket 完毕</li><li>主线程通过单线程的方式执行请求命令，请求数据读取并解析完成，但并不执行回写 socket</li><li>主线程阻塞等待 IO 线程将数据回写 socket 完毕</li><li>解除绑定，清空等待队列</li></ul></li><li>特点<ul><li>IO 线程要么同时在读 socket，要么同时在写，不会同时读或写</li><li>IO 线程只负责读写 socket 解析命令，不负责命令处理</li></ul></li><li>不需要去考虑控制key、lua、事务，LPUSH/LPOP 等等的并发及线程安全问题<ul><li>Redis 的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程顺序执行</li></ul></li></ul><hr><h2 id="Reactor-模式">Reactor 模式</h2><p>单线程 Reactor 模式：<br><img src="/static/IT/Redis/Redis-IO%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E7%BA%BF%E7%A8%8B-3.png" alt="单线程 Reactor 模式"></p><p>单线程 Reactor + 工作者线程池：<br><img src="/static/IT/Redis/Redis-IO%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E7%BA%BF%E7%A8%8B-4.png" alt="单线程 Reactor + 工作者线程池"></p><p>多 Reactor 线程模式：<br><img src="/static/IT/Redis/Redis-IO%E6%A8%A1%E5%9E%8B-%E5%A4%9A%E7%BA%BF%E7%A8%8B-5.png" alt="多 Reactor 线程模式"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;Redis I/O 多路复用&lt;/li&gt;
&lt;li&gt;Redis 多线程&lt;/li&gt;
&lt;li&gt;Reactor 模式&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;I-O-多路复用&quot;&gt;I/O 多路复用&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Redis 基于 Reactor 模式开</summary>
      
    
    
    
    <category term="IT学习笔记" scheme="https://jxch.github.io/categories/IT%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="Redis" scheme="https://jxch.github.io/tags/Redis/"/>
    
  </entry>
  
</feed>
